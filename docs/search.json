[
  {
    "objectID": "Panel_part3_solutions.html",
    "href": "Panel_part3_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Required packages\n\n\nCode\npkgs &lt;- c(\"plm\", \"feisr\", \"sandwich\", \"did\", \"texreg\", \"tidyr\", \"haven\", \"dplyr\", \"ggplot2\", \"ggforce\") \nlapply(pkgs, require, character.only = TRUE)\n\n\n\n\nSession info\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggforce_0.4.1  ggplot2_3.4.2  dplyr_1.1.2    haven_2.5.3    tidyr_1.3.0   \n [6] texreg_1.38.6  did_2.1.2      sandwich_3.0-2 feisr_1.3.0    plm_2.6-3     \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.3        xfun_0.39           htmlwidgets_1.6.2  \n [4] rstatix_0.7.2       collapse_1.9.6      lattice_0.21-8     \n [7] numDeriv_2016.8-1.1 vctrs_0.6.3         tools_4.3.1        \n[10] Rdpack_2.4          generics_0.1.3      parallel_4.3.1     \n[13] tibble_3.2.1        fansi_1.0.4         pkgconfig_2.0.3    \n[16] Matrix_1.5-4.1      data.table_1.14.8   lifecycle_1.0.3    \n[19] farver_2.1.1        compiler_4.3.1      maxLik_1.5-2       \n[22] munsell_0.5.0       carData_3.0-5       htmltools_0.5.5    \n[25] yaml_2.3.7          Formula_1.2-5       pillar_1.9.0       \n[28] car_3.1-2           ggpubr_0.6.0        MASS_7.3-60        \n[31] abind_1.4-5         nlme_3.1-162        tidyselect_1.2.0   \n[34] bdsmatrix_1.3-6     digest_0.6.32       purrr_1.0.1        \n[37] forcats_1.0.0       miscTools_0.6-28    polyclip_1.10-4    \n[40] fastmap_1.1.1       grid_4.3.1          colorspace_2.1-0   \n[43] cli_3.6.1           lfe_2.9-0           magrittr_2.0.3     \n[46] utf8_1.2.3          broom_1.0.5         withr_2.5.0        \n[49] dreamerr_1.2.3      scales_1.2.1        backports_1.4.1    \n[52] httr_1.4.6          rmarkdown_2.23      ggsignif_0.6.4     \n[55] hms_1.1.3           zoo_1.8-12          evaluate_0.21      \n[58] knitr_1.43          rbibutils_2.2.13    lmtest_0.9-40      \n[61] fixest_0.11.1       rlang_1.1.1         Rcpp_1.0.10        \n[64] xtable_1.8-4        glue_1.6.2          tweenr_2.0.2       \n[67] BMisc_1.4.5         rstudioapi_0.14     jsonlite_1.8.5     \n[70] R6_2.5.1           \n\n\n\n\nLoad data\nFor the purpose of this exercise, we will use a real-world data set. However, instead of constructing our own data, we use a shortcut and use data from the replication package of Hospido (2012). The replication package can be found here.\nTHis is an unbalanced panel with 32,066 observations and 2066 individuals for the period 1968–1993 of the PSID. It consists of male heads aged 25–55 with at least 9 years of usable wages data.\n\n\nCode\n# Load stata file\ndata.df &lt;- read_dta(\"_data/h-data.dta\")\n\n# Lets order this\nnames &lt;- names(data.df)\nnames &lt;- c(\"pid\", \"year\", names[-which(names %in% c(\"pid\", \"year\"))])\ndata.df &lt;- data.df[, names]\n\ndata.df &lt;- data.df[order(data.df$pid, data.df$year), ]\n\n\n\n\n\n\n\n\n\nvariable name\ndescription\n\n\n\n\npid\nINDIVIDUAL IDENTIFIER\n\n\nyear\nYEAR OF INTERVIEW\n\n\nage\nAGE OF INDIVIDUAL\n\n\nwhite\nWHITE DUMMY\n\n\ndropout\nDROPOUT DUMMY\n\n\ngrad\nGRADUATE DUMMY\n\n\ncollege\nCOLLEGE DUMMY\n\n\nmarried\nMARRIED DUMMY\n\n\nchild\nNUMBER OF CHILDREN\n\n\nfsize\nFAMILY SIZE\n\n\nhours\nYEARLY HOURS OF WORK\n\n\nlogwages\nLOG OF REAL ANNUAL WAGES\n\n\nchangejob\nJOB CHANGE DUMMY\n\n\nten1\nTENURE DUMMY less than a year\n\n\nten2\nTENURE DUMMY a year\n\n\nten3\nTENURE DUMMY 2-3 years\n\n\nten4\nTENURE DUMMY 4 through 9 years\n\n\nten5\nTENURE DUMMY 10 through 19 years\n\n\nten6\nTENURE DUMMY 20 years or more\n\n\nprofes\nPROFESSIONAL, TECHNICAL, AND KINDRED WORKERS DUMMY\n\n\nadmin\nMANAGERS AND ADMINISTRATORS DUMMY\n\n\nsales\nCLERICAL AND SALES WORKERS DUMMY\n\n\ncrafts\nCRAFTSMAN AND KINDRED WORKERS DUMMY\n\n\noperat\nOPERATIVES WORKERS DUMMY\n\n\nservic\nLABORERS AND SERVICES WORKERS DUMMY\n\n\nsmsa\nSMSA (Standard Metropolitan Statistical Area) DUMMY\n\n\nneast\nNORTH-EAST DUMMY\n\n\nncentr\nNORTH-CENTRAL DUMMY\n\n\nsouth\nSOUTH DUMMY\n\n\nwest\nWEST DUMMY\n\n\n\n\n\nExercise 1\nHave a look at the data.\n\nHow many observations do we have in 1968? How many in 1990?\n\n\n\nCode\ntable(data.df$year)\n\n\n\n1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 \n 655  694  738  780  856  943 1018 1098 1178 1229 1263 1310 1380 1419 1464 1506 \n1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 \n1559 1626 1583 1536 1486 1434 1392 1348 1315 1256 \n\n\n\nWhat is the average age in 1968? What was it in 1984? And how is this possible?\n\n\n\nCode\nby(data.df$age, data.df$year, FUN = function(x) summary(x))\n\n\ndata.df$year: 1968\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   31.50   38.00   36.99   42.00   50.00 \n------------------------------------------------------------ \ndata.df$year: 1969\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   25.0    31.0    38.0    37.4    43.0    52.0 \n------------------------------------------------------------ \ndata.df$year: 1970\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   31.00   38.00   37.56   44.00   51.00 \n------------------------------------------------------------ \ndata.df$year: 1971\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   31.00   39.00   37.99   44.00   52.00 \n------------------------------------------------------------ \ndata.df$year: 1972\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   30.00   39.00   37.95   45.00   52.00 \n------------------------------------------------------------ \ndata.df$year: 1973\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   30.00   38.00   37.83   45.50   53.00 \n------------------------------------------------------------ \ndata.df$year: 1974\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   25.0    29.0    38.0    37.9    46.0    54.0 \n------------------------------------------------------------ \ndata.df$year: 1975\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   37.00   37.98   47.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1976\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   25.0    29.0    37.0    38.1    47.0    55.0 \n------------------------------------------------------------ \ndata.df$year: 1977\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   36.00   37.78   47.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1978\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   35.00   37.27   46.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1979\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   34.00   36.90   45.75   55.00 \n------------------------------------------------------------ \ndata.df$year: 1980\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   33.00   36.61   44.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1981\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   34.00   36.31   43.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1982\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   34.00   36.23   42.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1983\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   34.00   36.07   41.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1984\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.00   34.00   35.83   40.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1985\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   29.25   34.00   35.77   40.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1986\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   30.00   35.00   36.34   41.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1987\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  26.00   31.00   35.00   37.02   41.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1988\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  27.00   32.00   36.00   37.61   42.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1989\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  28.00   33.00   37.00   38.25   42.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1990\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  29.00   34.00   38.00   38.92   43.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1991\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   35.00   39.00   39.71   44.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1992\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  31.00   36.00   39.00   40.48   44.00   55.00 \n------------------------------------------------------------ \ndata.df$year: 1993\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  32.00   37.00   41.00   41.45   45.00   55.00 \n\n\n\nAt which age did individual with ID “5790002” become father?\n\n\n\nCode\ndata.df[data.df$pid == 5790002, ]\n\n\n# A tibble: 25 × 30\n       pid  year   age fsize hours child  ten1  ten2  ten3  ten4  ten5  ten6\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 5790002  1969    31     2  2352     0     0     0     1     0     0     0\n 2 5790002  1970    32     2  1960     0     0     0     1     0     0     0\n 3 5790002  1971    33     2  2040     0     0     0     0     1     0     0\n 4 5790002  1972    34     2  1880     0     0     0     0     1     0     0\n 5 5790002  1973    35     3  1578     1     1     0     0     0     0     0\n 6 5790002  1974    37     3  2400     1     0     1     0     0     0     0\n 7 5790002  1975    37     3   890     1     0     0     1     0     0     0\n 8 5790002  1976    37     3  1060     1     0     1     0     0     0     0\n 9 5790002  1977    38     3  2041     1     0     0     1     0     0     0\n10 5790002  1978    40     3  1820     1     0     0     1     0     0     0\n# ℹ 15 more rows\n# ℹ 18 more variables: changejob &lt;dbl&gt;, logwages &lt;dbl&gt;, white &lt;dbl&gt;,\n#   smsa &lt;dbl&gt;, dropout &lt;dbl&gt;, grad &lt;dbl&gt;, college &lt;dbl&gt;, neast &lt;dbl&gt;,\n#   ncentr &lt;dbl&gt;, south &lt;dbl&gt;, west &lt;dbl&gt;, married &lt;dbl&gt;, profes &lt;dbl&gt;,\n#   admin &lt;dbl&gt;, sales &lt;dbl&gt;, crafts &lt;dbl&gt;, operat &lt;dbl&gt;, servic &lt;dbl&gt;\n\n\nCode\n# View(data.df[data.df$pid == 5790002, ])\n\n\n\n\nExercise 2\nJust to play a little bit around with the data, let us estimate some models.\n\nWhat is the correlation between age and wage? Please use different estimators to determine different tzpes of correlations.\n\n\nPooled OLS\n\n\nCode\nmod1.lm &lt;- lm(logwages ~ age, data = data.df)\nsummary(mod1.lm)\n\n\n\nCall:\nlm(formula = logwages ~ age, data = data.df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7705 -0.2812  0.0759  0.3672  2.3104 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.8320553  0.0152740   643.7   &lt;2e-16 ***\nage         0.0118445  0.0003974    29.8   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5814 on 32064 degrees of freedom\nMultiple R-squared:  0.02696,   Adjusted R-squared:  0.02693 \nF-statistic: 888.3 on 1 and 32064 DF,  p-value: &lt; 2.2e-16\n\n\nThe higher the age of an observation, the higher their wage. If the age of an observation is 1 year higher, the log annual wage of this observation is 0.0118. In other words, the wage of an observation is higher by 1.19% with an additional year of age (the mean annual wage is 33672).\n\n\nBetween model\n\n\nCode\nmod1.btw &lt;- plm(logwages ~ age, \n                id = c(\"pid\", \"year\"), # id and time\n                effect = \"individual\", # individual, time, or twoways as FE? \n                model = \"between\",     # between, FE, RE, FD\n                data = data.df)\nsummary(mod1.btw)\n\n\nOneway (individual) effect Between Model\n\nCall:\nplm(formula = logwages ~ age, data = data.df, effect = \"individual\", \n    model = \"between\", id = c(\"pid\", \"year\"))\n\nUnbalanced Panel: n = 2066, T = 9-26, N = 32066\nObservations used in estimation: 2066\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-2.804339 -0.279375  0.055394  0.327354  1.415169 \n\nCoefficients:\n             Estimate Std. Error  t-value  Pr(&gt;|t|)    \n(Intercept) 9.9989475  0.0592097 168.8733 &lt; 2.2e-16 ***\nage         0.0068539  0.0015675   4.3725  1.29e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    498.39\nResidual Sum of Squares: 493.81\nR-Squared:      0.0091778\nAdj. R-Squared: 0.0086977\nF-statistic: 19.1184 on 1 and 2064 DF, p-value: 1.2896e-05\n\n\nThe higher the age of an individual, the higher their wage. If the age of an individual is 1 year higher, the log annual wage of this individual is 0.0069. In other words, the wage of an individual is higher by 0.69% with an additional year of age (the mean annual wage is 33672).\n\n\nFixed Effects model\n\n\nCode\nmod1.fe &lt;- plm(logwages ~ age, \n                id = c(\"pid\", \"year\"), # id and time\n                effect = \"individual\", # individual, time, or twoways as FE? \n                model = \"within\",     # between, FE, RE, FD\n                data = data.df)\nsummary(mod1.fe)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = logwages ~ age, data = data.df, effect = \"individual\", \n    model = \"within\", id = c(\"pid\", \"year\"))\n\nUnbalanced Panel: n = 2066, T = 9-26, N = 32066\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-3.735420 -0.103058  0.023293  0.151912  2.380972 \n\nCoefficients:\n     Estimate Std. Error t-value  Pr(&gt;|t|)    \nage 0.0197047  0.0003639  54.149 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    3718.1\nResidual Sum of Squares: 3387\nR-Squared:      0.089038\nAdj. R-Squared: 0.026301\nF-statistic: 2932.11 on 1 and 29999 DF, p-value: &lt; 2.22e-16\n\n\nThe higher the age of an individual, the higher their wage. If an individual become one year older, the log annual wage of this individual increases by 0.0197. In other words, the wage of an individual increases by 1.99% with an additional year of age for this individual (the mean annual wage is 33672).\n\n\nRandom Effects model\n\n\nCode\nmod1.fe &lt;- plm(logwages ~ age, \n                id = c(\"pid\", \"year\"), # id and time\n                effect = \"individual\", # individual, time, or twoways as FE? \n                model = \"random\",     # between, FE, RE, FD\n                data = data.df)\nsummary(mod1.fe)\n\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = logwages ~ age, data = data.df, effect = \"individual\", \n    model = \"random\", id = c(\"pid\", \"year\"))\n\nUnbalanced Panel: n = 2066, T = 9-26, N = 32066\n\nEffects:\n                 var std.dev share\nidiosyncratic 0.1129  0.3360 0.337\nindividual    0.2226  0.4718 0.663\ntheta:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7690  0.8062  0.8298  0.8245  0.8464  0.8617 \n\nResiduals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-3.8127 -0.1039  0.0374  0.0013  0.1713  2.3549 \n\nCoefficients:\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \n(Intercept) 9.54734049 0.01691335 564.485 &lt; 2.2e-16 ***\nage         0.01902706 0.00035495  53.605 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    6229.4\nResidual Sum of Squares: 3635.9\nR-Squared:      0.41669\nAdj. R-Squared: 0.41667\nChisq: 2873.53 on 1 DF, p-value: &lt; 2.22e-16\n\n\nThe higher the age of an individual, the higher their wage. Well,… I don’t actually know how to translate that into a substantive interpretation.\n\n\n\nExercise 3\nCan we use this dataset to replicate our earlier analysis on the marital wage premium? What might be a problem here? (tip: have a look a the marriage variable).\n\n\nCode\n### Lets check if variable are time-varying or constant\n\nlapply(data.df, FUN = function(x) mean(ave(x, data.df$pid, FUN = sd)))\n\n\n$pid\n[1] 0\n\n$year\n[1] 5.084193\n\n$age\n[1] 5.091139\n\n$fsize\n[1] 0.8707511\n\n$hours\n[1] 371.2656\n\n$child\n[1] 0.8319735\n\n$ten1\n[1] 0.2655135\n\n$ten2\n[1] 0.2251966\n\n$ten3\n[1] 0.3190326\n\n$ten4\n[1] 0.4291132\n\n$ten5\n[1] 0.3305668\n\n$ten6\n[1] 0.1446421\n\n$changejob\n[1] 0.293274\n\n$logwages\n[1] 0.2923183\n\n$white\n[1] 0.01130643\n\n$smsa\n[1] 0.1294999\n\n$dropout\n[1] 0.03838475\n\n$grad\n[1] 0.06823993\n\n$college\n[1] 0.03114909\n\n$neast\n[1] 0.01985548\n\n$ncentr\n[1] 0.02750533\n\n$south\n[1] 0.03582719\n\n$west\n[1] 0.02641131\n\n$married\n[1] 0\n\n$profes\n[1] 0.1099356\n\n$admin\n[1] 0.1295905\n\n$sales\n[1] 0.09946584\n\n$crafts\n[1] 0.1912406\n\n$operat\n[1] 0.1720871\n\n$servic\n[1] 0.1322412\n\n\nThe variable married is time-constant and does not have any within-person variance. This is somehow not very useful, as marital status obviously is not a time-constant trait.\nHowever, we do something similar: is there a fatherhood wage premium? In other words, do men experience an increase in wages when they become fathers?\n* Restrict the age at the start (first wage) to people aged 25-35\n\n\nCode\ndata.df &lt;- data.df[order(data.df$pid, data.df$year), ] # just to be sure\n\n# Person year number\ndata.df$pynr &lt;- ave(data.df$year,\n                data.df$pid,\n                FUN = function(x) 1:length(x))\n\n# Age at first wave\ndata.df$f_age &lt;- ifelse(data.df$pynr == 1, data.df$age, NA)\n\n# Distribute across individual, using mean and na.rm = TRUE\ndata.df$f_age &lt;- ave(data.df$f_age,\n                     data.df$pid,\n                     FUN = function(x) mean(x, na.rm = TRUE))\n\ntable(data.df$f_age)\n\n\n\n   25    26    27    28    29    30    31    32    33    34    35    36    37 \n14803  2973  1417  1091  1155   813   743   496   715   888   678   564   653 \n   38    39    40    41    42    43    44    45    46    47    48    49    50 \n  618   673   709   553   549   419   398   402   365   296    47    28    20 \n\n\nCode\n# Restrict\ndata.df &lt;- data.df[data.df$f_age &lt;= 35,]\n\n\n* Use number of children to construct a binary indicator of wether the is a child in the household or not\n\n\nCode\ndata.df$child_hh &lt;- ifelse(data.df$child &gt; 0, 1, 0)\n\ntable(data.df$child, data.df$child_hh)\n\n\n    \n        0    1\n  0  7651    0\n  1     0 5428\n  2     0 7397\n  3     0 3525\n  4     0 1207\n  5     0  328\n  6     0  158\n  7     0   42\n  8     0   15\n  9     0   12\n  10    0    8\n  11    0    1\n\n\n* Make sure we start only with men who are not yet fathers.\n\n\nCode\n# Child status at first wave\ndata.df$f_child_hh &lt;- ifelse(data.df$pynr == 1, data.df$child_hh, NA)\ntable(data.df$f_child_hh) # More than 1,500 individuals are already child_hh from the start on\n\n\n\n  0   1 \n748 859 \n\n\nCode\n# Distribute across individual, using mean and na.rm = TRUE\ndata.df$f_child_hh &lt;- ave(data.df$f_child_hh,\n                         data.df$pid,\n                         FUN = function(x) mean(x, na.rm = TRUE))\n\ntable(data.df$f_child_hh)\n\n\n\n    0     1 \n11424 14348 \n\n\nCode\n# Drop those who are child_hh from the start\ndata.df &lt;- data.df[data.df$f_child_hh == 0, ]\n\n\nNote: this feels like dropping a lot of information! However, it makes sense if we want to correctly identify the effect of interest.\n* Do we need to drop observations where people go from child to no child?\n\n\nCode\ndata.df &lt;- data.df[order(data.df$pid, data.df$year), ]\n\n# Change in child status within an id\ndata.df$fd_child_hh &lt;- ave(data.df$child_hh,\n                          data.df$pid,\n                          FUN = function(x) x - dplyr::lag(x, 1, default = 0)) # 0 instead of NA for 1st year\n\ntable(data.df$fd_child_hh)\n\n\n\n   -1     0     1 \n  143 10660   621 \n\n\nCode\n# There are indeed people going from child to \"no child\"\n# It might be that children leave the households, or parents get divorced?\n\n# One should usually look into this more carefully, but here let's just drop those observations.\n\n\n# Mark observations starting with a negative fd value (jump from marry=1 to marry =0)\ndata.df$notstag_child_hh &lt;- ave(ifelse(data.df$fd_child_hh == -1, 1, 0),\n                               data.df$pid,\n                               FUN = function(x) cumsum(x))\ntable(data.df$notstag_child_hh)\n\n\n\n    0     1     2 \n10560   821    43 \n\n\nCode\n# Drop after \"loosing a child\"\ndata.df &lt;- data.df[data.df$notstag_child_hh == 0, ]\ntable(data.df$fd_child_hh)\n\n\n\n   0    1 \n9979  581 \n\n\n* Calculate the effect of having a child on the wage of men (including controls if reasonable). \n\n* Calculate effects for POLS, RE, and FE (if you have some extra time, also FEIS). (#Hint: feis needs a class `data.frame` as input data)\n\n\nCode\n# Age intervals\nint &lt;- seq(25, max(data.df$age), 5)\ndata.df$age_gr &lt;- cut(data.df$age, int, include.lowest = TRUE)\n\n# Formula with controls\nfm &lt;- as.formula(logwages ~ child_hh + \n                   hours + age_gr +\n                   profes + admin  + sales + crafts + operat + servic  + \n                   smsa   +\n                   ten1 + ten2 + ten3 + ten4 + ten5 + ten6)\n\n### POLS\nmod.lm &lt;- lm(fm, data = data.df)\n\n### RE\nmod.re &lt;- plm(fm, data = data.df,\n              index = c(\"pid\", \"year\"),\n              effect = \"twoways\", model = \"random\")\n\n### FE\nmod.fe &lt;- plm(fm, data = data.df,\n              index = c(\"pid\", \"year\"),\n              effect = \"twoways\", model = \"within\")\n\n### FEIS\n# GIven that we have a relatively large T, we can actually use the 5 tenure dummies here\nmod.feis &lt;- feis(logwages ~ child_hh + \n                   hours + age_gr +\n                   profes + admin  + sales + crafts + operat + servic  + \n                   smsa + \n                   as.factor(year) | ten1 + ten2 + ten3 + ten4 + ten5 + ten6, # Note we add year manually here\n                data = data.frame(data.df),\n                id = c(\"pid\"), robust = TRUE) # we use the cluster-robust SEs right away\n\n\nWarning in feis(logwages ~ child_hh + hours + age_gr + profes + admin + : FEIS needs at least n(slopes)+1 observations per group. \n You specified 6 slope parameter(s) plus intercept, all groups with t &lt;= 7 dropped\n\n\n* Compare using cluster robust standard errors (and screenreg).\n\n\nCode\n### Cluster robust SEs for RE and FE\nvcov.re &lt;- vcovHC(mod.re, cluster = \"group\", method = \"arellano\", type = \"HC3\")\n\nvcov.fe &lt;- vcovHC(mod.fe, cluster = \"group\", method = \"arellano\", type = \"HC3\")\n\n# Plug into model object\nmod.re$vcov &lt;- vcov.re\nmod.fe$vcov &lt;- vcov.fe\n\n\n### Compare\nscreenreg(list(mod.lm, mod.re, mod.fe, mod.feis), digits = 3,\n          custom.model.names = c(\"POLS\", \"RE\", \"FE\", \"FEIS\"), \n          include.groups = FALSE, # don't know why include.groups = FALSE is necessary here\n          omit.coef = \"year\") \n\n\n\n=========================================================================\n               POLS           RE             FE             FEIS         \n-------------------------------------------------------------------------\n(Intercept)        8.747 ***      8.976 ***                              \n                  (0.051)        (0.106)                                 \nchild_hh           0.137 ***      0.101 ***      0.040 *        0.055 ** \n                  (0.010)        (0.029)        (0.016)        (0.019)   \nhours              0.000 ***      0.000 ***      0.000 ***      0.000 ***\n                  (0.000)        (0.000)        (0.000)        (0.000)   \nage_gr(30,35]      0.110 ***      0.143 ***      0.008          0.027    \n                  (0.012)        (0.019)        (0.013)        (0.015)   \nage_gr(35,40]      0.201 ***      0.233 ***     -0.036         -0.023    \n                  (0.014)        (0.030)        (0.022)        (0.022)   \nage_gr(40,45]      0.239 ***      0.282 ***     -0.131 ***     -0.106 ***\n                  (0.019)        (0.042)        (0.031)        (0.032)   \nage_gr(45,50]      0.251 ***      0.313 ***     -0.236 ***     -0.208 ***\n                  (0.029)        (0.060)        (0.043)        (0.046)   \nage_gr(50,55]      0.173 ***      0.264 *       -0.426 ***     -0.444 ***\n                  (0.053)        (0.107)        (0.084)        (0.094)   \nprofes             0.249 ***      0.179 *        0.121 **       0.133 ** \n                  (0.045)        (0.074)        (0.045)        (0.045)   \nadmin              0.250 ***      0.160 *        0.104 *        0.128 ** \n                  (0.046)        (0.076)        (0.045)        (0.046)   \nsales             -0.004          0.057          0.041          0.084    \n                  (0.046)        (0.075)        (0.046)        (0.048)   \ncrafts            -0.031          0.057          0.060          0.091 *  \n                  (0.045)        (0.071)        (0.042)        (0.044)   \noperat            -0.178 ***      0.032          0.068          0.098 *  \n                  (0.046)        (0.076)        (0.045)        (0.047)   \nservic            -0.371 ***     -0.028          0.021          0.057    \n                  (0.046)        (0.087)        (0.047)        (0.048)   \nsmsa               0.114 ***      0.023          0.018          0.026    \n                  (0.010)        (0.029)        (0.020)        (0.021)   \nten1               0.242 ***      0.112 *        0.101 *                 \n                  (0.028)        (0.054)        (0.039)                  \nten2               0.382 ***      0.223 ***      0.209 ***               \n                  (0.029)        (0.056)        (0.038)                  \nten3               0.404 ***      0.275 ***      0.263 ***               \n                  (0.028)        (0.054)        (0.037)                  \nten4               0.471 ***      0.331 ***      0.308 ***               \n                  (0.027)        (0.057)        (0.038)                  \nten5               0.509 ***      0.317 ***      0.270 ***               \n                  (0.028)        (0.064)        (0.037)                  \nten6               0.536 ***      0.290 **       0.220 ***               \n                  (0.037)        (0.092)        (0.043)                  \n-------------------------------------------------------------------------\nR^2                0.426          0.374          0.287          0.273    \nAdj. R^2           0.425          0.373          0.230          0.270    \nNum. obs.      10560          10560          10560          10336        \ns_idios                           0.299                                  \ns_id                              0.332                                  \ns_time                            0.016                                  \nRMSE                                                            0.313    \n=========================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\nInterpret the results\n\nWe can interpret the results above as follows:\nPOLS and RE: Observations with at least one child in the household tend to have higher annual wages\nFE: If men become fathers, this increases their annual wage by ~ 4%. The effect is significant (would be highly significant without cluster robust SEs).\nFEIS: Given the heterogeneous wage trajectories, if men become fathers, this increases their annual wage by ~ 5%.\nIn this case, FEIS estimates a higher premium than FE. This could indicate that those with steeper wage trajectories actually are less likely to get children or get their children later.\nDo you know why FEIS has a lower \\(N\\) than the other models?\n\nTry to perform a placebo test: what happens if you use the “lead” of becoming father. Why is this an interesting test?\n\n\n\nCode\ndata.df &lt;- data.df[order(data.df$pid, data.df$year), ]\n\n# Lag \ndata.df$lag_child_hh &lt;- ave(data.df$child_hh,\n                            data.df$pid,\n                            FUN = function(x) lag(x, 2)) # lets use two years\n\n# Lead\ndata.df$lead_child_hh &lt;- ave(data.df$child_hh,\n                             data.df$pid,\n                             FUN = function(x) lead(x, 2)) # lets use two years\n\n### Estimate FE models\nlag.fe &lt;- plm(logwages ~ lag_child_hh + \n                   hours + age_gr +\n                   profes + admin  + sales + crafts + operat + servic  + \n                   smsa   +\n                   ten1 + ten2 + ten3 + ten4 + ten5 + ten6, \n              data = data.df,\n              index = c(\"pid\", \"year\"),\n              effect = \"twoways\", model = \"within\")\n\nlead.fe &lt;- plm(logwages ~ lead_child_hh + \n                   hours + age_gr +\n                   profes + admin  + sales + crafts + operat + servic  + \n                   smsa   +\n                   ten1 + ten2 + ten3 + ten4 + ten5 + ten6, \n               data = data.df,\n              index = c(\"pid\", \"year\"),\n              effect = \"twoways\", model = \"within\")\n\n# Compare\nscreenreg(list(lag.fe, lead.fe), digits = 3,\n          custom.coef.map = list(lag_child_hh = \"Lag Child\", \n                              lead_child_hh = \"Lead Child\"))\n\n\n\n====================================\n            Model 1     Model 2     \n------------------------------------\nLag Child      0.025 *              \n              (0.011)               \nLead Child                 0.070 ***\n                          (0.013)   \n------------------------------------\nR^2            0.248       0.286    \nAdj. R^2       0.176       0.218    \nNum. obs.   9064        9064        \n====================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nInterestingly we already see a significant and non-trivial fatherhood effect on the wage two years before the future father actually become fathers.\nIf we observe a treatment effect for a treatment that has not yet happened, this might indicate either 1) anticipation effects, or 2) that something is actually confounding our treatment (e.g. selection on pre-treatment trends)\nWhat do you think, what might be happening here?\n\n\nExercise 4\nCan we use one of the new event-study approaches, such as the Callaway and SantAnna estimator?\n\nPreprocess data (treatment group and timing indicator)\n\n\n\nCode\n# treatment timing = year if child in household\ndata.df$treat_timing &lt;- ifelse(data.df$child_hh == 1, data.df$year, NA)\n\n# Define those who never have a child\ndata.df$tmp &lt;- ave(data.df$child_hh,\n                   data.df$pid,\n                   FUN = function(x) mean(x, na.rm = TRUE))\n\ndata.df$never_child &lt;- ifelse(data.df$tmp == 0, 1, 0)\n\n# set never treated to zero\ndata.df$treat_timing[data.df$never_child == 1] &lt;- 0\n\n# if married is not NA, used min year per id (removing NAs)\noo &lt;- which(!is.na(data.df$child_hh))\ndata.df$treat_timing[oo] &lt;- ave(data.df$treat_timing[oo],\n                                data.df$pid[oo],\n                                FUN = function(x) min(x, na.rm = TRUE))\n\n\nhead(data.df[, c(\"pid\", \"year\", \"child_hh\", \"never_child\", \"treat_timing\")], n = 35)\n\n\n# A tibble: 35 × 5\n     pid  year child_hh never_child treat_timing\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 11003  1979        0           1            0\n 2 11003  1980        0           1            0\n 3 11003  1981        0           1            0\n 4 11003  1982        0           1            0\n 5 11003  1983        0           1            0\n 6 11003  1984        0           1            0\n 7 11003  1985        0           1            0\n 8 11003  1986        0           1            0\n 9 11003  1987        0           1            0\n10 11003  1988        0           1            0\n# ℹ 25 more rows\n\n\n\nEstimate the model using att_gt\n\n\n\nCode\n# estimate group-time average treatment effects using att_gt method\nwages.attgt &lt;- att_gt(yname = \"logwages\",\n                      tname = \"year\",\n                      idname = \"pid\",\n                      gname = \"treat_timing\",\n                      xformla = ~ hours + age_gr,\n                      data = data.df,\n                      allow_unbalanced_panel = TRUE,\n                      control_group = \"notyettreated\",\n                      est_method = \"ipw\"\n                        )\n\n\n\n\nCode\n# Show the group-time specific estimates\nwages.dyn &lt;- aggte(wages.attgt, type = \"dynamic\", na.rm = TRUE,\n                   min_e = -3, max_e = 6)\nsummary(wages.dyn)\n\n\n\nCall:\naggte(MP = wages.attgt, type = \"dynamic\", min_e = -3, max_e = 6, \n    na.rm = TRUE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:  \n    ATT    Std. Error     [ 95%  Conf. Int.]  \n 0.1035        0.0269     0.0508      0.1562 *\n\n\nDynamic Effects:\n Event time Estimate Std. Error [95% Simult.  Conf. Band]  \n         -3   0.0589     0.0363       -0.0392      0.1569  \n         -2   0.0292     0.0273       -0.0445      0.1029  \n         -1  -0.0566     0.0214       -0.1145      0.0013  \n          0   0.0424     0.0190       -0.0090      0.0938  \n          1   0.0573     0.0241       -0.0079      0.1225  \n          2   0.0843     0.0305        0.0017      0.1668 *\n          3   0.1103     0.0325        0.0225      0.1981 *\n          4   0.1462     0.0361        0.0488      0.2437 *\n          5   0.1487     0.0386        0.0445      0.2529 *\n          6   0.1353     0.0442        0.0158      0.2548 *\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Not Yet Treated,  Anticipation Periods:  0\nEstimation Method:  Inverse Probability Weighting\n\n\nCode\nzp3 &lt;- ggdid(wages.dyn) \n  \nzp3 &lt;- zp3 + \n  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2) +\n  geom_vline(xintercept = -0.5, colour = \"black\", lty = 1)\n\nzp3\n\n\n\n\n\n\nInterpret the results\n\nThe event-time results above again tell a little different story. First, hose getting father already have a little higher wage two and three years before they become fathers. Then, there is a negative anticipation effect in the year before birth (why could this be plausible?). After birth of the first child, fathers’ wages increase significantly more then the wages of non-fathers (however, this is not much more than the effect in year -3).\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHospido, L. 2012. “Modelling Heterogeneity and Dynamics in the Volatility of Individual Wages.” Journal of Applied Econometrics 27 (3): 386–414. https://doi.org/10.1002/jae.1204."
  },
  {
    "objectID": "Panel_part2.html",
    "href": "Panel_part2.html",
    "title": "2) Advanced methods",
    "section": "",
    "text": "Required packages\n\n\nCode\npkgs &lt;- c(\"plm\", \"feisr\", \"did\", \"Synth\", \"SCtools\", \n          \"panelView\", \"texreg\", \"tidyr\", \"dplyr\", \"ggplot2\", \"ggforce\") \nlapply(pkgs, require, character.only = TRUE)\n\n\n\n\nSession info\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggforce_0.4.1    ggplot2_3.4.2    dplyr_1.1.2      tidyr_1.3.0     \n [5] texreg_1.38.6    panelView_1.1.17 SCtools_0.3.2.1  future_1.33.0   \n [9] Synth_1.1-8      did_2.1.2        feisr_1.3.0      plm_2.6-3       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0    farver_2.1.1        fastmap_1.1.1      \n [4] tweenr_2.0.2        fixest_0.11.1       digest_0.6.32      \n [7] lifecycle_1.0.3     dreamerr_1.2.3      magrittr_2.0.3     \n[10] kernlab_0.9-32      compiler_4.3.1      rlang_1.1.1        \n[13] tools_4.3.1         utf8_1.2.3          yaml_2.3.7         \n[16] data.table_1.14.8   collapse_1.9.6      knitr_1.43         \n[19] ggsignif_0.6.4      htmlwidgets_1.6.2   abind_1.4-5        \n[22] withr_2.5.0         purrr_1.0.1         numDeriv_2016.8-1.1\n[25] polyclip_1.10-4     grid_4.3.1          rgenoud_5.9-0.3    \n[28] fansi_1.0.4         ggpubr_0.6.0        xtable_1.8-4       \n[31] lfe_2.9-0           colorspace_2.1-0    BMisc_1.4.5        \n[34] globals_0.16.2      scales_1.2.1        MASS_7.3-60        \n[37] cli_3.6.1           rmarkdown_2.23      miscTools_0.6-28   \n[40] generics_0.1.3      rstudioapi_0.14     httr_1.4.6         \n[43] bdsmatrix_1.3-6     parallel_4.3.1      vctrs_0.6.3        \n[46] Matrix_1.5-4.1      sandwich_3.0-2      jsonlite_1.8.5     \n[49] carData_3.0-5       car_3.1-2           rstatix_0.7.2      \n[52] Formula_1.2-5       listenv_0.9.0       optimx_2022-4.30   \n[55] glue_1.6.2          parallelly_1.36.0   codetools_0.2-19   \n[58] gtable_0.3.3        lmtest_0.9-40       munsell_0.5.0      \n[61] tibble_3.2.1        pillar_1.9.0        htmltools_0.5.5    \n[64] R6_2.5.1            maxLik_1.5-2        Rdpack_2.4         \n[67] evaluate_0.21       lattice_0.21-8      rbibutils_2.2.13   \n[70] backports_1.4.1     broom_1.0.5         Rcpp_1.0.10        \n[73] gridExtra_2.3       nlme_3.1-162        xfun_0.39          \n[76] zoo_1.8-12          pkgconfig_2.0.3    \n\n\n\n\nOutline\n\nFixed Effects Individual Slopes\nDynamic treatment effects\nDynamic Diff-in-Diff\nSynthetic Control\nTBD: Generalized Synthetic Control\n\n\n\nFixed Effects Individual Slopes\nRemeber that we have to make the parallel trends assumption in twoways FE models. A violation of the parallel trends assumption leads to biased estimates. Usually, when controlling for time fixed effects, we make the assumption that every observation experiences the same “effect of time”.\nHowever, we can relax this assumption by giving each individual their own intercept and their own slope.\nThe fixed effects individual slope (FEIS) estimator is a more general version of the well-known fixed effects estimator (FE), which allows to control for heterogeneous slopes in addition to time-constant heterogeneity (e.g. Brüderl and Ludwig 2015; Polachek and Kim 1994; Rüttenauer and Ludwig 2023; Wooldridge 2010). Formally, the FEIS estimator can be expressed as\n\\[\n\\begin{align}\n\\boldsymbol{\\mathbf{y}}_{i} =& \\boldsymbol{\\mathbf{X}}_{i}\\boldsymbol{\\mathbf{\\beta }}+ \\boldsymbol{\\mathbf{W}}_i \\boldsymbol{\\mathbf{\\alpha}}_i + \\boldsymbol{\\mathbf{\\epsilon}}_{i},\n\\end{align}\n\\] where \\(\\boldsymbol{\\mathbf{y}}_{i}\\) is \\(T \\times 1\\), \\(\\boldsymbol{\\mathbf{X}}_{i}\\) is \\(T \\times K\\), and \\(\\boldsymbol{\\mathbf{\\epsilon}}_{i}\\) is \\(T \\times 1\\). \\(\\boldsymbol{\\mathbf{W}}_i\\) is a \\(T \\times J\\) matrix of slope variables, and \\(\\boldsymbol{\\mathbf{\\alpha}}_i\\) a \\(J \\times 1\\) vector of individual-specific slope parameters, for \\(J\\) slope parameters including a constant term. If \\(\\boldsymbol{\\mathbf{W}}_i\\) consists of a constant term only, \\(\\boldsymbol{\\mathbf{W}}_i = \\boldsymbol{\\mathbf{1}}\\), thus \\(\\boldsymbol{\\mathbf{\\alpha}}_i\\) reduces to \\(\\alpha_{i1}\\), and the above equation represents the well-known formula of a conventional FE model with individual fixed effects.\nAs with the conventional FE, FEIS can be estimated using lm() by including \\(N-1\\) individual-specific dummies and interaction terms of each slope variable with the \\(N-1\\) individual-specific dummies (\\((N-1) *J\\) controls). This is however highly inefficient. As with the conventional FE estimator, we can achieve the same result by running an lm() on pre-transformed data. Therefore, specify the ‘residual maker’ matrix \\(\\boldsymbol{\\mathbf{M}}_i = \\boldsymbol{\\mathbf{I}}_T - \\boldsymbol{\\mathbf{W}}_i(\\boldsymbol{\\mathbf{W}}^\\intercal_i \\boldsymbol{\\mathbf{W}}_i)^{-1}\\boldsymbol{\\mathbf{W}}^\\intercal_i\\), and estimate \\[\n\\begin{align}\ny_{it} - \\hat{y}_{it} =& (\\boldsymbol{\\mathbf{x}}_{it} - \\hat{\\boldsymbol{\\mathbf{x}}}_{it})\\boldsymbol{\\mathbf{\\beta }}+ \\epsilon_{it} - \\hat{\\epsilon}_{it}, \\\\\n\\boldsymbol{\\mathbf{M}}_i \\boldsymbol{\\mathbf{y}}_i =& \\boldsymbol{\\mathbf{M}}_i \\boldsymbol{\\mathbf{X}}_i\\boldsymbol{\\mathbf{\\beta }}+ \\boldsymbol{\\mathbf{M}}_i \\boldsymbol{\\mathbf{\\epsilon}}_{i}, \\\\\n\\tilde{\\boldsymbol{\\mathbf{y}}}_{i} =& \\tilde{\\boldsymbol{\\mathbf{X}}}_{i}\\boldsymbol{\\mathbf{\\beta }}+ \\tilde{\\boldsymbol{\\mathbf{\\epsilon}}}_{i},\n\\end{align}\n\\] where \\(\\tilde{\\boldsymbol{\\mathbf{y}}}_{i}\\), \\(\\tilde{\\boldsymbol{\\mathbf{X}}}_{i}\\), and \\(\\tilde{\\boldsymbol{\\mathbf{\\epsilon}}}_{i}\\) are the residuals of regressing \\(\\boldsymbol{\\mathbf{y}}_{i}\\), each column-vector of \\(\\boldsymbol{\\mathbf{X}}_{i}\\), and \\(\\boldsymbol{\\mathbf{\\epsilon}}_{i}\\) on \\(\\boldsymbol{\\mathbf{W}}_i\\).\nIntuitively, we\n\nestimate the individual-specific predicted values for the dependent variable and each covariate based on an individual intercept and the additional slope variables of \\(\\boldsymbol{\\mathbf{W}}_i\\),\n‘detrend’ the original data by these individual-specific predicted values, and\nrun an OLS model on the residual (‘detrended’) data.\n\nSimilarly, we can estimate a correlated random effects (CRE) model (Chamberlain 1982; Mundlak 1978; Wooldridge 2010) including the individual specific predictions \\(\\hat{\\boldsymbol{\\mathbf{X}}}_{i}\\) to obtain the FEIS estimator: \\[\n\\begin{align}\n\\boldsymbol{\\mathbf{y}}_{i} =& \\boldsymbol{\\mathbf{X}}_{i}\\boldsymbol{\\mathbf{\\beta }}+ \\hat{\\boldsymbol{\\mathbf{X}}}_{i}\\boldsymbol{\\mathbf{\\rho }}+ \\boldsymbol{\\mathbf{\\epsilon}}_{i}.\n\\end{align}\n\\]\nAlthough we are here mainly interested in controlling for individual time, FEIS can be used to control for individual specific effects of any covariate. For instance, Rüttenauer and Ludwig (2023) discuss an example of controlling for family-specific pre-treatment conditions in a sibling study on the effect of participating in pre-school programs on later life outcomes.\n\nExample\nAs an example, we use the mwp panel data, containing information on wages and family status of 268 men. This is a random sample drawn from the National Longitudinal Survey of Youth (NLSY79, n.d.), and more details on the selection of observations and variable construction can be found in Ludwig and Brüderl (2018).\n\n\nCode\ndata(\"mwp\", package = \"feisr\")\nhead(mwp)\n\n\n  id year      lnw      exp      expq marry evermarry enrol yeduc age cohort\n1  1 1981 1.934358 1.076923  1.159763     0         1     1    11  18   1963\n2  1 1983 2.468140 3.019231  9.115755     0         1     1    12  20   1963\n3  1 1984 2.162480 4.038462 16.309174     0         1     1    12  21   1963\n4  1 1985 1.746280 5.076923 25.775146     0         1     0    12  22   1963\n5  1 1986 2.527840 6.096154 37.163090     0         1     1    13  23   1963\n6  1 1987 2.365361 7.500000 56.250000     0         1     1    13  24   1963\n  yeargr yeargr1 yeargr2 yeargr3 yeargr4 yeargr5\n1      2       0       1       0       0       0\n2      2       0       1       0       0       0\n3      2       0       1       0       0       0\n4      2       0       1       0       0       0\n5      3       0       0       1       0       0\n6      3       0       0       1       0       0\n\n\nThe data set contains a unique person identifier (id) and survey year indicator (year). Furthermore, we have information about the log hourly wage rate (lnwage), work experience (exp) and its square (expq), family status (marry), enrollment in current education (enrol), years of formal education education (yeduc), age (age), birth cohort (cohort), and a grouped year indicator (yeargr).\nwe exemplary investigate the ‘marriage wage premium’: we analyze whether marriage leads to an increase in the hourly wage for men. We use the function feis to estimate fixed effects individual slope models to control for the hypothesis that those men who are more likely to marry or marry earlier, also have a steeper wage growth over time.\nLet’s start with our most common panel models (FE and RE):\n\n\nCode\nwages.fe &lt;- plm(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n                + exp + I(exp^2), data = mwp, index = c(\"id\", \"year\"),\n                model = \"within\", effect = \"individual\")\nwages.re &lt;- plm(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n                + exp + I(exp^2), data = mwp, index = c(\"id\", \"year\"),\n                model = \"random\", effect = \"individual\")\nsummary(wages.fe)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = lnw ~ marry + enrol + yeduc + as.factor(yeargr) + \n    exp + I(exp^2), data = mwp, effect = \"individual\", model = \"within\", \n    index = c(\"id\", \"year\"))\n\nUnbalanced Panel: n = 268, T = 4-19, N = 3100\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-2.5870006 -0.1580744  0.0081262  0.1701488  1.9958088 \n\nCoefficients:\n                      Estimate  Std. Error t-value  Pr(&gt;|t|)    \nmarry               0.07773216  0.02160148  3.5985 0.0003256 ***\nenrol              -0.20810059  0.02282898 -9.1156 &lt; 2.2e-16 ***\nyeduc               0.05584485  0.00715655  7.8033 8.424e-15 ***\nas.factor(yeargr)2 -0.14080625  0.03036533 -4.6371 3.694e-06 ***\nas.factor(yeargr)3 -0.16453499  0.04696595 -3.5033 0.0004667 ***\nas.factor(yeargr)4 -0.27553668  0.06196892 -4.4464 9.071e-06 ***\nas.factor(yeargr)5 -0.29750723  0.07932341 -3.7506 0.0001800 ***\nexp                 0.07299927  0.00867777  8.4122 &lt; 2.2e-16 ***\nI(exp^2)           -0.00127502  0.00036103 -3.5317 0.0004196 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    559.75\nResidual Sum of Squares: 327.88\nR-Squared:      0.41424\nAdj. R-Squared: 0.35697\nF-statistic: 221.816 on 9 and 2823 DF, p-value: &lt; 2.22e-16\n\n\nand we calculate panel robust standard errors and attach them back to the model output:\n\n\nCode\n# Calculate vcov\nvcovx_fe &lt;- vcovHC(wages.fe, cluster = \"group\", method = \"arellano\", type = \"HC3\")\nvcovx_re &lt;- vcovHC(wages.re, cluster = \"group\", method = \"arellano\", type = \"HC3\")\n\n# Replace original vcov in output\nwages.fe$vcov &lt;- vcovx_fe\nwages.re$vcov &lt;- vcovx_re\n\n\nReplacing the vcov in the model output has the advantage that we now use the cluster robust SEs in all following operations (like summary() or screenreg).\n\n\nCode\nsummary(wages.fe)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = lnw ~ marry + enrol + yeduc + as.factor(yeargr) + \n    exp + I(exp^2), data = mwp, effect = \"individual\", model = \"within\", \n    index = c(\"id\", \"year\"))\n\nUnbalanced Panel: n = 268, T = 4-19, N = 3100\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-2.5870006 -0.1580744  0.0081262  0.1701488  1.9958088 \n\nCoefficients:\n                      Estimate  Std. Error t-value  Pr(&gt;|t|)    \nmarry               0.07773216  0.03160634  2.4594 0.0139771 *  \nenrol              -0.20810059  0.02738300 -7.5996 4.015e-14 ***\nyeduc               0.05584485  0.01025801  5.4440 5.656e-08 ***\nas.factor(yeargr)2 -0.14080625  0.03554205 -3.9617 7.627e-05 ***\nas.factor(yeargr)3 -0.16453499  0.05338645 -3.0820 0.0020763 ** \nas.factor(yeargr)4 -0.27553668  0.06829208 -4.0347 5.613e-05 ***\nas.factor(yeargr)5 -0.29750723  0.08916462 -3.3366 0.0008591 ***\nexp                 0.07299927  0.01245954  5.8589 5.198e-09 ***\nI(exp^2)           -0.00127502  0.00057274 -2.2262 0.0260794 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    559.75\nResidual Sum of Squares: 327.88\nR-Squared:      0.41424\nAdj. R-Squared: 0.35697\nF-statistic: 77.9716 on 9 and 2823 DF, p-value: &lt; 2.22e-16\n\n\nAnd finally, we allow for individual specific trends. To replicate the analysis of Ludwig and Brüderl (2018), we use work experience (exp) and squared work experience as the slope variables.\nOne mayor advantage of using work experience as slope is that we can still control for (grouped) time fixed effects\nAssuming linear trends (only using exp), is a strong assumption. However, for each additional slope (e.g. polynomial), FEIS becomes more data hungry: each individual needs at least \\(T \\geq K + 1\\) observations to contribute to the model. If not, they are dropped!\nHere we use feis with panel robust standard errors. The command felm from lfe can be used to calculate individual slopes as well.\n\n\nCode\nwages.feis &lt;- feis(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n                   | exp + I(exp^2), data = mwp, id = \"id\",\n                   robust = TRUE)\nsummary(wages.feis)\n\n\n\n\nCall:\nfeis(formula = lnw ~ marry + enrol + yeduc + as.factor(yeargr) | \n    exp + I(exp^2), data = mwp, id = \"id\", robust = TRUE)\n\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-2.0790815 -0.1050450  0.0046876  0.1112708  1.9412090 \n\nCoefficients:\n                     Estimate Std. Error t-value  Pr(&gt;|t|)    \nmarry               0.0134582  0.0292771  0.4597   0.64579    \nenrol              -0.1181725  0.0235003 -5.0286 5.325e-07 ***\nyeduc              -0.0020607  0.0175059 -0.1177   0.90630    \nas.factor(yeargr)2 -0.0464504  0.0378675 -1.2267   0.22008    \nas.factor(yeargr)3 -0.0189333  0.0524265 -0.3611   0.71803    \nas.factor(yeargr)4 -0.1361305  0.0615033 -2.2134   0.02697 *  \nas.factor(yeargr)5 -0.1868589  0.0742904 -2.5152   0.01196 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCluster robust standard errors\nSlope parameters:  exp, I(exp^2) \nTotal Sum of Squares:    190.33\nResidual Sum of Squares: 185.64\nR-Squared:      0.024626\nAdj. R-Squared: 0.022419\n\n\nLet’s compare the results.\n\n\nCode\nscreenreg(list(wages.re, wages.fe, wages.feis), digits = 3,\n          custom.model.names = c(\"RE\", \"FE\", \"FEIS\"))\n\n\n\n============================================================\n                    RE            FE            FEIS        \n------------------------------------------------------------\n(Intercept)            1.562 ***                            \n                      (0.094)                               \nmarry                  0.091 **      0.078 *       0.013    \n                      (0.032)       (0.032)       (0.029)   \nenrol                 -0.202 ***    -0.208 ***    -0.118 ***\n                      (0.025)       (0.027)       (0.024)   \nyeduc                  0.063 ***     0.056 ***    -0.002    \n                      (0.008)       (0.010)       (0.018)   \nas.factor(yeargr)2    -0.157 ***    -0.141 ***    -0.046    \n                      (0.034)       (0.036)       (0.038)   \nas.factor(yeargr)3    -0.197 ***    -0.165 **     -0.019    \n                      (0.050)       (0.053)       (0.052)   \nas.factor(yeargr)4    -0.316 ***    -0.276 ***    -0.136 *  \n                      (0.066)       (0.068)       (0.062)   \nas.factor(yeargr)5    -0.349 ***    -0.298 ***    -0.187 *  \n                      (0.089)       (0.089)       (0.074)   \nexp                    0.074 ***     0.073 ***              \n                      (0.012)       (0.012)                 \nexp^2                 -0.001 *      -0.001 *                \n                      (0.001)       (0.001)                 \n------------------------------------------------------------\ns_idios                0.341                                \ns_id                   0.279                                \nR^2                    0.440         0.414         0.025    \nAdj. R^2               0.439         0.357         0.022    \nNum. obs.           3100          3100          3100        \nNum. groups: id                                  268        \nRMSE                                               0.285    \n============================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nInterpretation:\n\nRE: Married observations have a significantly higher wage than unmarried observations.\nFE: If people marry, they experience an increase in wages afterwards. The effect is significant and slightly lower than the RE.\nFEIS: Accounting for the individual wage trend before marriage, we do not observe an increase in wages if people marry. The effect is small and non-significant.\n\nOverall, this indicates that there is a problem with non-parallel trends: Those with steeper wage trajectories are more likely to marry (or marry earlier).\nAs mentioned above, we can achieve the same by 1) manually calculating the individual specific trends and 2) including them as additional covariates in the model.\n\n\nCode\n### Individual predicted value of covariates\nvars &lt;- c(\"marry\", \"enrol\", \"yeduc\", \n          \"yeargr2\", \"yeargr3\", \"yeargr4\", \"yeargr5\")\nfor(v in vars){\n  fm &lt;- as.formula(paste(v, \"~ exp + expq\"))\n  pred_x &lt;- by(mwp[, c(v, \"exp\", \"expq\")],\n                mwp$id,\n                FUN = function(z) predict(lm(fm, data = z)))\n  pred_x &lt;- unlist(pred_x)\n  \n  mwp[, paste0(\"pred_\", v)] &lt;- pred_x\n}\n\nhead(mwp[, c(\"id\", \"pred_marry\", \"pred_enrol\")], n = 20)\n\n\n   id  pred_marry   pred_enrol\n1   1 -0.12205318  1.206579223\n2   1 -0.03068796  0.863033817\n3   1  0.03926230  0.716309276\n4   1  0.12611077  0.590568171\n5   1  0.22664093  0.490467488\n6   1  0.38990636  0.390403467\n7   1  0.54837205  0.340389443\n8   1  0.66317654  0.324200699\n9   1  0.87480284  0.326554307\n10  1  1.04489924  0.351640932\n11  1  1.23957011  0.399853176\n12  2  0.00000000  1.103938282\n13  2  0.00000000  0.503880518\n14  2  0.00000000  0.304456317\n15  2  0.00000000  0.176757843\n16  2  0.00000000  0.012061441\n17  2  0.00000000 -0.105551709\n18  2  0.00000000  0.004457309\n19  3  0.00000000  0.000000000\n20  3  0.00000000  0.000000000\n\n\nThis gives us individual-specific predicted values of each covariate based on and intercept, exp and expsq. Note that - in contrast to person-specific means - these predicted values (can) vary within a person.\nDo you know why person-id 2 has all zeros on the pre_marry variable?\nUsing these individual predicted values, we can retreive the FEIS estimates in a Mundlak-style model.\n\n\nCode\nwages.lmfeis &lt;- lm(lnw ~ marry + enrol + yeduc + as.factor(yeargr) +\n                     pred_marry + pred_enrol + pred_yeduc +\n                     pred_yeargr2 + pred_yeargr3 + pred_yeargr4 + pred_yeargr5,\n                   data = mwp)\n\nscreenreg(list(wages.feis, wages.lmfeis), digits = 3,\n          custom.model.names = c(\"FEIS\", \"Manual FEIS\"))\n\n\n\n==============================================\n                    FEIS          Manual FEIS \n----------------------------------------------\nmarry                  0.013         0.013    \n                      (0.029)       (0.043)   \nenrol                 -0.118 ***    -0.118 ** \n                      (0.024)       (0.037)   \nyeduc                 -0.002        -0.002    \n                      (0.018)       (0.022)   \nas.factor(yeargr)2    -0.046        -0.046    \n                      (0.038)       (0.055)   \nas.factor(yeargr)3    -0.019        -0.019    \n                      (0.052)       (0.080)   \nas.factor(yeargr)4    -0.136 *      -0.136    \n                      (0.062)       (0.097)   \nas.factor(yeargr)5    -0.187 *      -0.187    \n                      (0.074)       (0.121)   \n(Intercept)                          1.576 ***\n                                    (0.056)   \npred_marry                           0.269 ***\n                                    (0.048)   \npred_enrol                          -0.238 ***\n                                    (0.048)   \npred_yeduc                           0.079 ***\n                                    (0.022)   \npred_yeargr2                        -0.061    \n                                    (0.074)   \npred_yeargr3                        -0.012    \n                                    (0.092)   \npred_yeargr4                         0.169    \n                                    (0.110)   \npred_yeargr5                         0.347 ** \n                                    (0.132)   \n----------------------------------------------\nR^2                    0.025         0.359    \nAdj. R^2               0.022         0.356    \nNum. obs.           3100          3100        \nNum. groups: id      268                      \nRMSE                   0.285                  \n==============================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nNote, however, that this manual approach will lead to incorrect standard errors!\n\n\n\nDynamic treatment effects\nOften, we are not only interested in the overall treatment effect, but we also want to know how treatment effects unfold after a treatment. For example, how does happiness change around specific life course events (Clark and Georgellis 2013), or how do housing prices develop after the opening of an industrial plant (Currie et al. 2015)?\nThere are various ways of calculating how a treatment effect develops over time:\n\n\n\nVarious impact functions for event study designs from Brüderl/Ludwig 2019 teaching materials\n\n\nUsually, it is best to not impose a structural form, but rather to use dummy impact functions. However, even with this, there is an ongoing debate on what is the best choice of specification (Ludwig and Brüderl 2021), or see for instance blog post by Pedro H. C. Sant’Anna and Brantly Callaway.\nNote that these settings usually require a staggered treatment adoption: individuals are treated once, and afterwards remain treated\nThere are many cases where this does not apply. However, one can think about potential ways of artificially creating such designs:\n\nDichotomize continuous treatments (if theoretically plausible!)\nCreate id-period splits. E.g. if a person gets divorced, either drop from sample, or treat as a “new id” as a person can re-marry (note that this assumes that first and second marriage have equal effects).\n\n\nExample\nWe stick with our example and try to estimate how the wage changes around the year of marriage.\nTo do so, we first make sure the data is ordered by id and time\n\n\nCode\nmwp &lt;- mwp[order(mwp$id, mwp$year), ]\nhead(mwp[, 1:6], n = 20)\n\n\n   id year      lnw        exp        expq marry\n1   1 1981 1.934358  1.0769231   1.1597635     0\n2   1 1983 2.468140  3.0192308   9.1157551     0\n3   1 1984 2.162480  4.0384617  16.3091736     0\n4   1 1985 1.746280  5.0769229  25.7751465     0\n5   1 1986 2.527840  6.0961537  37.1630898     0\n6   1 1987 2.365361  7.5000000  56.2500000     0\n7   1 1988 2.467478  8.6730766  75.2222595     1\n8   1 1989 4.398027  9.4423075  89.1571732     1\n9   1 1990 2.822144 10.7307692 115.1494064     1\n10  1 1991 2.654965 11.6730766 136.2607117     1\n11  1 1992 2.665088 12.6730766 160.6068726     1\n12  2 1979 2.236233  0.9423077   0.8879438     0\n13  2 1981 2.916389  2.9615386   8.7707109     0\n14  2 1982 2.751646  3.8269231  14.6453409     0\n15  2 1983 2.629372  4.4807692  20.0772915     0\n16  2 1984 2.965442  5.5384617  30.6745586     0\n17  2 1985 2.890669  6.6538463  44.2736702     0\n18  2 1989 2.392579 11.1538458 124.4082794     0\n19  3 1979 2.456405  1.1730769   1.3761094     0\n20  3 1980 2.661142  2.1153846   4.4748521     0\n\n\nThen, we make sure that our data looks like a staggered treatment design. Are there people who jump from married to not married in the data?\n\n\nCode\n# Change in marriage status within an id\nmwp$fd_marry &lt;- ave(mwp$marry,\n                    mwp$id,\n                    FUN = function(x) x - dplyr::lag(x, 1, default = 0)) # 0 insteat of NA for 1st year\n\n# Mark observations starting with a negative fd value (jump from marry=1 to marry =0)\nmwp$notstag_marry &lt;- ave(ifelse(mwp$fd_marry == -1, 1, 0),\n                         mwp$id,\n                         FUN = function(x) cumsum(x))\ntable(mwp$fd_marry)\n\n\n\n   0    1 \n2896  204 \n\n\nCode\ntable(mwp$notstag_marry)\n\n\n\n   0 \n3100 \n\n\nLuckily, the dataset is already cleaned: there are only transitions into marriage, not out of marriage.\nNext we want to make sure if there are any individuals who already start with the treatment (who are married right from their first wave on).\nWe only want to have those in our sample who potentially can go from not-treated to treated!\n\n\nCode\nmwp &lt;- mwp[order(mwp$id, mwp$year), ] # just to be sure\n\n# Person year number\nmwp$pynr &lt;- ave(mwp$year,\n                mwp$id,\n                FUN = function(x) 1:length(x))\n\n# Marry status at first wave\nmwp$f_marry &lt;- ifelse(mwp$pynr == 1, mwp$marry, NA)\n\n# Distribute across individual, using mean and na.rm = TRUE\nmwp$f_marry &lt;- ave(mwp$f_marry,\n                   mwp$id,\n                   FUN = function(x) mean(x, na.rm = TRUE))\n\ntable(mwp$f_marry)\n\n\n\n   0 \n3100 \n\n\nAgain, someone has already done the job. There are no individuals who start married in the first wave.\nWe can also look at this graphically with panelView (mainly helpful for small N data):\n\n\nCode\npanelview(lnw ~ marry, \n          data = mwp, index = c(\"id\",\"year\"),\n          type = \"treat\", theme.bw = TRUE)\n\n\nTime is not evenly distributed (possibly due to missing data).\n\n\n\n\n\nAlright, so lets create a dummy impact function / a count variable around the treatment.\n\n\nCode\nmwp &lt;- mwp[order(mwp$id, mwp$year), ] # just to be sure!!\n\n# Function that creates distance to the treatment (assuming 0=control, 1=treated)\nimpfun &lt;- function(x, default = -99){\n  nas &lt;- which(is.na(x))  #save nas\n  ft &lt;- which(x == 1)[1]  #first teatment index\n  if(is.na(ft)){          #replicate default if never treated\n    count &lt;- rep(default, length(x))\n  }else{\n    ri &lt;- 1:length(x)       #running index\n    count &lt;- ri - ft        #distance to first treatment\n  }\n  if(length(nas) != 0){   #replace nas if any\n    count[nas] &lt;- NA\n  }\n  return(count)           #return counter\n}\n\n# Apply to each individual\nmwp$marry_if &lt;- ave(mwp$marry,\n                    mwp$id,\n                    FUN = function(x) impfun(x))\n\nhead(mwp[, c(\"id\", \"year\", \"marry\", \"marry_if\")], n = 50)\n\n\n   id year marry marry_if\n1   1 1981     0       -6\n2   1 1983     0       -5\n3   1 1984     0       -4\n4   1 1985     0       -3\n5   1 1986     0       -2\n6   1 1987     0       -1\n7   1 1988     1        0\n8   1 1989     1        1\n9   1 1990     1        2\n10  1 1991     1        3\n11  1 1992     1        4\n12  2 1979     0      -99\n13  2 1981     0      -99\n14  2 1982     0      -99\n15  2 1983     0      -99\n16  2 1984     0      -99\n17  2 1985     0      -99\n18  2 1989     0      -99\n19  3 1979     0      -99\n20  3 1980     0      -99\n21  3 1981     0      -99\n22  3 1982     0      -99\n23  3 1983     0      -99\n24  3 1984     0      -99\n25  3 1985     0      -99\n26  3 1986     0      -99\n27  3 1987     0      -99\n28  3 1988     0      -99\n29  3 1989     0      -99\n30  3 1993     0      -99\n31  3 1994     0      -99\n32  3 2000     0      -99\n33  4 1979     0       -2\n34  4 1981     0       -1\n35  4 1982     1        0\n36  4 1983     1        1\n37  4 1984     1        2\n38  4 1985     1        3\n39  4 1986     1        4\n40  4 1987     1        5\n41  4 1988     1        6\n42  4 1989     1        7\n43  4 1990     1        8\n44  4 1991     1        9\n45  4 1992     1       10\n46  4 1993     1       11\n47  5 1979     0       -6\n48  5 1980     0       -5\n49  5 1981     0       -4\n50  5 1982     0       -3\n\n\nWe can now use this time count function to estimate dynamic treatment effects.\nNote that we need to make to important decisions (blog post by Pedro H. C. Sant’Anna and Brantly Callaway):\n\nWhich dates to use a reference category\nHow many pre-treatment periods to include (to test for anticipation or potential pre-treatment differences)\n\nHere, re will just include three periods before marriage and use the rest as reference categories\n\n\nCode\n# Set all before -3 to -99\nmwp$marry_if[mwp$marry_if &lt; -3 & mwp$marry_if &gt; -99] &lt;- -99\n\n# Make factor with -99 as reference category\nmwp$marry_if &lt;- as.factor(mwp$marry_if)\nmwp$marry_if &lt;- relevel(mwp$marry_if, \"-99\")\n\n\nAnd we use this as our treatment variable in the FE estimator.\n\n\nCode\n# Standard marriage indicator\nwages.fe &lt;- plm(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n                + exp + I(exp^2), data = mwp, index = c(\"id\", \"year\"),\n                model = \"within\", effect = \"individual\")\n\n# with dummy impact function\nwages2.fe &lt;- plm(lnw ~ marry_if + enrol + yeduc + as.factor(yeargr)\n                 + exp + I(exp^2), data = mwp, index = c(\"id\", \"year\"),\n                 model = \"within\", effect = \"individual\")\n\n# add cluster robust SEs\nvcovx_fe2 &lt;- vcovHC(wages2.fe, cluster = \"group\", method = \"arellano\", type = \"HC3\")\nwages2.fe$vcov &lt;- vcovx_fe2\n\n\nsummary(wages2.fe)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = lnw ~ marry_if + enrol + yeduc + as.factor(yeargr) + \n    exp + I(exp^2), data = mwp, effect = \"individual\", model = \"within\", \n    index = c(\"id\", \"year\"))\n\nUnbalanced Panel: n = 268, T = 4-19, N = 3100\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-2.566752 -0.160001  0.010038  0.175003  2.011790 \n\nCoefficients:\n                      Estimate  Std. Error t-value  Pr(&gt;|t|)    \nmarry_if-3          0.03388225  0.03253942  1.0413 0.2978411    \nmarry_if-2          0.05166995  0.04238715  1.2190 0.2229466    \nmarry_if-1          0.09151624  0.04265046  2.1457 0.0319803 *  \nmarry_if0           0.08816919  0.05121551  1.7215 0.0852644 .  \nmarry_if1           0.15544954  0.05601346  2.7752 0.0055530 ** \nmarry_if2           0.15658061  0.06078860  2.5758 0.0100509 *  \nmarry_if3           0.16687443  0.06762257  2.4677 0.0136564 *  \nmarry_if4           0.14591461  0.07412203  1.9686 0.0491005 *  \nmarry_if5           0.14033773  0.07930392  1.7696 0.0768992 .  \nmarry_if6           0.15706194  0.08660097  1.8136 0.0698418 .  \nmarry_if7           0.13008248  0.09727237  1.3373 0.1812327    \nmarry_if8           0.11979150  0.10451397  1.1462 0.2518197    \nmarry_if9           0.14007172  0.10390453  1.3481 0.1777412    \nmarry_if10          0.11550144  0.11151584  1.0357 0.3004126    \nmarry_if11          0.20483579  0.12281306  1.6679 0.0954538 .  \nmarry_if12          0.10767466  0.12345702  0.8722 0.3831940    \nmarry_if13          0.10542488  0.13897395  0.7586 0.4481589    \nmarry_if14         -0.13934447  0.12815514 -1.0873 0.2769929    \nenrol              -0.20477142  0.02737332 -7.4807 9.833e-14 ***\nyeduc               0.05500238  0.01059840  5.1897 2.257e-07 ***\nas.factor(yeargr)2 -0.13830215  0.03594278 -3.8478 0.0001218 ***\nas.factor(yeargr)3 -0.16356110  0.05379641 -3.0404 0.0023847 ** \nas.factor(yeargr)4 -0.27248909  0.06839547 -3.9840 6.948e-05 ***\nas.factor(yeargr)5 -0.28895919  0.08907335 -3.2441 0.0011922 ** \nexp                 0.06728858  0.01357307  4.9575 7.565e-07 ***\nI(exp^2)           -0.00111675  0.00061151 -1.8262 0.0679250 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    559.75\nResidual Sum of Squares: 325.96\nR-Squared:      0.41767\nAdj. R-Squared: 0.35686\nF-statistic: 75.8727 on 26 and 2806 DF, p-value: &lt; 2.22e-16\n\n\nLet’s plot that\n\n\nCode\n# Set up results matrix\ncoef.df &lt;- data.frame(matrix(NA,\n                  nrow = (length(levels(mwp$marry_if)) - 1),\n                  ncol = 3))\ncolnames(coef.df) &lt;- c(\"time\", \"att\", \"se\")\n\n# paste results\ncoef.df$time &lt;- c(-3:14)\noutput &lt;- summary(wages2.fe)$coefficients\ncoef.df[, c(\"att\", \"se\")] &lt;- output[which(grepl(\"marry_if\", rownames(output))), 1:2]\n\n# 95% CI\ninterval2  &lt;-  -qnorm((1-0.95)/2)  # 95% multiplier\ncoef.df$ll &lt;- coef.df$att - coef.df$se*interval2\ncoef.df$ul &lt;- coef.df$att + coef.df$se*interval2\n\n# Plot\nzp1 &lt;- ggplot(coef.df[coef.df$time &lt; 7, ], \n              aes(x = time, y = att)) +  \n  geom_pointrange(aes(x = time, y = att, ymin = ll, ymax = ul),\n                                lwd = 1, fatten = 2) +\n  geom_line(aes(x = time, y = att)) +\n  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2, lwd = 1) +\n  geom_vline(xintercept = -0.5, colour = \"black\", lty = 1, lwd = 1) +\n  theme_bw()\nzp1\n\n\n\n\n\nAn interesting finding here. There is a positive anticipation effect: “The anticipation of marriage already increases the husbands wage”.\nIs this plausible?\nFEIS\nObviously, we can also use these dummy impact function in other estimators.\n\n\nCode\n### FEIS with dummy impact function\nwages2.feis &lt;- feis(lnw ~ marry_if + enrol + yeduc + as.factor(yeargr)\n                   | exp + I(exp^2), \n                   data = mwp, id = \"id\",\n                  robust = TRUE)\nsummary(wages2.feis)\n\n\n\n\nCall:\nfeis(formula = lnw ~ marry_if + enrol + yeduc + as.factor(yeargr) | \n    exp + I(exp^2), data = mwp, id = \"id\", robust = TRUE)\n\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-2.0385140 -0.1070148  0.0063363  0.1116305  1.9422823 \n\nCoefficients:\n                      Estimate  Std. Error t-value  Pr(&gt;|t|)    \nmarry_if-3         -0.01009512  0.03282428 -0.3076   0.75845    \nmarry_if-2         -0.01269923  0.05387564 -0.2357   0.81368    \nmarry_if-1          0.00945697  0.06556529  0.1442   0.88533    \nmarry_if0          -0.00934465  0.08669423 -0.1078   0.91417    \nmarry_if1           0.05239527  0.10439373  0.5019   0.61579    \nmarry_if2           0.06024667  0.11996538  0.5022   0.61558    \nmarry_if3           0.05481404  0.14066586  0.3897   0.69681    \nmarry_if4           0.03469671  0.15894441  0.2183   0.82722    \nmarry_if5           0.04209049  0.18134196  0.2321   0.81648    \nmarry_if6           0.07219889  0.20694589  0.3489   0.72721    \nmarry_if7           0.05371600  0.22543835  0.2383   0.81169    \nmarry_if8           0.00422209  0.26924683  0.0157   0.98749    \nmarry_if9           0.05496285  0.29238662  0.1880   0.85091    \nmarry_if10          0.08980275  0.34176479  0.2628   0.79276    \nmarry_if11          0.18093806  0.40047134  0.4518   0.65145    \nmarry_if12          0.11491753  0.44824664  0.2564   0.79769    \nmarry_if13          0.07479506  0.51906834  0.1441   0.88544    \nmarry_if14          0.14775390  0.57381382  0.2575   0.79682    \nenrol              -0.11902058  0.02363290 -5.0362 5.122e-07 ***\nyeduc              -0.00082298  0.01772763 -0.0464   0.96298    \nas.factor(yeargr)2 -0.04618140  0.04080643 -1.1317   0.25787    \nas.factor(yeargr)3 -0.01934802  0.05387474 -0.3591   0.71953    \nas.factor(yeargr)4 -0.13656650  0.06179928 -2.2098   0.02722 *  \nas.factor(yeargr)5 -0.18414365  0.07407322 -2.4860   0.01299 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCluster robust standard errors\nSlope parameters:  exp, I(exp^2) \nTotal Sum of Squares:    190.33\nResidual Sum of Squares: 184.71\nR-Squared:      0.029511\nAdj. R-Squared: 0.021939\n\n\nCode\n### Plot\n\n# Set up results matrix\ncoef2.df &lt;- data.frame(matrix(NA,\n                  nrow = (length(levels(mwp$marry_if)) - 1),\n                  ncol = 3))\ncolnames(coef2.df) &lt;- c(\"time\", \"att\", \"se\")\n\n# paste results\ncoef2.df$time &lt;- c(-3:14)\noutput &lt;- summary(wages2.feis)$coefficients\ncoef2.df[, c(\"att\", \"se\")] &lt;- output[which(grepl(\"marry_if\", rownames(output))), 1:2]\n\n# 95% CI\ninterval2  &lt;-  -qnorm((1-0.95)/2)  # 95% multiplier\ncoef2.df$ll &lt;- coef2.df$att - coef2.df$se*interval2\ncoef2.df$ul &lt;- coef2.df$att + coef2.df$se*interval2\n\n# Plot\nzp2 &lt;- ggplot(coef2.df[coef2.df$time &lt; 7, ], \n              aes(x = time, y = att)) +  \n  geom_pointrange(aes(x = time, y = att, ymin = ll, ymax = ul),\n                                lwd = 1, fatten = 2) +\n  geom_line(aes(x = time, y = att)) +\n  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2, lwd = 1) +\n  geom_vline(xintercept = -0.5, colour = \"black\", lty = 1, lwd = 1) +\n  theme_bw()\nzp2\n\n\n\n\n\nThis gives us what we already expected: using FEIS, the marital wage premium disappears.\n\n\n\nDynamic Diff-in-Diff\nRemember, we have defined the 2 \\(\\times\\) 2 Diff-in-Diff as:\n\\[\ny_{it} = \\alpha + \\gamma D_{i} + \\lambda Post_{t} + \\delta_{DD} (D_{i} \\times Post_{t}) + \\upsilon_{it},\n\\]\nwhich we can easily estimate as:\n\\[\n\\hat{\\delta}_{DD} = \\mathrm{E}(\\Delta y_{T}) - \\mathrm{E}(\\Delta y_{C}) = (\\mathrm{E}(y_{T}^{post}) - \\mathrm{E}(y_{T}^{pre})) - (\\mathrm{E}(y_{C}^{post}) - \\mathrm{E}(y_{C}^{pre})).\n\\]\nMoreover, we have written the twoways FE estimator as:\n\\[\ny_{it} = \\beta_{TWFE} D_{it} + \\alpha_i + \\zeta_t + \\epsilon_{it},\n\\] In a setting with only two time periods, a binary treatment, and all observations untreated in \\(t=1\\), the Diff-in-Diff estimator equals the twoways FE estimator \\(\\hat{\\delta}_{DD} = \\hat{\\beta}_{TWFE}\\).\nHowever, it is more complicated when we go beyond the 2 \\(\\times\\) 2 setting. There is an ongoing discussion on how the Difference in Differences estimator relates to the two-ways FE estimator when treatment timing varies: different individuals receive the treatment at different periods.\nAssume we can divide our setting into treatment groups (treated vs. control) and into timing groups (every observation treated in the same period form a timing group).\nGoodman-Bacon (2021) shows that the two-ways FE is a weighted average of all possible two-group/two-period DD estimators. The weights determine how much each of these single combinations contributes to the two-ways FE are determined by the group size (e.g. how long do we observe each combination before and after treatment) and the variance in the treatment.\n\n\n\nTwo-ways FE and DD with varying treatment timing (Goodman-Bacon 2021).\n\n\nIn the example above we have three groups: 1) control / never treated (\\(C\\)), 2) early treated (at period \\(k\\)), and 3) late treated (at period \\(l\\)). Those who are treated in later time periods are not only compared to those who are never treated but also to those who have already been treated in earlier periods.\nGoodman-Bacon (2021) shows that this setting with three treatment groups consists of 4 possible 2 \\(\\times\\) 2 Diff-in-Diff settings.\nPanels A) and B) compare \\(j = k,l\\) against control group \\(C\\), and can be written as: \\[\n\\hat{\\delta}_{DD}^{jC} = (\\mathrm{E}(y_{j}^{post(j)}) - \\mathrm{E}(y_{j}^{pre(j)})) - (\\mathrm{E}(y_{C}^{post}) - \\mathrm{E}(y_{C}^{pre})), ~\\text{with} ~ j = k,l.\n\\]\nPanel C) compares early treated \\(k\\) against untreated periods of late treated \\(l\\). Note that we replace the \\(POST\\) period with period between treatment of the early treated and the late treated \\(MID(k,l)\\) \\[\n\\hat{\\delta}_{DD}^{kl} = (\\mathrm{E}(y_{k}^{MID(k,l)}) - \\mathrm{E}(y_{k}^{pre(k)})) - (\\mathrm{E}(y_{l}^{MID(k,l)}) - \\mathrm{E}(y_{l}^{pre(k)})).\n\\]\nPanel D) compares late treated \\(l\\) against already treated periods of early treated \\(k\\). Note that we replace the \\(PRE\\) period with period between treatment of the early treated and the late treated \\(MID(k,l)\\) \\[\n\\hat{\\delta}_{DD}^{lk} = (\\mathrm{E}(y_{l}^{post(l)}) - \\mathrm{E}(y_{l}^{MID(k,l)})) - (\\mathrm{E}(y_{k}^{post(l)}) - \\mathrm{E}(y_{k}^{MID(k,l)})).\n\\]\nTHE twoways FE estimator can now be recovered as a weighted combinations of these four \\(2\\times2\\) Diff-in-Diff estimators.\nThe weights of each of them depend on\n\nthe number of periods each subsample uses, and\nthe amount of treatment variance within each subsample.\n\nDefine \\(\\bar{D}_{kl}\\) as the mean of \\(D_{it}\\) in the subsample that compares groups \\(k\\) and \\(l\\) - denoting the share of time the group spends treated -, and the relative size of each group in the pair \\(n_{kl} = \\frac{n_k}{n_k + n_l}\\). Further \\(\\hat{V}_D\\) equals the overal variance in \\(D_{it}\\), and \\(\\hat{V}_D^{jC}\\) denotes the amount of identifying variation for comparison of groups \\(j\\) and \\(C\\). Then, the weights are given by:\n\\[\nW_{jC} = \\frac{(n_j + n_C)^2 \\hat{V}_D^{jC}}{\\hat{V}_D}, ~\\text{with}~ \\hat{V}_D^{jC} = n_{jC}(1-n_{jC}) \\bar{D}_j(1-\\bar{D}_j), ~for~ j = k,l.\n\\]\n\\[\nW_{kl} = \\frac{\\bigl( (n_k + n_l)(1-\\bar{D}_l) \\bigr)^2 \\hat{V}_D^{kl}}{\\hat{V}_D}, ~\\text{with}~ \\hat{V}_D^{kl} = n_{kl}(1-n_{kl}) \\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l} \\frac{1-\\bar{D}_k}{1-\\bar{D}_l}.\n\\]\n\\[\nW_{lk} = \\frac{\\bigl( (n_l + n_k)\\bar{D}_k \\bigr)^2 \\hat{V}_D^{lk}}{\\hat{V}_D}, ~\\text{with}~ \\hat{V}_D^{lk} = n_{lk}(1-n_{lk}) \\frac{\\bar{D}_l}{\\bar{D}_k} \\frac{\\bar{D}_k-\\bar{D}_l}{\\bar{D}_k}.\n\\]\nIf group-sizes are equal, then the treatment variance \\(\\hat{V}_D\\) in each combination group depends on the timing of the treatment. The variance get larger the more similar the groups sizes \\(n_{ju}\\), and the more central (in the middle of the observation period) the treatment timing (\\(\\bar{D}_k, \\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l}, \\frac{\\bar{D}_l}{\\bar{D}_k}\\) close to 0.5).\nNote that these weights are always positive.\nQUESTION: Do you know which of the above groups A), B), C), D) get the highest weights in the example. Why?\n\nBiased Estimates\nSo why (or when) could this lead to problems with the twoways FE estimator? de Chaisemartin and D’Haultfœuille (2020) and Sun and Abraham (2021) criticize the TWFE on the grounds of negative weights of some subgroups / sub-effects, arguing that this may induce substantial bias in the case of heterogeneous treatment effects.\nGoodman-Bacon (2021) also explains the observation of these “negative” weights:\n\n“Negative weights only arise when average treatment effects vary over time. The DD decomposition shows why: when already-treated units act as controls, changes in their outcomes are subtracted and these changes may include time-varying treatment effects. This does not imply a failure of the design in the sense of non-parallel trends in counterfactual outcomes, but it does suggest caution when using TWFE estimators to summarize treatment effects.”\n\nBasically, if the treatment effect varies over time (e.g. treatment effect grows over time) TWFE might be biased because early treated groups (with an increasing treatment effect) are used as control groups for late treated groups, thereby masking the treatment effect of these late treated groups (which might however receive a high weight for the overall treatment effect).\nEspecially for “trend-breaking” treatment effects like in the following figure, this will lead to biased estimates of the average treatment effect (Goodman-Bacon 2021; Meer and West 2016).\n\n\n\nBias of two-ways FE with trend-breaking treatment (Goodman-Bacon 2021).\n\n\nIn the middle period, we compare the trend in the early treated with the not-yet treated periods of the late treatment group, and we see the divergence between those two groups (a positive treatment effect). However, for the late treated (right art of the figure), the earlier treated are the control cases (which already includes a trend-breaking treatment effect). For the late treatment group, and we do not observe a positive, but actually a negative treatment effect as we erroneously have the treatment effect in the control group!!!\nThe trend-breaking treatment case is obviously an exception (this would be a really strong treatment effect). However, a similar problem arises with less strong dynamic treatment effects. Below, we set up panel data with three groups and and 12 periods. There is a never-treated group, and early-treated group and a late-treated group. As shown in Figure 1, we could either have A) a statistic treatment effect that changes the outcome level from one period to the other, or B) assume that the treatment effect unfolds dynamically over six periods before it stabilises.\n\n\nCode\n#############################\n### Example: Callaway DID ###\n#############################\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(ggpubr)\nlibrary(dplyr)\n\n### Set up six individuals with age and happiness\nN &lt;- 3\nT &lt;- 12\n\n# id and time\ndf2 &lt;- data.frame(matrix(NA, ncol = 2, nrow = N*T))\nnames(df2) &lt;- c(\"id\", \"time\")\n\ndf2$id &lt;- rep(1:N, each = T)\ndf2$time &lt;- rep(1:T, times = N)\ndf2$idname &lt;- factor(df2$id, levels = c(1:N), labels = paste(\"Person\", c(1:N)))\n\n# Treatment group\ndf2$D &lt;- 0 \ndf2$D[(1*T + 1):(2*T)] &lt;- 1\ndf2$D[(2*T + 1):(3*T)] &lt;- 2\n\ndf2$Treatment &lt;- ifelse(df2$time &gt;= 4 & df2$D == 1, 1, 0)\noo &lt;- which(df2$time &gt;= 8 & df2$D == 2)\ndf2$Treatment[oo] &lt;- 1\n\n# Starting wage\nstw &lt;- c(2300, 3000, 4000)\n\n# Dynamic treatment\ndf2$DT &lt;- as.numeric(as.character(df2$Treatment))\ndf2$DT_time &lt;- ave(df2$DT,\n                   df2$id,\n                   FUN = function(x) cumsum(x))\n\nfor(i in 1:max(df2$DT_time)){\n  X &lt;- ifelse(df2$DT_time == i, 1, 0)\n  if(i == 1){\n    Treatmat &lt;- X\n  }else{\n    Treatmat &lt;- cbind(Treatmat, X)\n  }\n}\n\nbeta &lt;- c(100, 300, 500, 400, 300, 200, 100, 100, 100)\n\n\n# wage equation\ndf2$wage &lt;- unname(rep(stw, each = T)) + (df2$time - 1)*50 + Treatmat %*% beta\n\n# alternative wage equation\ndf2$alt_wage &lt;- unname(rep(stw, each = T)) + (df2$time - 1)*50 + df2$Treatment * 500\n\n# counterfactual\ndf2$wage0 &lt;- unname(rep(stw, each = T)) + (df2$time - 1)*50\n\ndf2$Treatment &lt;- as.factor(df2$Treatment)\n\n# Add comparison groups\ndf2$wage2 &lt;- ave(df2$wage,\n                 df2$id,\n                 FUN = function(x) dplyr::lag(x))\ndf2$time2 &lt;- ave(df2$time,\n                 df2$id,\n                 FUN = function(x) dplyr::lag(x))\noo &lt;- which(df2$Treatment == 1)\ndf2$wage2[oo] &lt;- ave(df2$wage2[oo],\n                 df2$id[oo],\n                 FUN = function(x) x[1])\ndf2$time2[oo] &lt;- ave(df2$time2[oo],\n                 df2$id[oo],\n                 FUN = function(x) x[1])\n\n# for alpha\ndf2$D2 &lt;- df2$D\ndf2$D3 &lt;- df2$D\ndf2$D2[which(df2$id == 2 & df2$time &gt;= 8)] &lt;- 0\ndf2$D3[which(df2$id == 2 & df2$time &gt;= 7)] &lt;- 0\n\n\n### Plot the Callaway Sant Anna Comparisons ###\n\n\nzp1 &lt;- ggplot(df2, aes(time, wage)) +\n  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D)), lty = \"solid\", \n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point( aes(x = time, y = wage, fill = Treatment, shape = Treatment, alpha = as.factor(D)), \n              size = 4, stroke = 1.5, color = \"white\") +\n  scale_alpha_manual(values = c(1, 1, 0.2), guide = \"none\") +\n  theme_classic() +\n  scale_x_continuous( breaks = seq(1, 12, 2))  +\n  scale_fill_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_color_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_shape_manual(values = c(21, 24)) +\n  ggtitle(\"Group 1: 11 2x2 DID estimates vs. never-treated\") +\n  theme(legend.position = c(0.05,0.95), legend.justification = c(\"left\", \"top\"),\n        legend.background = element_blank(),\n        text = element_text(size = 14),\n        legend.box.background = element_rect(colour = \"black\")) +\n  geom_curve(aes(x = time2, y = wage2, xend = time, yend = wage, color = Treatment), \n             curvature = 0.3, data = df2[df2$D == 1 & !is.na(df2$wage2), ])\n\n\nzp2 &lt;- ggplot(df2, aes(time, wage)) +\n  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D3)), lty = \"solid\", \n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point( aes(x = time, y = wage,  fill = Treatment, shape = Treatment, alpha = as.factor(D2)), \n              size = 4, stroke = 1.5,  color = \"white\") +\n  scale_shape_manual(values = c(21, 24)) +\n  scale_alpha_manual(values = c(0.2, 1, 0.2), guide = \"none\") +\n  scale_fill_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_color_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  geom_line(aes(x = time, y = wage, group = id, ), lty = \"solid\", colour = \"black\", lwd = 1,\n            data = df2[df2$D2 == 2 & df2$time &lt;= 7, ]) + \n  geom_point( aes(x = time, y = wage, fill = Treatment, shape = Treatment),\n              data = df2[df2$D == 2 & df2$time &lt;= 7, ],\n              size = 4, stroke = 1.5,  color = \"white\") +\n  scale_shape_manual(values = c(21, 24)) +\n  theme_classic() +\n  scale_x_continuous( breaks = seq(1, 12, 2))  +\n  ggtitle(\"Group 1: 6 2x2 DID estimates vs. not-yet-treated\") +\n  theme(legend.position = c(0.05,0.95), legend.justification = c(\"left\", \"top\"),\n        legend.background = element_blank(),\n        text = element_text(size = 14),\n        legend.box.background = element_rect(colour = \"black\")) +\n  geom_curve(aes(x = time2, y = wage2, xend = time, yend = wage, color = Treatment), \n             curvature = 0.3, data = df2[df2$D == 1 & !is.na(df2$wage2) & df2$time &lt;= 7, ])\n\n\nzp3 &lt;- ggplot(df2, aes(time, wage)) +\n  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D)), lty = \"solid\", \n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point(aes(x = time, y = wage, fill = Treatment, shape = Treatment, alpha = as.factor(D)), \n              size = 4, stroke =1.5, color = \"white\") +\n  scale_color_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_fill_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_shape_manual(values = c(21, 24)) +\n  scale_alpha_manual(values = c(1, 0.2, 1), guide = \"none\") +\n  theme_classic() +\n  scale_x_continuous( breaks = seq(1, 12, 2))  +\n  ggtitle(\"Group 2: 11 2x2 DID estimates vs. never-treated\") +\n  theme(legend.position = c(0.05,0.95), legend.justification = c(\"left\", \"top\"),\n        text = element_text(size = 14),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  geom_curve(aes(x = time2, y = wage2, xend = time, yend = wage, color = Treatment), \n             curvature = 0.3, data = df2[df2$D == 2 & !is.na(df2$wage2), ])\n\n\ntext &lt;- paste(\"DOES NOT compare\\n\",\n                             \"group 2 (late treatment) vs.\\n\",\n                             \"the already treated periods of group 1\")\nzp4 &lt;- ggplot() + \n  annotate(\"text\", x = 4, y = 25, size=8, label = text, color = \"red\") + \n  theme_void()\n\n\n\n\n### Plot the Forbidden Comparisons Comparisons ###\n\ndf2$D4 &lt;- df2$D\ndf2$D4[df2$time &lt;= 3] &lt;- 0\n\n# Feed forward\ndf2$l_Treatment &lt;- ave(df2$Treatment,\n                       df2$id,\n                       FUN = function(x) dplyr::lead(x))\n\nfp1 &lt;- ggplot(df2, aes(time, wage)) +\n  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D4)), lty = \"solid\", \n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point(aes(x = time, y = wage, fill = Treatment, shape = Treatment, alpha = as.factor(D4)), \n              size = 4, stroke =1.5, color = \"white\") +\n  geom_line(aes(x = time, y = wage0, group = id), \n            data = df2[df2$l_Treatment == 1 | df2$Treatment == 1, ],\n            lty = \"dashed\", alpha = 0.2,\n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point(aes(x = time, y = wage0), \n             data = df2[df2$Treatment == 1, ],\n             size = 3, stroke = 1, alpha = 0.2, shape = 21) +\n  geom_blank(aes(x = time, y = alt_wage, fill = Treatment, shape = Treatment, alpha = as.factor(D4)), \n              size = 4, stroke =1.5, color = \"white\") +\n  scale_color_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_fill_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_shape_manual(values = c(21, 24)) +\n  scale_alpha_manual(values = c(0.2, 1, 1), guide = \"none\") +\n  theme_classic() +\n  scale_x_continuous( breaks = seq(1, 12, 2))  +\n  ggtitle(\"B) Dynamic treatment effect \\n    Late-treated vs. already-treated\") +\n  theme(legend.position = c(0.05,0.95), legend.justification = c(\"left\", \"top\"),\n        text = element_text(size = 14),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) \n\n\n\nfp2 &lt;- ggplot(df2, aes(time, alt_wage)) +\n  geom_line(aes(x = time, y = alt_wage, group = id, alpha = as.factor(D4)), lty = \"solid\", \n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point(aes(x = time, y = alt_wage, fill = Treatment, shape = Treatment, alpha = as.factor(D4)), \n              size = 4, stroke =1.5, color = \"white\") +\n  geom_line(aes(x = time, y = wage0, group = id), \n            data = df2[df2$l_Treatment == 1 | df2$Treatment == 1, ],\n            lty = \"dashed\", alpha = 0.2,\n            colour = \"black\", lwd = 1, show.legend = FALSE) + \n  geom_point(aes(x = time, y = wage0), \n             data = df2[df2$Treatment == 1, ],\n             size = 3, stroke = 1, alpha = 0.2, shape = 21) +\n  scale_color_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_fill_manual(values = c(\"#85144b\", \"#0074D9\")) +\n  scale_shape_manual(values = c(21, 24)) +\n  scale_alpha_manual(values = c(0.2, 1, 1), guide = \"none\") +\n  theme_classic() +\n  scale_x_continuous( breaks = seq(1, 12, 2))  +\n  ggtitle(\"A) Static treatment effect \\n    Late-treated vs. already-treated\") +\n  ylab(\"wage\") +\n  theme(legend.position = c(0.05,0.95), legend.justification = c(\"left\", \"top\"),\n        text = element_text(size = 14),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) \n\n\n\n\nCode\nzp &lt;- ggarrange(fp2, fp1  + rremove(\"legend\"), \n                ncol = 2, nrow = 1)\n\ncairo_ps(file = paste(\"fig/\", \"Forbidden.eps\", sep=\"\"), width = 11, height = 4.5, \n          bg = \"white\", family = \"Times New Roman\")\npar(mar = c(0, 0, 0, 0))\npar(mfrow = c(1, 1), oma = c(0, 0, 0, 0))\nzp\ndev.off()\n\njpeg(file = paste(\"fig/\", \"Forbidden.jpeg\", sep=\"\"), width = 11, height = 4.5, \n    units = \"in\", res = 300, type = \"cairo\",\n          bg = \"white\", family = \"Times New Roman\")\npar(mar = c(0, 0, 0, 0))\npar(mfrow = c(1, 1), oma = c(0, 0, 0, 0))\nzp\ndev.off()\n\n\n\n\n\nFigure 1: Forbidden comparison of late-treated vs already-treated\n\n\nThe problem now arises when FE compares the late treated as treatment group vs. the already treated as control group. In the static treatment case (Figure 1 A), everything is fine as the already-treated run parallel to counterfactual of the late-treated. We can thus use them as control. However, in the dynamic treatment case (Figure 1 B), the “control” group of already-treated experience still an ongoing dynamic treatment effect when the late-treated are treated. They are thus not running parallel to the counterfactual of the late-treated group. This comparison is thus also called the forbidden comparison (Roth et al. 2023).\nOne way of counteracting this problem is to use event study / impact function designs (see above) to explicitly model time varying treatment effects.\n\n\nCallaway & Sant’Anna dynamic Diff-in-Diff\nA second way is the application of flexible Diff-in-Diff estimators as proposed by Callaway and Sant’Anna (2020).\nLet’s start again with the \\(2 \\times 2\\) Diff-in-Diff estimator as\n\\[\n\\delta = \\mathrm{E}(\\Delta y_{T}) - \\mathrm{E}(\\Delta y_{C}) = (\\mathrm{E}(y_{T}^{post}) - \\mathrm{E}(y_{T}^{pre})) - (\\mathrm{E}(y_{C}^{post}) - \\mathrm{E}(y_{C}^{pre})),\n\\] where \\(\\delta\\) is the average treatment effect on the treated (ATT).\nCallaway and Sant’Anna (2020) show that we can generalize this \\(2 \\times 2\\) Diff-in-Diff to a mutlti-group and multi-timing setting by computing group-time average treatment effects. We group all treatment units which receive treatment at the same period into a common group \\(g\\), and for each treatment-group \\(g\\) and time period \\(t\\) we estimate group-specific and time-specific ATTs:\n\\[\n\\delta_{g,t} = \\mathrm{E}(\\Delta y_{g}) - \\mathrm{E}(\\Delta y_{C}) = (\\mathrm{E}(y_{g}^{t}) - \\mathrm{E}(y_{g}^{g-1})) - (\\mathrm{E}(y_{C}^{t}) - \\mathrm{E}(y_{C}^{g-1})),\n\\]\nwhere the control group can either be the never-treated or the not-yet-treated. As shown in Figure 2 by the convex lines, this means, we estimate an individual treatment effect for each combination of treatment-timing-group and control group.\nObviously, this gives us a large number of different treatment effects. So, in a second step, we re-aggregate these individual combinations back to group or time averaged treatment effect. In an event study design, Callaway and Sant’Anna (2020) propose the following dynamic treatment effect for each period \\(e\\) after the treatment:\n\\[\n  \\theta_D(e) := \\sum_{g=1}^G \\mathbf{1} \\{ g + e \\leq T \\} \\delta(g,g+e) P(G=g | G+e \\leq T),\n\\] where \\(e\\) specifies for how long a unit has been exposed to the treatment. It’s basically the average effects across all treatment-timing groups at the period \\(e\\) after treatment. From here, one can easily calculate the cumulative effect or the overall aggregated effect.\nConsider the situation in Figure 2, where we have a control group of never-treated units, one treatment group that is treated early (group 1) and one treatment group that is treated late (group 2). As shown below, with \\(T=12\\) we can estimate 11 2 \\(\\times\\) 2 Diff-in-Diff estimates of group 1 against the never treated, we can estimate 6 2 \\(\\times\\) 2 Diff-in-Diff estimates of group 1 against the not-yet treated (late treatment group), and we can estimate 11 2 \\(\\times\\) 2 Diff-in-Diff estimates of group 2 against the never treated.\nNote that the control period for all treated periods by default is set to the period before the treatment happened in each group. For group 1 this is period 3, and for group 2 this is period 7. This makes only sense if there is no treatment anticipation. Obviously, we can also use other (earlier) periods if we assume treatment anticipation.\n\n\nCode\nzp &lt;- ggarrange(zp1, zp2  + rremove(\"legend\"), \n                zp3  + rremove(\"legend\"), zp4,\n                ncol = 2, nrow = 2)\n\ncairo_ps(file = paste(\"fig/\", \"DiD.eps\", sep=\"\"), width = 11, height = 8, \n          bg = \"white\", family = \"Times New Roman\")\npar(mar = c(0, 0, 0, 0))\npar(mfrow = c(1, 1), oma = c(0, 0, 0, 0))\nzp\ndev.off()\n\njpeg(file = paste(\"fig/\", \"DiD.jpeg\", sep=\"\"), width = 11, height = 8, \n    units = \"in\", res = 300, type = \"cairo\",\n          bg = \"white\", family = \"Times New Roman\")\npar(mar = c(0, 0, 0, 0))\npar(mfrow = c(1, 1), oma = c(0, 0, 0, 0))\nzp\ndev.off()\n\n\n\n\n\nFigure 2: Multiple 2x2 DiD comparisons with dynamic DiD estimators\n\n\nFor a more detailled introdution see Callaway and Sant’Anna (2020) or the respective package introcution.\nAssumptions:\n\nStaggered treatment adoption: once a unit has been treated, it remains treated thereafter (see also the note above).\nParallel trends assumption\n\n\nbased on never-treated: treated units would have had parallel trends to never-treated if they would not have experienced treatment. In many empirical settings this is a strong assumption (why do some units never experience treatment?).\nbased on not-yet-treated: treated units would have had parallel trends to not-yet-treated if they would not have experienced treatment. This assumption might be a bit more likely to hold.\n\n\nNo treatment anticipation\n\n\nbased on never-treated: We can relax the assumption by either back-dating the treatment or include the appropriate pre-treatment periods in our results to inspect anticipation qualitatively.\nbased on not-yet-treated: If there is treatment anticipation, our control group will be confounded by the treatment effect. A violation of the assumption can lead to stronger bias if we use the not-yet-treated as control group.\n\nTrade-off: If assumption 2) is likely to hold, we can use only the never-treated as controls to relax assumption 3). If assumption 3) is likely to hold, we can include the not-yet-treated as control to relax assumption 2).\n\n\nExample\nHow does that look in our marriage example? To estimate the dynamic DD we use the did package, as describes in more detail here or in the authors blog.\nNote: This package works with staggered treatment adoption! We thus should perform all the steps we have performed above to restrict and prepare the data!\nAs a first step, we need to define a variable that contains the treatment timing: the first year an ever-treated individual is treated.\nThis should be a positive number for all observations in treated groups. It defines which “group” a unit belongs to. It should be 0 for units in the untreated group.\n\n\nCode\n# treatment timing = year if married\nmwp$treat_timing &lt;- ifelse(mwp$marry == 1, mwp$year, NA)\n\n# set never treated to zero\nmwp$treat_timing[mwp$evermarry == 0] &lt;- 0\n\n# if married is not NA, used min year per id (removing NAs)\nmwp$treat_timing[!is.na(mwp$marry)] &lt;- ave(mwp$treat_timing[!is.na(mwp$marry)],\n                                           mwp$id[!is.na(mwp$marry)],\n                                           FUN = function(x) min(x, na.rm = TRUE))\n\n\nhead(mwp[, c(\"id\", \"year\", \"marry\", \"evermarry\", \"treat_timing\")], n = 35)\n\n\n   id year marry evermarry treat_timing\n1   1 1981     0         1         1988\n2   1 1983     0         1         1988\n3   1 1984     0         1         1988\n4   1 1985     0         1         1988\n5   1 1986     0         1         1988\n6   1 1987     0         1         1988\n7   1 1988     1         1         1988\n8   1 1989     1         1         1988\n9   1 1990     1         1         1988\n10  1 1991     1         1         1988\n11  1 1992     1         1         1988\n12  2 1979     0         0            0\n13  2 1981     0         0            0\n14  2 1982     0         0            0\n15  2 1983     0         0            0\n16  2 1984     0         0            0\n17  2 1985     0         0            0\n18  2 1989     0         0            0\n19  3 1979     0         0            0\n20  3 1980     0         0            0\n21  3 1981     0         0            0\n22  3 1982     0         0            0\n23  3 1983     0         0            0\n24  3 1984     0         0            0\n25  3 1985     0         0            0\n26  3 1986     0         0            0\n27  3 1987     0         0            0\n28  3 1988     0         0            0\n29  3 1989     0         0            0\n30  3 1993     0         0            0\n31  3 1994     0         0            0\n32  3 2000     0         0            0\n33  4 1979     0         1         1982\n34  4 1981     0         1         1982\n35  4 1982     1         1         1982\n\n\n\n\nCode\n# estimate group-time average treatment effects using att_gt method\nwages.attgt &lt;- att_gt(yname = \"lnw\",\n                      tname = \"year\",\n                      idname = \"id\",\n                      gname = \"treat_timing\",\n                      #xformla = ~ enrol + yeduc + exp + I(exp^2), # note that we omit the yeargroup here\n                      data = mwp,\n                      allow_unbalanced_panel = TRUE,\n                      control_group = \"notyettreated\",\n                      est_method = \"ipw\"\n                        )\n\n\nWarning in pre_process_did(yname = yname, tname = tname, idname = idname, : Be aware that there are some small groups in your dataset.\n  Check groups: 1980,1982,1990,1994.\n\n\nWarning in compute.att_gt(dp): No units in group 1980 in time period 1996\n\n\nWarning in compute.att_gt(dp): No units in group 1980 in time period 1998\n\n\nWarning in compute.att_gt(dp): No units in group 1980 in time period 2000\n\n\nWarning in compute.att_gt(dp): No units in group 1981 in time period 1996\n\n\nWarning in compute.att_gt(dp): No units in group 1981 in time period 1998\n\n\nWarning in compute.att_gt(dp): No units in group 1981 in time period 2000\n\n\nWarning in compute.att_gt(dp): No units in group 1982 in time period 1994\n\n\nWarning in compute.att_gt(dp): No units in group 1982 in time period 1996\n\n\nWarning in compute.att_gt(dp): No units in group 1982 in time period 1998\n\n\nWarning in compute.att_gt(dp): No units in group 1982 in time period 2000\n\n\nWarning in compute.att_gt(dp): No units in group 1983 in time period 1998\n\n\nWarning in compute.att_gt(dp): No units in group 1983 in time period 2000\n\n\nWarning in compute.att_gt(dp): No units in group 1984 in time period 2000\n\n\nWarning in compute.att_gt(dp): No units in group 1985 in time period 2000\n\n\nWarning in compute.att_gt(dp): No units in group 1994 in time period 1979\n\n\nOne huge advantage: We do not need to make a decision about which periods (before treatment) we want to include, and which observations go into the reference category.\nHowever, we get a lot of individual treatment effects.\n\n\nCode\n# Show the group-time specific estimates\nsummary(wages.attgt)\n\n\n\nCall:\natt_gt(yname = \"lnw\", tname = \"year\", idname = \"id\", gname = \"treat_timing\", \n    data = mwp, allow_unbalanced_panel = TRUE, control_group = \"notyettreated\", \n    est_method = \"ipw\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\nGroup-Time Average Treatment Effects:\n Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]  \n  1980 1980   0.0458     0.1705       -0.4971      0.5887  \n  1980 1981   0.0231     0.2653       -0.8217      0.8678  \n  1980 1982   0.1336     0.1469       -0.3339      0.6011  \n  1980 1983  -0.0032     0.2943       -0.9403      0.9338  \n  1980 1984   0.1506     0.1571       -0.3497      0.6509  \n  1980 1985   0.1559     0.1595       -0.3519      0.6637  \n  1980 1986   0.0510     0.1634       -0.4693      0.5713  \n  1980 1987   0.0072     0.1556       -0.4883      0.5026  \n  1980 1988  -0.0259     0.1654       -0.5526      0.5007  \n  1980 1989  -0.0530     0.1956       -0.6757      0.5696  \n  1980 1990  -0.0255     0.1635       -0.5460      0.4950  \n  1980 1991   0.0051     0.1785       -0.5633      0.5734  \n  1980 1992   0.0074     0.2541       -0.8015      0.8163  \n  1980 1993   0.1456     0.1942       -0.4727      0.7639  \n  1980 1994  -0.2120     0.2070       -0.8710      0.4471  \n  1980 1996       NA         NA            NA          NA  \n  1980 1998       NA         NA            NA          NA  \n  1980 2000       NA         NA            NA          NA  \n  1981 1980  -0.1289     0.1112       -0.4830      0.2252  \n  1981 1981   0.0490     0.0865       -0.2264      0.3243  \n  1981 1982   0.1016     0.1358       -0.3309      0.5340  \n  1981 1983  -0.0413     0.1541       -0.5319      0.4494  \n  1981 1984   0.0303     0.1689       -0.5075      0.5680  \n  1981 1985  -0.0700     0.1418       -0.5214      0.3814  \n  1981 1986  -0.1021     0.1415       -0.5527      0.3485  \n  1981 1987  -0.0572     0.1661       -0.5862      0.4717  \n  1981 1988  -0.0552     0.1686       -0.5918      0.4815  \n  1981 1989  -0.1644     0.1452       -0.6266      0.2978  \n  1981 1990  -0.1310     0.1256       -0.5309      0.2688  \n  1981 1991  -0.2290     0.1891       -0.8311      0.3730  \n  1981 1992  -0.0732     0.1616       -0.5878      0.4414  \n  1981 1993   0.0749     0.1653       -0.4514      0.6011  \n  1981 1994  -0.1662     0.1789       -0.7357      0.4034  \n  1981 1996       NA         NA            NA          NA  \n  1981 1998       NA         NA            NA          NA  \n  1981 2000       NA         NA            NA          NA  \n  1982 1980  -0.1950     0.1742       -0.7497      0.3597  \n  1982 1981   0.2637     0.1901       -0.3416      0.8689  \n  1982 1982   0.1626     0.3080       -0.8179      1.1432  \n  1982 1983   0.1908     0.3872       -1.0420      1.4236  \n  1982 1984   0.0543     0.3287       -0.9921      1.1007  \n  1982 1985   0.3166     0.3390       -0.7626      1.3957  \n  1982 1986  -0.0989     0.2082       -0.7616      0.5638  \n  1982 1987   0.0248     0.2058       -0.6303      0.6800  \n  1982 1988   0.0292     0.2042       -0.6210      0.6794  \n  1982 1989   0.1573     0.2028       -0.4885      0.8030  \n  1982 1990  -0.0694     0.2141       -0.7509      0.6121  \n  1982 1991   0.0606     0.2134       -0.6186      0.7399  \n  1982 1992   0.1227     0.2269       -0.5996      0.8451  \n  1982 1993   0.0560     0.2336       -0.6877      0.7998  \n  1982 1994       NA         NA            NA          NA  \n  1982 1996       NA         NA            NA          NA  \n  1982 1998       NA         NA            NA          NA  \n  1982 2000       NA         NA            NA          NA  \n  1983 1980   0.1212     0.0920       -0.1717      0.4141  \n  1983 1981  -0.0378     0.0745       -0.2750      0.1995  \n  1983 1982   0.1181     0.0790       -0.1335      0.3696  \n  1983 1983  -0.0772     0.0880       -0.3574      0.2030  \n  1983 1984  -0.0860     0.0901       -0.3730      0.2010  \n  1983 1985  -0.0279     0.1055       -0.3637      0.3079  \n  1983 1986  -0.0125     0.1273       -0.4178      0.3928  \n  1983 1987  -0.0215     0.1820       -0.6008      0.5577  \n  1983 1988  -0.1636     0.1643       -0.6868      0.3596  \n  1983 1989  -0.1976     0.1554       -0.6922      0.2970  \n  1983 1990  -0.2338     0.1530       -0.7208      0.2532  \n  1983 1991  -0.1548     0.1669       -0.6861      0.3765  \n  1983 1992  -0.1332     0.1838       -0.7185      0.4520  \n  1983 1993  -0.0297     0.2373       -0.7852      0.7258  \n  1983 1994   0.0511     0.3258       -0.9860      1.0882  \n  1983 1996   0.0934     0.2863       -0.8180      1.0048  \n  1983 1998       NA         NA            NA          NA  \n  1983 2000       NA         NA            NA          NA  \n  1984 1980   0.1525     0.0583       -0.0330      0.3379  \n  1984 1981  -0.2389     0.2153       -0.9243      0.4464  \n  1984 1982   0.4429     0.2138       -0.2379      1.1237  \n  1984 1983  -0.1672     0.1165       -0.5381      0.2036  \n  1984 1984  -0.2668     0.3667       -1.4342      0.9007  \n  1984 1985   0.1108     0.1124       -0.2470      0.4686  \n  1984 1986   0.1001     0.1483       -0.3720      0.5722  \n  1984 1987  -0.0674     0.2902       -0.9913      0.8565  \n  1984 1988   0.1631     0.2376       -0.5934      0.9195  \n  1984 1989   0.1929     0.1219       -0.1952      0.5811  \n  1984 1990   0.1890     0.1841       -0.3972      0.7751  \n  1984 1991   0.2210     0.1222       -0.1680      0.6100  \n  1984 1992   0.3217     0.1417       -0.1294      0.7729  \n  1984 1993   0.1625     0.1620       -0.3533      0.6783  \n  1984 1994   0.3103     0.2299       -0.4216      1.0422  \n  1984 1996   0.3587     0.1999       -0.2776      0.9950  \n  1984 1998   0.3045     0.2195       -0.3943      1.0033  \n  1984 2000       NA         NA            NA          NA  \n  1985 1980   0.1140     0.1417       -0.3370      0.5650  \n  1985 1981  -0.0779     0.1356       -0.5097      0.3539  \n  1985 1982   0.0244     0.1154       -0.3430      0.3919  \n  1985 1983   0.0026     0.0758       -0.2388      0.2439  \n  1985 1984  -0.0224     0.0743       -0.2590      0.2143  \n  1985 1985  -0.0594     0.0595       -0.2488      0.1301  \n  1985 1986   0.0003     0.0913       -0.2904      0.2910  \n  1985 1987   0.0666     0.1084       -0.2784      0.4115  \n  1985 1988   0.1122     0.1246       -0.2844      0.5087  \n  1985 1989   0.1080     0.1463       -0.3576      0.5737  \n  1985 1990   0.0859     0.1704       -0.4565      0.6283  \n  1985 1991   0.0433     0.1862       -0.5493      0.6360  \n  1985 1992   0.2087     0.2158       -0.4783      0.8957  \n  1985 1993   0.3690     0.2160       -0.3186      1.0565  \n  1985 1994   0.3064     0.2410       -0.4609      1.0737  \n  1985 1996   0.0689     0.2365       -0.6841      0.8219  \n  1985 1998   0.2565     0.3145       -0.7447      1.2577  \n  1985 2000       NA         NA            NA          NA  \n  1986 1980  -0.0355     0.1974       -0.6638      0.5929  \n  1986 1981   0.1164     0.1815       -0.4615      0.6942  \n  1986 1982  -0.0427     0.1611       -0.5555      0.4700  \n  1986 1983  -0.1012     0.1213       -0.4875      0.2851  \n  1986 1984   0.0884     0.1604       -0.4222      0.5990  \n  1986 1985   0.0113     0.0854       -0.2607      0.2832  \n  1986 1986  -0.2075     0.0845       -0.4764      0.0615  \n  1986 1987  -0.0586     0.1031       -0.3867      0.2695  \n  1986 1988  -0.0168     0.1322       -0.4377      0.4040  \n  1986 1989  -0.3852     0.1567       -0.8842      0.1138  \n  1986 1990  -0.1849     0.1459       -0.6495      0.2797  \n  1986 1991  -0.2494     0.1398       -0.6944      0.1956  \n  1986 1992  -0.1631     0.1629       -0.6817      0.3555  \n  1986 1993  -0.3148     0.1901       -0.9201      0.2904  \n  1986 1994  -0.1290     0.2529       -0.9341      0.6761  \n  1986 1996  -0.3379     0.4452       -1.7551      1.0793  \n  1986 1998  -0.4934     0.5864       -2.3603      1.3735  \n  1986 2000   0.1782     0.2622       -0.6563      1.0128  \n  1987 1980  -0.0870     0.1167       -0.4584      0.2844  \n  1987 1981  -0.1077     0.1387       -0.5491      0.3337  \n  1987 1982  -0.2367     0.1317       -0.6560      0.1825  \n  1987 1983  -0.0504     0.1163       -0.4206      0.3198  \n  1987 1984   0.0422     0.1720       -0.5053      0.5896  \n  1987 1985   0.3759     0.1382       -0.0640      0.8158  \n  1987 1986  -0.0060     0.0998       -0.3237      0.3116  \n  1987 1987  -0.0954     0.1434       -0.5518      0.3610  \n  1987 1988  -0.1029     0.1158       -0.4716      0.2657  \n  1987 1989  -0.1012     0.1108       -0.4540      0.2516  \n  1987 1990  -0.0826     0.1132       -0.4431      0.2778  \n  1987 1991   0.0224     0.0936       -0.2756      0.3204  \n  1987 1992   0.0868     0.1223       -0.3024      0.4761  \n  1987 1993   0.0554     0.1344       -0.3725      0.4832  \n  1987 1994   0.0322     0.1556       -0.4630      0.5274  \n  1987 1996   0.0578     0.1350       -0.3721      0.4877  \n  1987 1998   0.1176     0.1753       -0.4405      0.6757  \n  1987 2000  -0.0506     0.1945       -0.6696      0.5685  \n  1988 1980   0.1629     0.2484       -0.6281      0.9538  \n  1988 1981   0.1777     0.1354       -0.2533      0.6086  \n  1988 1982   0.0052     0.1561       -0.4918      0.5022  \n  1988 1983   0.0105     0.1412       -0.4390      0.4600  \n  1988 1984  -0.0048     0.1142       -0.3685      0.3588  \n  1988 1985   0.1437     0.0884       -0.1377      0.4251  \n  1988 1986  -0.2029     0.1301       -0.6171      0.2112  \n  1988 1987  -0.0009     0.0834       -0.2663      0.2645  \n  1988 1988   0.1335     0.0703       -0.0904      0.3573  \n  1988 1989   0.3157     0.1162       -0.0542      0.6856  \n  1988 1990   0.3005     0.1018       -0.0237      0.6247  \n  1988 1991   0.3596     0.1092        0.0120      0.7073 *\n  1988 1992   0.2546     0.1168       -0.1171      0.6264  \n  1988 1993   0.3282     0.1285       -0.0807      0.7372  \n  1988 1994   0.3124     0.1705       -0.2305      0.8553  \n  1988 1996   0.3425     0.1370       -0.0936      0.7787  \n  1988 1998   0.1312     0.2610       -0.6997      0.9621  \n  1988 2000   0.1996     0.1583       -0.3043      0.7034  \n  1989 1980   0.0841     0.1737       -0.4688      0.6370  \n  1989 1981  -0.1884     0.2518       -0.9901      0.6134  \n  1989 1982   0.2056     0.2435       -0.5695      0.9807  \n  1989 1983   0.0025     0.3195       -1.0148      1.0198  \n  1989 1984   0.0223     0.0896       -0.2628      0.3075  \n  1989 1985  -0.2832     0.2260       -1.0028      0.4364  \n  1989 1986   0.0454     0.2726       -0.8224      0.9132  \n  1989 1987  -0.0350     0.1588       -0.5406      0.4705  \n  1989 1988   0.0708     0.0897       -0.2148      0.3564  \n  1989 1989   0.0486     0.1088       -0.2976      0.3949  \n  1989 1990   0.0521     0.1234       -0.3409      0.4451  \n  1989 1991   0.0720     0.1636       -0.4487      0.5928  \n  1989 1992   0.2708     0.2808       -0.6232      1.1648  \n  1989 1993   0.0143     0.2104       -0.6557      0.6842  \n  1989 1994   0.1855     0.3078       -0.7944      1.1655  \n  1989 1996   0.2842     0.4274       -1.0765      1.6449  \n  1989 1998   0.3437     0.3158       -0.6618      1.3492  \n  1989 2000   0.0745     0.4148       -1.2461      1.3950  \n  1990 1980   0.0271     0.3437       -1.0670      1.1213  \n  1990 1981  -0.0903     0.1382       -0.5302      0.3496  \n  1990 1982  -0.3019     0.2640       -1.1423      0.5384  \n  1990 1983   0.2384     0.2058       -0.4168      0.8936  \n  1990 1984   0.4904     0.2609       -0.3400      1.3208  \n  1990 1985  -0.5329     0.2190       -1.2301      0.1643  \n  1990 1986  -0.1602     0.2090       -0.8255      0.5051  \n  1990 1987  -0.0241     0.1236       -0.4175      0.3694  \n  1990 1988   0.0585     0.3800       -1.1514      1.2684  \n  1990 1989   0.0291     0.3101       -0.9583      1.0164  \n  1990 1990   0.0951     0.2418       -0.6748      0.8650  \n  1990 1991   0.0717     0.4218       -1.2711      1.4145  \n  1990 1992   0.2766     0.2484       -0.5142      1.0673  \n  1990 1993   0.3321     0.4031       -0.9511      1.6154  \n  1990 1994   0.2388     0.4051       -1.0508      1.5283  \n  1990 1996   0.2235     0.3680       -0.9482      1.3952  \n  1990 1998   0.2999     0.3549       -0.8301      1.4299  \n  1990 2000   0.0711     0.3727       -1.1153      1.2575  \n  1991 1980   0.0674     0.3106       -0.9213      1.0561  \n  1991 1981   0.1846     0.1249       -0.2131      0.5823  \n  1991 1982  -0.3465     0.2610       -1.1774      0.4844  \n  1991 1983   0.1857     0.2057       -0.4690      0.8405  \n  1991 1984  -0.3355     0.1494       -0.8111      0.1402  \n  1991 1985   0.2739     0.2337       -0.4700      1.0179  \n  1991 1986   0.0838     0.1681       -0.4515      0.6191  \n  1991 1987   0.0179     0.1677       -0.5160      0.5518  \n  1991 1988   0.0641     0.1637       -0.4569      0.5851  \n  1991 1989  -0.1817     0.1710       -0.7259      0.3626  \n  1991 1990   0.2377     0.1234       -0.1552      0.6305  \n  1991 1991  -0.0155     0.1047       -0.3486      0.3177  \n  1991 1992  -0.0115     0.1382       -0.4514      0.4283  \n  1991 1993   0.0052     0.1477       -0.4651      0.4755  \n  1991 1994  -0.1098     0.1839       -0.6953      0.4757  \n  1991 1996  -0.2591     0.1676       -0.7926      0.2743  \n  1991 1998  -0.1275     0.1660       -0.6560      0.4010  \n  1991 2000  -0.2352     0.2376       -0.9916      0.5212  \n  1992 1980   0.0273     0.2203       -0.6739      0.7285  \n  1992 1981  -0.0104     0.2124       -0.6866      0.6659  \n  1992 1982   0.1930     0.1387       -0.2485      0.6345  \n  1992 1983  -0.0356     0.1416       -0.4866      0.4153  \n  1992 1984  -0.1464     0.0981       -0.4588      0.1659  \n  1992 1985   0.0910     0.1839       -0.4944      0.6764  \n  1992 1986  -0.0967     0.1781       -0.6639      0.4704  \n  1992 1987  -0.0081     0.2520       -0.8102      0.7941  \n  1992 1988   0.0831     0.1355       -0.3481      0.5144  \n  1992 1989  -0.0646     0.0908       -0.3537      0.2245  \n  1992 1990   0.1328     0.0866       -0.1428      0.4083  \n  1992 1991   0.0683     0.1305       -0.3472      0.4838  \n  1992 1992   0.1507     0.1096       -0.1982      0.4996  \n  1992 1993  -0.0434     0.1233       -0.4358      0.3491  \n  1992 1994  -0.0208     0.1503       -0.4994      0.4578  \n  1992 1996  -0.0925     0.1771       -0.6563      0.4712  \n  1992 1998  -0.1061     0.2151       -0.7907      0.5786  \n  1992 2000  -0.1084     0.1805       -0.6831      0.4663  \n  1993 1980   0.0885     0.1361       -0.3449      0.5219  \n  1993 1981  -0.1510     0.0472       -0.3012     -0.0008 *\n  1993 1982  -0.2665     0.2145       -0.9492      0.4163  \n  1993 1983   0.5454     0.2727       -0.3226      1.4134  \n  1993 1984  -0.2586     0.1323       -0.6798      0.1626  \n  1993 1985   0.3506     0.2189       -0.3463      1.0474  \n  1993 1986   0.1320     0.3301       -0.9190      1.1829  \n  1993 1987  -0.0727     0.2749       -0.9478      0.8025  \n  1993 1988  -0.0676     0.0863       -0.3425      0.2072  \n  1993 1989   0.0641     0.0934       -0.2331      0.3614  \n  1993 1990  -0.0863     0.0767       -0.3304      0.1577  \n  1993 1991  -0.0007     0.0879       -0.2807      0.2793  \n  1993 1992  -0.0081     0.1517       -0.4911      0.4749  \n  1993 1993  -0.1149     0.1441       -0.5736      0.3437  \n  1993 1994  -0.1264     0.1838       -0.7114      0.4587  \n  1993 1996   0.0338     0.1675       -0.4995      0.5672  \n  1993 1998  -0.0599     0.1892       -0.6622      0.5425  \n  1993 2000  -0.2164     0.2332       -0.9587      0.5259  \n  1994 1980       NA         NA            NA          NA  \n  1994 1981   0.2721     0.0737        0.0375      0.5066 *\n  1994 1982  -0.2316     0.0981       -0.5438      0.0807  \n  1994 1983   0.2402     0.1582       -0.2636      0.7440  \n  1994 1984  -0.4270     0.0517       -0.5916     -0.2624 *\n  1994 1985  -0.1525     0.1673       -0.6851      0.3801  \n  1994 1986   0.4009     0.2116       -0.2729      1.0747  \n  1994 1987   0.2158     0.3557       -0.9167      1.3483  \n  1994 1988   0.1474     0.1169       -0.2247      0.5195  \n  1994 1989   0.0033     0.0865       -0.2722      0.2788  \n  1994 1990  -0.0831     0.0559       -0.2611      0.0950  \n  1994 1991   0.0904     0.0580       -0.0943      0.2752  \n  1994 1992   0.3169     0.1337       -0.1086      0.7425  \n  1994 1993  -0.1169     0.0872       -0.3946      0.1607  \n  1994 1994  -0.1990     0.1631       -0.7182      0.3202  \n  1994 1996  -0.0386     0.1395       -0.4826      0.4055  \n  1994 1998  -0.1785     0.2628       -1.0151      0.6581  \n  1994 2000  -0.5138     0.3777       -1.7163      0.6887  \n  1996 1980  -0.5476     0.2587       -1.3712      0.2759  \n  1996 1981   0.2139     0.1906       -0.3930      0.8207  \n  1996 1982   0.0462     0.1638       -0.4752      0.5676  \n  1996 1983  -0.3550     0.2068       -1.0132      0.3032  \n  1996 1984   0.1749     0.1190       -0.2039      0.5538  \n  1996 1985  -0.1094     0.2320       -0.8480      0.6293  \n  1996 1986  -0.0334     0.2722       -0.8998      0.8330  \n  1996 1987   0.4432     0.3143       -0.5574      1.4438  \n  1996 1988  -0.5265     0.3467       -1.6302      0.5773  \n  1996 1989   0.3380     0.2378       -0.4191      1.0951  \n  1996 1990  -0.0997     0.0825       -0.3623      0.1629  \n  1996 1991  -0.0993     0.1375       -0.5370      0.3385  \n  1996 1992  -0.0424     0.1843       -0.6292      0.5444  \n  1996 1993   0.2969     0.3830       -0.9224      1.5162  \n  1996 1994  -0.1758     0.3971       -1.4399      1.0883  \n  1996 1996   0.3742     0.2594       -0.4516      1.2001  \n  1996 1998   0.1061     0.2186       -0.5899      0.8021  \n  1996 2000   0.0994     0.2772       -0.7831      0.9820  \n  1998 1980  -0.3484     0.1193       -0.7283      0.0314  \n  1998 1981   0.2643     0.1095       -0.0844      0.6130  \n  1998 1982   0.1757     0.1717       -0.3710      0.7225  \n  1998 1983  -0.1331     0.2186       -0.8289      0.5627  \n  1998 1984  -0.0396     0.1387       -0.4812      0.4020  \n  1998 1985  -0.2229     0.0932       -0.5197      0.0739  \n  1998 1986   0.4393     0.1536       -0.0497      0.9282  \n  1998 1987  -0.1735     0.1332       -0.5975      0.2504  \n  1998 1988   0.2486     0.0874       -0.0298      0.5270  \n  1998 1989  -0.1116     0.3912       -1.3571      1.1339  \n  1998 1990  -0.1982     0.3725       -1.3839      0.9876  \n  1998 1991   0.1722     0.1384       -0.2685      0.6129  \n  1998 1992   0.0115     0.1171       -0.3613      0.3842  \n  1998 1993  -0.0237     0.1197       -0.4047      0.3573  \n  1998 1994   0.2398     0.3376       -0.8351      1.3147  \n  1998 1996  -0.3608     0.3304       -1.4128      0.6912  \n  1998 1998  -0.0142     0.1609       -0.5265      0.4981  \n  1998 2000   0.1968     0.3466       -0.9066      1.3003  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nP-value for pre-test of parallel trends assumption:  0\nControl Group:  Not Yet Treated,  Anticipation Periods:  0\nEstimation Method:  Inverse Probability Weighting\n\n\nThese individual effects are similar to running a lot of individual regressions, where we compute a lot of individual \\(2 \\times 2\\) DD estimators, e.g. for group 1981:\n\n\nCode\nt &lt;- 1981\n\n# run individual effects\nfor(i in sort(unique(mwp$year))[-1]){\n  \n  # not yet treated\n  mwp$notyettreated &lt;- ifelse(mwp$treat_timing &gt; t & mwp$treat_timing &gt; i, 1, 0)\n  \n  # select 1980 group, never-treated and not yet treated\n  oo &lt;- which(mwp$treat_timing == t | mwp$treat_timing == 0 | mwp$notyettreated == 1)\n  df &lt;- mwp[oo, ]\n  \n  # after set to 1 for year rolling year i\n  df$after &lt;- NA\n  df$after[df$year == i] &lt;- 1 \n  \n  # control year\n  if(i &lt; t){\n    # if i is still before actual treatment, compare to previous year\n    tc &lt;- i - 1\n  }else{\n    # if i is beyond actual treatment, compare to year before actual treatment (t-1)\n    tc &lt;- t - 1\n  }\n  df$after[df$year == tc] &lt;- 0\n  \n  # Restrict to the two years we want to compare\n  df &lt;- df[!is.na(df$after), ]\n  \n  # Define treated group\n  df$treat &lt;- ifelse(df$treat_timing == t, 1, 0)\n  \n  # Estiamte 2x2 DD\n  tmp.lm &lt;- lm(lnw ~ treat*after, data = df)\n  \n  # Print\n  print(paste0(i, \": \", round(tmp.lm$coefficients[4], 4)))\n}\n\n\n[1] \"1980: -0.1289\"\n[1] \"1981: 0.049\"\n[1] \"1982: 0.1016\"\n[1] \"1983: -0.0413\"\n[1] \"1984: 0.0303\"\n[1] \"1985: -0.07\"\n[1] \"1986: -0.1021\"\n[1] \"1987: -0.0572\"\n[1] \"1988: -0.0552\"\n[1] \"1989: -0.1644\"\n[1] \"1990: -0.131\"\n[1] \"1991: -0.229\"\n[1] \"1992: -0.0732\"\n[1] \"1993: 0.0749\"\n[1] \"1994: -0.1662\"\n[1] \"1996: NA\"\n[1] \"1998: NA\"\n[1] \"2000: NA\"\n\n\nTo make this more interpretable, we re-aggregate the individuals results to a dynamic time-averaged effect (we now restrict this to observations from -3 to 6).\n\n\nCode\nwages.dyn &lt;- aggte(wages.attgt, type = \"dynamic\", na.rm = TRUE,\n                   min_e = -3, max_e = 6)\nsummary(wages.dyn)\n\n\n\nCall:\naggte(MP = wages.attgt, type = \"dynamic\", min_e = -3, max_e = 6, \n    na.rm = TRUE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:  \n    ATT    Std. Error     [ 95%  Conf. Int.] \n 0.0458        0.0418    -0.0362      0.1277 \n\n\nDynamic Effects:\n Event time Estimate Std. Error [95% Simult.  Conf. Band] \n         -3   0.0400     0.0460       -0.0792      0.1592 \n         -2   0.0172     0.0468       -0.1040      0.1383 \n         -1   0.0186     0.0300       -0.0590      0.0961 \n          0  -0.0039     0.0330       -0.0892      0.0815 \n          1   0.0352     0.0452       -0.0818      0.1522 \n          2   0.0705     0.0416       -0.0372      0.1782 \n          3   0.0710     0.0538       -0.0684      0.2104 \n          4   0.0584     0.0576       -0.0907      0.2074 \n          5   0.0508     0.0633       -0.1130      0.2146 \n          6   0.0384     0.0736       -0.1522      0.2290 \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Not Yet Treated,  Anticipation Periods:  0\nEstimation Method:  Inverse Probability Weighting\n\n\nThe did package also comes with a handy command ggdid() to plot the results\n\n\nCode\nzp3 &lt;- ggdid(wages.dyn) \n  \nzp3 &lt;- zp3 + \n  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2) +\n  geom_vline(xintercept = -0.5, colour = \"black\", lty = 1)\n\nzp3\n\n\n\n\n\nAlthough it is definitely not the same, this doesn’t look too different from our earlier FEIS results.\n\n\n\nSynthetic Control\nSo far, we have considered quantitative examples with multiple treated and multiple non-treated units, and we have applied “bruteforce” approaches to identify causal effects.\nHowever, sometimes we face the situation of a single treated unit. This is especially common in comparative case studies. For instance, a single country or region has implemented a specific policy or experienced a specific shock. For instance, we might be interested in how a terrorist attack might influence the economic development of a region (Abadie and Gardeazabal 2003).\nThe synthetic control estimator (Abadie and Gardeazabal 2003; Abadie, Diamond, and Hainmueller 2010; Abadie 2021) has become a very popular way of dealing with situation with a single treated unit.\nThe general idea: we use the control units to construct an average “synthetic control” unit in a way that the outcome trajectory of the synthetic control unit closely matches the outcome trajectory of the treated unit. Find weights to construct a “synthetic control” unit (by reweighing non-treated units) that optimally estimates a counterfactual trajectory for the treated unit.\nWith covariates, the method first calculates the importance of several covariates for the outcome and subsequently computes weights, which minimizes the difference between the treatment and control groups in the importance-weighted covariates.\n\n\n\nLeft: treated (California) unit and unweighted average of all other states. Right: treated (California) unit and synthetic control group (Abadie, Diamond, and Hainmueller 2010)\n\n\nNote that we have to assume that the control units are not affected by the treatment (no spillovers) and that the treatment has no effect on the outcome before the treatment period (no anticipation).\nIf there are reasons to believe that there potential anticipation effects (often the case), one can back-date the treatment, i.e. we set the treatment timing to the first period where there might possibly be an effect on the outcome.\nFormally, the (time-specific) treatment effect is estimated as the difference between the outcome of the treated unit and the average re-weighted control units Abadie, Diamond, and Hainmueller (2011): \\[\n\\hat{\\delta}_{1t} = Y_{1t} - \\sum_{j=2}^{J+1}w_j^*Y_{jt},\n\\] where \\(i=1\\) is the treated unit and \\(j\\neq i\\) are all potential control units (donor pool).\n\\(w_j^*\\) are the optimally chosen (non-negative) weights for each control unit, where we impose two restrictions: a) the weights are non-negative \\(w_j \\geq 0\\) for \\(j=2,\\dots, J+1\\), and the weights sum to one \\(\\sum_{j=2}^{J+1}w_j = 1\\).\nThis leaves us with the task to find the optimally chosen weights. So assume we have \\(k = 1,\\dots,K\\) covariates \\(\\boldsymbol{\\mathbf{X}}\\), where \\(x_{1k}\\) is the \\(k\\)-th covariate of the treated unit, and \\(\\boldsymbol{\\mathbf{x}}_{0k}\\) a \\(1 \\times J\\) vector of the same covariate for the control units / donor pool. Then we choose \\(W^*\\) as the value that minimizes \\[\n\\sum_{k=1}^K v_k(x_{1k} - x_{0k}W)^2,\n\\] with \\(v_k\\) reflecting the relative importance of the covariate \\(k\\). As we want the outcome trajectories of the treated unit and synthetic control to closely match, \\(v_1,\\dots,v_K\\) is usually determined by the predictive power of the respective covariate.\nThe original authors Abadie, Diamond, and Hainmueller (2011) use a data driven approach to choose \\(V^*\\) by minimizing the mean squared prediction error (MSPE) of the outcome over the pre-treatment periods \\(t=1,\\dots,T_0\\). Formally, we choose \\(V^*\\) that minimizes: \\[\n\\sum_{t=1}^{T_0}\\Big( Y_{1t} - \\sum_{j=2}^{J+1}w_j^*(V)Y_{jt}\\Big)\n\\]\nNote: with increasing pre-intervention periods (\\(T_0\\)), we can assume that the synthetic control also accounts for unobserved factors affecting \\(Y\\) (Abadie, Diamond, and Hainmueller 2010), assuming that only units with similar unobservables exhibit a similar outcome trajectory.\nHowever, with increasing pre-intervention periods, it is important to check if the outcome trajectories match well not only at far temporal distances to the treatment but also in periods closely before the treatment.\nA critical assumption is that the outcome trajectory (and other covariates) of the treated unit falls into the “convex hull” of the donor pool’s outcome trajectories. Intuitively: we construct the synthetic control by giving each control unit a weight of \\(\\geq 0\\) (we only interpolate, but do not extrapolate). Thus, there must be sufficient control units with “higher” and “lower” values in the outcome for pre-intervention periods. If the treated unit is an outlier before the intervention, Synthetic Control does not make much sense.\n\nStatistical inference with synthetic control\nCalculating uncertainty estimates for Synthetic Control methods is an ongoing methodological challenge. Recent work, for instance, suggest to quantify uncertainty based on permutation inference procedures (Chernozhukov, Kasahara, and Schrimpf 2021) or on conditional prediction intervals (Cattaneo, Feng, and Titiunik 2021). In practice, however, both methods are difficult to implement with standard software (especially with covariate matching).\nPlacebo tests\nAbadie and Gardeazabal (2003) and Abadie, Diamond, and Hainmueller (2010) propose a more qualitative approach to asses the significance of the findings: placebo tests. The idea is that we randomly assign the treatment to a control unit and perform the Synthetic Control method with this placebo treatment unit, and we repeat this exercise with every control unit.\nBasically, we test what treatment effect we find if we assign treatment status to a unit that actually did not receive the treatment, thereby giving us a distribution of “treatment effects” for non-treated contries. We can then compare these placebo results to our actual treatment effect.\n\n\n\nPlacebo test for Synthetic Control (Abadie, Diamond, and Hainmueller 2010)\n\n\nTo increase the comparability of the actual treatment unit and placebo treatment units, propose to exclude those units which have a poor pre-treatment fit (left panel above) by dropping those with a mean square prediction error above a specific threshold (right panel above).\nIf the actual treatment effect lies at the extremes of the distribution of the placebo “treatment” units, we can say that the treatment likely had a significant effect on the outcome. If, in contrast, the actual treatment effect lies in the middle of the placebo effects for those not receiving the treatment, this indicates that the treatment did not have a significant influence.\n\n\nExample\nCan we use our marriage wage premium example and the respective data to perform a Synthetic Control analysis? Why (not)?\nIn R, we can use the package Synth to produce Synthetic Control estimates. For a demonstration using Stata by Jens Hainmueller see the following video.\nHere, we use the example of Abadie and Gardeazabal (2003), analyzing the influence of the terrorist conflict in Basque on economic outcomes.\n\n\nCode\nlibrary(Synth)\n# devtools::install_github(\"bcastanho/SCtools\")\nlibrary(SCtools)\n\ndata(\"basque\")\nbasque &lt;- basque[-which(basque$regionno == 1), ]\nhead(basque[basque$year &gt; 1963, ])\n\n\n   regionno regionname year   gdpcap sec.agriculture sec.energy sec.industry\n53        2  Andalucia 1964 2.508855              NA         NA           NA\n54        2  Andalucia 1965 2.584690           24.52       2.65        18.18\n55        2  Andalucia 1966 2.694444              NA         NA           NA\n56        2  Andalucia 1967 2.802342           21.87       2.65        18.22\n57        2  Andalucia 1968 2.987361              NA         NA           NA\n58        2  Andalucia 1969 3.179092           19.73       3.24        18.43\n   sec.construction sec.services.venta sec.services.nonventa school.illit\n53               NA                 NA                    NA     924.4030\n54             8.55              38.32                  7.78     899.3490\n55               NA                 NA                    NA     874.7877\n56             8.54              39.08                  9.65     850.7106\n57               NA                 NA                    NA     827.1093\n58             8.86              39.23                 10.52     803.9755\n   school.prim school.med school.high school.post.high popdens   invest\n53    3028.127   129.5121    48.95936         24.93797      NA 17.40149\n54    3036.816   136.1797    51.77722         25.75277      NA 18.30896\n55    3049.442   143.5947    54.98323         26.63120      NA 18.42999\n56    3072.636   152.2357    58.61038         27.67255      NA 19.11566\n57    3086.955   174.5955    62.49103         28.29547      NA 20.60849\n58    3100.572   197.2741    66.77776         30.38357   68.51 22.05559\n\n\nThe data contains the GDP per capita for Spanish regions in long format, and several covariates. Our treatment is the onset of political unrest in Basque in the year 1970. However, the terrorist activities were low at the beginning, but increased sharply in 1974.\nFirst, we need to create a dataprep object which ensures that everything is in the right format for the synth() function. Apart from the standard information, we here have to decide which control units should be included, which covariates should be included in the matching procedure, and for which periods these covariates should be included.\nHere we want to include the following covariates:\n\nThe average GDP per capita in 1960 - 1964 and in 1965-1969 (we use the special.predictors option).\nThe share with high school (“school.high”) and more than high school (“school.post.high”) for the entire observation period (which need to specified using the option time.predictors.prior).\nPopulation density (“popdens”) in 1969 (we use the special.predictors option).\nPercentage working in the agricultural (“sec.agriculture”), the industrial (“sec.industry”), and the service sector (“sec.services.venta”) from 1961, 1963, 1965, 1967, 1969 (we use the special.predictors option).\n\n\n\nCode\ndata_prep.out &lt;- dataprep(foo = basque, \n                          predictors = c(\"school.high\", \"school.post.high\"),\n                          predictors.op = \"mean\", \n                          time.predictors.prior = 1960:1969,\n                          special.predictors = list(\n                            list(\"gdpcap\", 1960:1964, \"mean\"),\n                            list(\"gdpcap\", 1965:1969, \"mean\"),\n                            list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n                            list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n                            list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\")\n                          ),\n                          dependent = \"gdpcap\", \n                          unit.variable = \"regionno\",\n                          time.variable = \"year\", \n                          treatment.identifier = 17,\n                          controls.identifier = c(2:16, 18), \n                          time.optimize.ssr = 1960:1969, \n                          time.plot = 1960:1997,\n                          unit.names.variable = \"regionname\")\n\n\n\n Missing data- treated unit; predictor: school.high ; for period: 1960 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.high ; for period: 1961 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.high ; for period: 1962 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.high ; for period: 1963 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.post.high ; for period: 1960 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.post.high ; for period: 1961 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.post.high ; for period: 1962 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data- treated unit; predictor: school.post.high ; for period: 1963 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data - control unit: 2 ; predictor: school.high ; for period: 1960 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data - control unit: 2 ; predictor: school.high ; for period: 1961 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data - control unit: 2 ; predictor: school.post.high ; for period: 1960 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n Missing data - control unit: 2 ; predictor: school.post.high ; for period: 1961 \n We ignore (na.rm = TRUE) all missing values for predictors.op.\n\n\nCode\ncbind(data_prep.out$X0, data_prep.out$X1)\n\n\n                                             2         3         4         5\nschool.high                          57.266496 16.091676 14.799358  5.921604\nschool.post.high                     27.278924  8.684416  6.424505  3.680154\nspecial.gdpcap.1960.1964              2.271908  3.326892  3.350914  4.605498\nspecial.gdpcap.1965.1969              2.849586  4.072922  4.116838  5.826450\nspecial.sec.agriculture.1961.1969    24.194000 21.726000 12.362000 13.130000\nspecial.sec.industry.1961.1969       18.276000 22.780000 24.126000 18.258000\nspecial.sec.services.venta.1961.1969 38.186000 34.289999 30.152000 51.752000\n                                             6         7         8         9\nschool.high                          12.086849  7.304420 37.787317 16.539727\nschool.post.high                      5.844122  2.885214 19.173427  6.308165\nspecial.gdpcap.1960.1964              2.649514  3.526164  2.456512  1.922979\nspecial.gdpcap.1965.1969              3.452514  4.216181  3.157612  2.558398\nspecial.sec.agriculture.1961.1969    19.944000 15.922000 29.718000 36.086001\nspecial.sec.industry.1961.1969        9.816000 36.530000 16.474000 17.178000\nspecial.sec.services.venta.1961.1969 45.278001 33.484000 30.844000 29.238000\n                                            10        11        12        13\nschool.high                          63.608315 34.275772 11.271676 29.341941\nschool.post.high                     32.374611 16.822106  4.570566 13.543060\nspecial.gdpcap.1960.1964              4.778920  3.548129  1.707641  2.199986\nspecial.gdpcap.1965.1969              5.515096  4.138360  2.145316  2.832591\nspecial.sec.agriculture.1961.1969     6.936000 18.582000 37.948000 28.862000\nspecial.sec.industry.1961.1969       40.056000 28.046000 10.886000 17.882000\nspecial.sec.services.venta.1961.1969 39.068000 39.064001 31.720000 33.714001\n                                            14        15        16        18\nschool.high                          62.458658  9.082540  6.550353  3.377170\nschool.post.high                     57.704985  4.428528  4.190368  1.732763\nspecial.gdpcap.1960.1964              5.751671  2.507169  3.600557  3.400129\nspecial.gdpcap.1965.1969              6.201714  3.291945  4.391617  4.218281\nspecial.sec.agriculture.1961.1969     1.864000 19.352000 24.704000 30.322001\nspecial.sec.industry.1961.1969       23.834000 19.644000 28.398000 26.612000\nspecial.sec.services.venta.1961.1969 52.714000 36.177999 30.468000 28.300000\n                                            17\nschool.high                          25.727525\nschool.post.high                     13.479720\nspecial.gdpcap.1960.1964              4.859026\nspecial.gdpcap.1965.1969              5.711911\nspecial.sec.agriculture.1961.1969     6.844000\nspecial.sec.industry.1961.1969       45.082000\nspecial.sec.services.venta.1961.1969 33.754000\n\n\nSo, this prepares a list of different objects that are used in the optimazition algorithm later on. Above, we see all the used covariates used for mathing.\nwe can also use it to print the raw trajecotries of the treated region and the control pool\n\n\nCode\nY1 &lt;- data.frame(data_prep.out$Y1plot)\nnames(Y1) &lt;- \"y\"\nY1$treat &lt;- \"Treatment\"\nY1$year &lt;- as.numeric(rownames(Y1))\n\nY0 &lt;- data.frame(rowMeans(data_prep.out$Y0plot))\nnames(Y0) &lt;- \"y\"\nY0$treat &lt;- \"Control\"\nY0$year &lt;- as.numeric(rownames(Y0))\n\ndata &lt;- rbind(Y1, Y0)\n\np1 &lt;- ggplot(data = data, aes(x = year, y = y)) +\n  geom_line(aes(group = treat, color = treat, linetype = treat),\n            size = 1.2) +\n  scale_x_continuous(guide = guide_axis(check.overlap = TRUE))+\n  theme_bw() +\n  theme(legend.key = element_blank(), legend.title = element_blank(),\n        legend.position = c(0.05,0.95), legend.justification = c(\"left\", \"top\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"),\n        legend.spacing.y = unit(-0.1, \"cm\"))\np1\n\n\n\n\n\nThis seems pretty hard to compare as there were already strong differences between Basque and other Spanish reasons before the onset of terrorist activities.\nSo, let’s see if we can do better by estimating a synthetic control unit for Basque using the synth() function.\n\n\nCode\nsynth.out &lt;- synth(data.prep.obj = data_prep.out)\n\n\n\nX1, X0, Z1, Z0 all come directly from dataprep object.\n\n\n**************** \n searching for synthetic control unit  \n \n\n**************** \n**************** \n**************** \n\nMSPE (LOSS V): 0.004303228 \n\nsolution.v:\n 0.005771656 0.002774483 0.2459551 0.2711423 0.4700225 0.004333906 2.35e-08 \n\nsolution.w:\n 3.637e-07 1.0433e-06 0.0001466839 0.2090779 2.1999e-06 9.92302e-05 2.842e-07 2.729e-07 0.6449995 9.236e-07 2.988e-07 4.46e-07 0.1456666 3.3936e-06 6.043e-07 2.627e-07 \n\n\nThis already gives us some important information. “solution.v” tells us the weights for the individual covariates, and “solution.w” gives us the weights for the units in the donor pool for constructing the synthetic control unit.\nBefore looking at the actual results, it is helpful to check some core statistics. For instance, below we see how well the synthetic control unit mirrors our treated region across the included covariates.\n\n\nCode\nsynth.tables  &lt;- synth.tab(dataprep.res = data_prep.out, synth.res = synth.out)\nsynth.tables$tab.pred\n\n\n                                     Treated Synthetic Sample Mean\nschool.high                           25.728    51.367      24.235\nschool.post.high                      13.480    30.058      13.478\nspecial.gdpcap.1960.1964               4.859     4.884       3.225\nspecial.gdpcap.1965.1969               5.712     5.680       3.937\nspecial.sec.agriculture.1961.1969      6.844     7.494      21.353\nspecial.sec.industry.1961.1969        45.082    33.133      22.425\nspecial.sec.services.venta.1961.1969  33.754    43.706      36.528\n\n\nFor the GDP per capita variables, the synthetic control closely resembles the treated region. That’s good!! However, for other covariates like “school.high” or “school.post.high”, the synthetic control region matches notatbly worse than the raw sample… Nevertheless, why do we not need to worry so much about that? (look at “solution.v” above)\nObviously, we also want to know the weights that each region receives for the construction of the synthetic control.\n\n\nCode\nsynth.tables$tab.w\n\n\n   w.weights                   unit.names unit.numbers\n2      0.000                    Andalucia            2\n3      0.000                       Aragon            3\n4      0.000       Principado De Asturias            4\n5      0.209             Baleares (Islas)            5\n6      0.000                     Canarias            6\n7      0.000                    Cantabria            7\n8      0.000              Castilla Y Leon            8\n9      0.000           Castilla-La Mancha            9\n10     0.645                     Cataluna           10\n11     0.000         Comunidad Valenciana           11\n12     0.000                  Extremadura           12\n13     0.000                      Galicia           13\n14     0.146        Madrid (Comunidad De)           14\n15     0.000           Murcia (Region de)           15\n16     0.000 Navarra (Comunidad Foral De)           16\n18     0.000                   Rioja (La)           18\n\n\nSo, only the Baleares, Cataluna and Madrid contribute to the synthetic control region, while all others are set to zero. Cataluna by far receives the highest weight.\nFinally, how does the synthetic control average look like and how does it compare to the treated unit of Basque?\n\n\nCode\npath.plot(synth.res = synth.out, dataprep.res = data_prep.out,\n          Ylab = c(\"GDP per capita\"))\n\n\n\n\n\nBefore 1970, the synthetic control region aligns very well with the Basque region. From 1970, Basque shows a slightly lower GDP than its counterfactual with terrorist activities. From 1974/75 (after the sharp increase in terrorist activities) we see a pronounced decline in the GDP as compared to the synthetic control unit.\nWe can also show these differences more clearly, when plotting the differences instead of the trajectory.\n\n\nCode\ngaps.plot(synth.res = synth.out, dataprep.res = data_prep.out,\n          Ylab = c(\"Difference in GDP per capita\"))\n\n\n\n\n\nInferencial statistics\nHowever, is this difference statistically significant or might it be a result of pure chance?\nSo give some intuition of the significance of the treatment effect, we perform an interaction of placebo test, artificially assigning the treatment status to control regions.\nThe function generate.placebos() of the SCtools package does all of this automatically for us.\n\n\nCode\nplacebo &lt;- generate.placebos(dataprep.out = data_prep.out,\n                             synth.out = synth.out, \n                             strategy = \"multisession\")\n\n\nAlso creating the respective plot.\n\n\nCode\nplot_placebos(placebo)\n\n\n\n\n\nThe real treatment region seems quite special. For some control regions, we observe similarly stong “treatment effects”. However, these extreme regions have a poor pre-treatment fit, and thus should be disregarded.\n\n\n\nGeneralized Synthetic Control\nGeneralized Synthetic Control (Xu 2017) is a way of generalising the “qualitative” approach with one single treated unit to a setting with multiple treated units based on Interactive Fixed Effects or Factor Models:\n\\[\nY_{it} = \\sum_{r=1}^R \\gamma_{ir} \\delta_{tr} + \\epsilon_{it} \\quad \\text{or} \\quad,\n\\mathbf{Y} = \\mathbf U \\mathbf V^\\mathrm T + \\mathbf{\\varepsilon}.\n\\]\n\nwith with \\(\\mathbf U\\) being an \\(N \\times r\\) matrix of unknown factor loadings (unit-specific intercepts),\nand \\(\\mathbf V\\) an \\(T \\times r\\) matrix of unobserved common factors (time-varying coefficients).\n\nEstimate \\(\\\\delta\\) and \\(\\\\gamma\\) by least squares and use to impute missing values.\n\\[\n\\hat Y _{NT} = \\sum_{r=1}^R \\hat \\delta_{Nr} \\hat \\gamma_{rT}.\n\\]\nIn a matrix form, the \\(Y_{N \\times T}\\) can be rewritten as:\n\\[\nY_{N\\times T}= \\mathbf U \\mathbf V^\\mathrm T + \\epsilon_{N \\times T} =  \\mathbf L_{N \\times T} + \\epsilon_{N \\times T} = \\\\ \\left(\n\\begin{array}{ccccccc}\n\\delta_{11} & \\dots & \\delta_{R1}  \\\\\n\\vdots & \\dots & \\vdots   \\\\\n\\vdots & \\dots & \\vdots   \\\\\n\\vdots & \\dots & \\vdots   \\\\\n\\delta_{1N} & \\dots & \\delta_{RN}  \\\\\n\\end{array}\\right)\n\\left(\n\\begin{array}{ccccccc}\n\\gamma_{11}  & \\dots \\dots \\dots & \\gamma_{1T}  \\\\\n\\vdots & \\dots \\dots \\dots & \\vdots   \\\\\n\\gamma_{R1}  & \\dots \\dots \\dots & \\gamma_{RT}  \\\\\n\\end{array}\n\\right) + \\epsilon_{N \\times T}\n\\]\n\n\nMatrix Completion\nMatrix Completion (Athey et al. 2021) uses a very similar approach. Instead of estimating the factors, we estimate the matrix \\(\\mathbf L_{N \\times T}\\) directly. It is supposed to generalise the horizontal and vertical approach.\n\\[\nY_{N\\times T}=\\left(\n\\begin{array}{cccccccccc}\n{\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?}& \\checkmark  & \\dots  & {\\color{red} ?}\\\\\n\\checkmark & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & \\checkmark & {\\color{red} ?}   & \\dots & \\checkmark  \\\\\n{\\color{red} ?}  & \\checkmark & {\\color{red} ?}  & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & \\dots & {\\color{red} ?}  \\\\\n{\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?}& \\checkmark  & \\dots  & {\\color{red} ?}\\\\\n\\checkmark & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?}   & \\dots & \\checkmark  \\\\\n{\\color{red} ?}  & \\checkmark & {\\color{red} ?}  & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?} & \\dots & {\\color{red} ?}  \\\\\n\\vdots   &  \\vdots & \\vdots &\\vdots   &  \\vdots & \\vdots &\\ddots &\\vdots \\\\\n{\\color{red} ?}  & {\\color{red} ?} & {\\color{red} ?} & {\\color{red} ?}& \\checkmark & {\\color{red} ?}   & \\dots & {\\color{red} ?}\\\\\n\\end{array}\n\\right)\n\\]\nThis can be done via Nuclear Norm Minimization:\n\\[\n\\min_{L}\\frac{1}{|\\cal{O}|}\n\\sum_{(i,t) \\in \\cal{o}} \\left(Y_{it} -\nL_{it} \\right)^2+\\lambda_L \\|L\\|_*\n\\]\n\nwhere \\(\\cal{O}\\) denote the set of pairs of indices corresponding to the observed entries (the entries with \\(W_{it} = 0\\)).\n\nGiven any \\(N\\times T\\) matrix \\(A\\), define the two \\(N\\times T\\) matrices \\(P_\\cal{O}(A)\\) and \\(P_\\cal{O}^\\perp(A)\\) with typical elements: \\[\nP_\\cal{O}(A)_{it}=\n\\left\\{\n\\begin{array}{ll}\nA_{it}\\hskip1cm & {\\rm if}\\ (i,t)\\in\\cal{O}\\,,\\\\\n0&{\\rm if}\\ (i,t)\\notin\\cal{O}\\,,\n\\end{array}\\right.\n\\] and \\[\nP_\\cal{O}^\\perp(A)_{it}=\n\\left\\{\n\\begin{array}{ll}\n0\\hskip1cm & {\\rm if}\\ (i,t)\\in\\cal{O}\\,,\\\\\nA_{it}&{\\rm if}\\ (i,t)\\notin\\cal{O}\\,.\n\\end{array}\\right.\n\\]\nLet \\(A=S\\Sigma R^\\top\\) be the Singular Value Decomposition for \\(A\\), with \\(\\sigma_1(A),\\ldots,\\sigma_{\\min(N,T)}(A)\\), denoting the singular values. Then define the matrix shrinkage operator \\[\n\\ shrink_\\lambda(A)=S \\tilde\\Sigma R^\\top\\,,\n\\] where \\(\\tilde\\Sigma\\) is equal to \\(\\Sigma\\) with the \\(i\\)-th singular value \\(\\sigma_i(A)\\) replaced by \\(\\max(\\sigma_i(A)-\\lambda,0)\\).\n\nThe algorithm\n\nStart with the initial choice \\(L_1(\\lambda)=P_\\cal{O}(Y)\\), with zeros for the missing values.\nThen for \\(k=1,2,\\ldots,\\) define, \\[\nL_{k+1}(\\lambda)=shrink_\\lambda\\Biggl\\{P_\\cal{O}(Y)+P_\\cal{O}^\\perp\\Big(L_k(\\lambda)\\Big)\\Biggr\\}\\,\n\\] until the sequence \\(\\left\\{L_k(\\lambda)\\right\\}_{k\\ge 1}\\) converges.\nThe limiting matrix \\(L^*\\) is our estimator for the regularization parameter \\(\\lambda\\), denoted by \\(\\hat{L}(\\lambda,\\cal{O})\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAbadie, Alberto. 2021. “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects.” Journal of Economic Literature, Forthcoming.\n\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2010. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association 105 (490): 493–505. https://doi.org/10.1198/jasa.2009.ap08746.\n\n\n———. 2011. “Synth : An R Package for Synthetic Control Methods in Comparative Case Studies.” Journal of Statistical Software 42 (13). https://doi.org/10.18637/jss.v042.i13.\n\n\nAbadie, Alberto, and Javier Gardeazabal. 2003. “The Economic Costs of Conflict: A Case Study of the Basque Country.” American Economic Review 93 (1): 113–32. https://doi.org/10.1257/000282803321455188.\n\n\nAthey, Susan, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi. 2021. “Matrix Completion Methods for Causal Panel Data Models.” Journal of the American Statistical Association 59: 1–15. https://doi.org/10.1080/01621459.2021.1891924.\n\n\nBrüderl, Josef, and Volker Ludwig. 2015. “Fixed-Effects Panel Regression.” In The Sage Handbook of Regression Analysis and Causal Inference, edited by Henning Best and Christof Wolf, 327–57. Los Angeles: Sage.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2020. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 72 (5): 1. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nCattaneo, Matias D., Yingjie Feng, and Rocio Titiunik. 2021. “Prediction Intervals for Synthetic Control Methods.” Journal of the American Statistical Association 116 (536): 1865–80. https://doi.org/10.1080/01621459.2021.1979561.\n\n\nChamberlain, Gary. 1982. “Multivariate Regression Models for Panel Data.” Journal of Econometrics 18 (1): 5–46. https://doi.org/10.1016/0304-4076(82)90094-X.\n\n\nChernozhukov, Victor, Hiroyuki Kasahara, and Paul Schrimpf. 2021. “The Association of Opening K Schools with the Spread of COVID-19 in the United States: County-level Panel Data Analysis.” Proceedings of the National Academy of Sciences 118 (42): e2103420118. https://doi.org/10.1073/pnas.2103420118.\n\n\nClark, Andrew E., and Yannis Georgellis. 2013. “Back to Baseline in Britain: Adaptation in the British Household Panel Survey.” Economica 80 (319): 496–512. https://doi.org/10.1111/ecca.12007.\n\n\nCurrie, Janet, Lucas Davis, Michael Greenstone, and Reed Walker. 2015. “Environmental Health Risks and Housing Values: Evidence from 1,600 Toxic Plant Openings and Closings.” American Economic Review 105 (2): 678–709. https://doi.org/10.1257/aer.20121656.\n\n\nde Chaisemartin, Clément, and Xavier D’Haultfœuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nLudwig, Volker, and Josef Brüderl. 2018. “Is There a Male Marital Wage Premium? New Evidence from the United States.” American Sociological Review 83 (4): 744–70. https://doi.org/10.1177/0003122418784909.\n\n\n———. 2021. “What You Need to Know When Estimating Impact Functions with Panel Data for Demographic Research.” Comparative Population Studies 46. https://doi.org/10.12765/CPoS-2021-16.\n\n\nMeer, Jonathan, and Jeremy West. 2016. “Effects of the Minimum Wage on Employment Dynamics.” Journal of Human Resources 51 (2): 500–522. https://doi.org/10.3368/jhr.51.2.0414-6298R1.\n\n\nMundlak, Yair. 1978. “On the Pooling of Time Series and Cross Section Data.” Econometrica 46 (1): 69. https://doi.org/10.2307/1913646.\n\n\nNLSY79. n.d. NLSY79 Users’ Guide : A Guide to the 1979-2000 National Longitudinal Survey of Youth Data. Columbus, Ohio : Center for Human Resource Research, Ohio State University, [2001].\n\n\nPolachek, Solomon W., and Moon-Kak Kim. 1994. “Panel Estimates of the Gender Earnings Gap.” Journal of Econometrics 61 (1): 23–42. https://doi.org/10.1016/0304-4076(94)90075-2.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/10.1016/j.jeconom.2023.03.008.\n\n\nRüttenauer, Tobias, and Volker Ludwig. 2023. “Fixed Effects Individual Slopes: Accounting and Testing for Heterogeneous Effects in Panel Data or Other Multilevel Models.” Sociological Methods & Research 52 (1): 43–84. https://doi.org/10.1177/0049124120926211.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics 225 (2): 175–99. https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. Cambridge, Mass.: MIT Press.\n\n\nXu, Yiqing. 2017. “Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models.” Political Analysis 25 (1): 57–76. https://doi.org/10.1017/pan.2016.2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Panel Data Analysis",
    "section": "",
    "text": "Introduction\nWith usual cross-sectional data, we only observe each unit once.\nWith panel data, we observe the same unit (person, region, or country) repeatedly over time.\nThis allows applying statistical methods which identify relations based on the within-unit changes rather than the differences between units. We can then account for some statistical problems of non-experimental studies and increases the confidence in a causal interpretation of the results.\nThis course provides a hands on introduction to the preparation of panel data and the application of panel data methods. The course focuses on the most common methods like Difference in Differences, Random Effects, and Fixed Effects. We will also briefly discuss some extensions like novel Diff-in-Diff estimators and Fixed Effects Individual Slopes. The empirical part will be based on R and RMarkdown.\nThis course profited a lot from teaching materials by Josef Brüderl and Volker Ludwig: Slides\n\nWhy do we need panel data analysis?\nIn empirical social sciences, we are often interested in the causal research questions: we want to investigate questions of cause and effect. Using purely cross-sectional data, we can compare two (or more) different units (e.g. people, states, companies) and test is there exist differences according to our treatment or variable of interest.\nHowever, these units may not only differ in the dimension of interest, but may also be different in many other aspects, and it is likely that we do not obverse all the relevant aspects. In the end, we might erroneously conclude that there is a causal effect, even though we only observe correlation (which is confounded by other differences).\n\nRandomized controlled trials (RCT) provide a way to circumvent the problems of unobserved differences between treatment and control cases.\nBy randomly selecting some individuals and exposing them to the treatment of interest, we make sure that no (unobserved) characteristics of these individual can be correlated with the treatment. In other words, units in the treatment group should - on average - be identical to units in the control group on all characteristics except the treatment.\nHowever, randomly exposing some individuals to treatment and withholding treatment from others can be tricky in the social sciences. Think about the effects of education, the effects of marriage, or the effect of pregnancy (good luck with your ethics committee).\nA potential middle ground between those two approaches: “compare alike with alike” (Firebaugh 2008). This is what we usually aim for by using panel data. We do not compare two different units to each other. Rather, we compare a unit in an earlier stage to the same unit in a later stage. Not as save as an RCT, but much more ethical for many research questions.\n \nIllustration by Huntington-Klein (2021): Introducing a binary control variable (left) and introducing fixed effects (right)\n\n\n\nSome examples\n\nPlant openings and housing prices (Currie et al. 2015)\n\n\n\nLife course events and happiness (Clark and Georgellis 2013)\n\n\n\nLondon Congestion Charge and School Attendance (Conte Keivabu and Rüttenauer 2022)\n\n\n\nExtreme Weather Events Elevate Climate Change Belief but not Pro-Environmental Behaviour (Rüttenauer 2023)\n\nThere are obviously plenty of other examples on various topics, such the determinants of life satisfaction (Gattig and Minkus 2021; Kapelle et al. 2022), family dynamics and the gender wage gap Zoch (2023), or drivers of volunteering and pro-social behaviour (Aksoy and Wiertz 2023; Dederichs and Kruse 2023). Moreover, panel data and Differences-in-Differences designs occupy a prominent position the evaluation of social policy impacts (Conte Keivabu 2022; Franzen and Bahr 2023; Goodair and Reeves 2022; Mader and Rüttenauer 2022; Kneip and Bauer 2009).\n\n\n\nFurther materials\nExtensive slides by Josef Brüderl and Volker Ludwig: Slides\nSee also Brüderl and Ludwig (2015).\nBooks:\n\nIntuitive: Allison (2009)\nComprehensive and formal: Wooldridge (2010)\nFor R experts: Croissant and Millo (2019)\nGeneral introductions to causal estimation techniques: Angrist and Pischke (2015), Cunningham (2021), Firebaugh (2008), Huntington-Klein (2021)\n\nThe books by Cunningham (2021) (Link) and Huntington-Klein (2021) (Link) are freely available online!\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAksoy, Ozan, and Dingeman Wiertz. 2023. “The Impact of Religious Involvement on Trust, Volunteering, and Perceived Cooperativeness: Evidence from Two British Panels.” European Sociological Review, April, jcad024. https://doi.org/10.1093/esr/jcad024.\n\n\nAllison, Paul David. 2009. Fixed Effects Regression Models. Vol. 160. Quantitative Applications in the Social Sciences. Los Angeles: Sage.\n\n\nAngrist, Joshua David, and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton: Princeton Univ. Press.\n\n\nBrüderl, Josef, and Volker Ludwig. 2015. “Fixed-Effects Panel Regression.” In The Sage Handbook of Regression Analysis and Causal Inference, edited by Henning Best and Christof Wolf, 327–57. Los Angeles: Sage.\n\n\nClark, Andrew E., and Yannis Georgellis. 2013. “Back to Baseline in Britain: Adaptation in the British Household Panel Survey.” Economica 80 (319): 496–512. https://doi.org/10.1111/ecca.12007.\n\n\nConte Keivabu, Risto. 2022. “Extreme Temperature and Mortality by Educational Attainment in Spain, 2012.” European Journal of Population 38 (5): 1145–82. https://doi.org/10.1007/s10680-022-09641-4.\n\n\nConte Keivabu, Risto, and Tobias Rüttenauer. 2022. “London Congestion Charge: The Impact on Air Pollution and School Attendance by Socioeconomic Status.” Population and Environment 43 (4): 576–96. https://doi.org/10.1007/s11111-022-00401-4.\n\n\nCroissant, Yves, and Giovanni Millo. 2019. Panel Data Econometrics with R. Hoboken, NJ: John Wiley and Sons.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven and London: Yale University Press.\n\n\nCurrie, Janet, Lucas Davis, Michael Greenstone, and Reed Walker. 2015. “Environmental Health Risks and Housing Values: Evidence from 1,600 Toxic Plant Openings and Closings.” American Economic Review 105 (2): 678–709. https://doi.org/10.1257/aer.20121656.\n\n\nDederichs, Kasimir, and Hanno Kruse. 2023. “Who Stays Involved? A Longitudinal Study on Adolescents’ Participation in Voluntary Associations in Germany.” European Sociological Review 39 (1): 30–43. https://doi.org/10.1093/esr/jcac013.\n\n\nFirebaugh, Glenn. 2008. Seven Rules for Social Research. Princeton, N.J. and Woodstock: Princeton University Press.\n\n\nFranzen, Axel, and Sebastian Bahr. 2023. “Poverty in Europe: How Long-term Poverty Developed Following the Financial Crisis and What Drives It.” International Journal of Social Welfare, May, ijsw.12614. https://doi.org/10.1111/ijsw.12614.\n\n\nGattig, Alexander, and Lara Minkus. 2021. “Does Marriage Increase Couples’ Life Satisfaction?: Evidence Using Panel Data and Fixed-effects Individual Slopes.” Comparative Population Studies 46 (May). https://doi.org/10.12765/CPoS-2021-05.\n\n\nGoodair, Benjamin, and Aaron Reeves. 2022. “Outsourcing Health-Care Services to the Private Sector and Treatable Mortality Rates in England, 2013: An Observational Study of NHS Privatisation.” The Lancet Public Health 7 (7): e638–46. https://doi.org/10.1016/S2468-2667(22)00133-5.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Boca Raton: Chapman & Hall/CRC.\n\n\nKapelle, Nicole, Theresa Nutz, Daria Tisch, Manuel Schechtl, Philipp M. Lersch, and Emanuela Struffolino. 2022. “My Wealth, (Y)Our Life Satisfaction? Sole and Joint Wealth Ownership and Life Satisfaction in Marriage.” European Journal of Population 38 (4): 811–34. https://doi.org/10.1007/s10680-022-09630-7.\n\n\nKneip, Thorsten, and Gerrit Bauer. 2009. “Did Unilateral Divorce Laws Raise Divorce Rates in Western Europe?” Journal of Marriage and the Family 71 (3): 592–607. https://doi.org/10.1111/j.1741-3737.2009.00621.x.\n\n\nMader, Sebastian, and Tobias Rüttenauer. 2022. “The Effects of Non-pharmaceutical Interventions on COVID-19 Mortality: A Generalized Synthetic Control Approach Across 169 Countries.” Frontiers in Public Health 10: 1–8. https://doi.org/10.3389/fpubh.2022.820642.\n\n\nRüttenauer, Tobias. 2023. “More Talk, No Action? The Link Between Exposure to Extreme Weather Events, Climate Change Belief and Pro-Environmental Behaviour.” European Societies, November, 1–25. https://doi.org/10.1080/14616696.2023.2277281.\n\n\nSchmitt, Laila, and Katrin Auspurg. 2022. “A Stall Only on the Surface? Working Hours and the Persistence of the Gender Wage Gap in Western Germany 1985.” European Sociological Review 38 (5): 754–69. https://doi.org/10.1093/esr/jcac001.\n\n\nVan Winkle, Zachary, and Anette Eva Fasang. 2020. “Parenthood Wage Gaps Across the Life Course: A Comparison by Gender and Race.” Journal of Marriage and Family 82 (5): 1515–33. https://doi.org/10.1111/jomf.12713.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. Cambridge, Mass.: MIT Press.\n\n\nZoch, Gundula. 2023. “Participation in Job-Related Training: Is There a Parenthood Training Penalty?” Work, Employment and Society 37 (1): 274–92. https://doi.org/10.1177/09500170221128692."
  },
  {
    "objectID": "Panel_part1.html",
    "href": "Panel_part1.html",
    "title": "1) Panel data methods",
    "section": "",
    "text": "Code\npkgs &lt;- c(\"plm\", \"lfe\", \"texreg\", \"tidyr\", \"dplyr\", \"lmtest\", \"sandwich\", \n          \"ggplot2\", \"ggforce\") \nlapply(pkgs, require, character.only = TRUE)"
  },
  {
    "objectID": "Panel_part1.html#cross-sectional-setting",
    "href": "Panel_part1.html#cross-sectional-setting",
    "title": "1) Panel data methods",
    "section": "Cross-sectional setting",
    "text": "Cross-sectional setting\nIn a cross-sectional setting, we could just run a standard linear regression model using Omitted Least Squares (OLS) of the form \\[\ny_{i} = \\alpha + \\beta_1 x_{i} + \\upsilon_{i},\n\\] where \\(y_{i}\\) is the dependent variable (happiness) and \\(x_i\\) the independent variable of each observation \\(i \\in \\{1, \\dots, 24\\}\\). \\(\\beta_1\\) is the coefficient of interest, \\(\\alpha\\) the overall intercept and \\(\\upsilon_{i}\\) the error term.\n\n\nCode\nlm1 &lt;- lm(happiness ~ age, data = df)\nsummary(lm1)\n\n\n\nCall:\nlm(formula = happiness ~ age, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4964 -0.4209 -0.1201  0.6615  1.6868 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0506     0.5696   1.844   0.0787 .  \nage           0.1151     0.0121   9.515 2.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8206 on 22 degrees of freedom\nMultiple R-squared:  0.8045,    Adjusted R-squared:  0.7956 \nF-statistic: 90.53 on 1 and 22 DF,  p-value: 2.96e-09\n\n\nThis indicates a positive relation between age and happiness. Graphically, this would look like:\n\n\nCode\n# The palette with black:\ncbp2 &lt;- c(\"#000000\", \n          \"#E69F00\", \n          \"#56B4E9\", \n          \"#009E73\",\n          \"#F0E442\", \n          \"#0072B2\", \n          \"#D55E00\", \n          \"#CC79A7\")\n\n# Save the residual values\ndf$predicted &lt;- predict(lm1)\ndf$residuals &lt;- residuals(lm1)\n\nzp1 &lt;- ggplot(df, aes(age, happiness)) +\n  geom_point( aes(x = age, y = happiness), size = 2, stroke = 1) +\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, color  = \"deeppink\") +\n  geom_segment(data = df, aes(xend = age, yend = predicted), \n               alpha = .3, color = \"purple\") +\n  ylim(3.3, 9.2) + expand_limits(y = c(0, 0)) + \n  theme_classic() +\n  theme(legend.key = element_blank(), legend.title = element_blank(),\n        legend.position = c(0.95,0.05), legend.justification = c(\"right\", \"bottom\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"))\nzp1  \n\n\n\n\n\nObviously, we might suspect some other characteristics to influence our results. For instance, birth cohort might be a potential confounder that affects age and happiness. We can easily add a control for the cohort to account for this possibility. We would then estimate the model\n\\[\ny_{i} = \\alpha + \\beta_1 x_{i} + \\beta_2 z_{it} + \\upsilon_{i},\n\\] where \\(z_i\\) is the control variable or confounder (cohort).\n\n\nCode\nlm2 &lt;- lm(happiness ~ age + cohort, data = df)\nsummary(lm2)\n\n\n\nCall:\nlm(formula = happiness ~ age + cohort, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01188 -0.35012 -0.01682  0.31611  0.92289 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         3.32191    0.52030   6.385 2.50e-06 ***\nage                 0.03687    0.01512   2.439   0.0237 *  \ncohortOlder cohort  2.49959    0.41862   5.971 6.31e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5114 on 21 degrees of freedom\nMultiple R-squared:  0.9275,    Adjusted R-squared:  0.9206 \nF-statistic: 134.4 on 2 and 21 DF,  p-value: 1.075e-12\n\n\nAs a result, the effect of age becomes weaker. Graphically, this would look like:\n\n\nCode\n# Save the residual values\nfor(i in unique(df$cohort)){\n  oo &lt;- which(df$cohort == i)\n  lmt &lt;- lm(happiness ~ age, data = df[oo, ])\n  df$predicted[oo] &lt;- predict(lmt)\n  df$residuals[oo] &lt;- residuals(lmt)\n}\n\n\nzp2 &lt;- ggplot(df, aes(age, happiness)) +\n  geom_point(aes(x = age, y = happiness, shape = cohort, colour = cohort), \n              size = 2, stroke = 1) +\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE,\n              mapping = aes(colour = cohort, linetype = cohort)) +  \n  geom_segment(data = df, aes(xend = age, yend = predicted), \n               alpha = .3, color = \"purple\") +\n  geom_abline(intercept = lm2$coefficients[1] + 0.5 * lm2$coefficients[3], \n                               slope = lm2$coefficients[2], color  = \"deeppink\") +  \n  ylim(3.3, 9.2) + expand_limits(y = c(0, 0)) + \n  scale_colour_manual(values = cbp2[-c(1, 2)]) + \n  scale_fill_manual(values = cbp2[-c(1, 2)]) + \n  scale_shape_manual(values = c(15:18, 25, 20)) +\n  theme_classic() +\n  theme(legend.key = element_blank(), legend.title = element_blank(),\n        legend.position = c(0.95,0.05), legend.justification = c(\"right\", \"bottom\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"))\nzp2  \n\n\n\n\n\nSo, what we basically receive here is the (weighted) average effect within each birth cohort. We use only similar observations (in terms of cohort) to estimate our effect of interest - the correlation between age and happiness. If we add any controls, we always decompose the variance which contributes to our estimator, and we eliminate the variance that is attributed to the confounder / control variable.\n\n\n\nIllustration by Huntington-Klein (2021): Introducing a binary control variable"
  },
  {
    "objectID": "Panel_part1.html#panel-data-setting",
    "href": "Panel_part1.html#panel-data-setting",
    "title": "1) Panel data methods",
    "section": "Panel data setting",
    "text": "Panel data setting\nNow, with panel data, we can even go a step further. Assume we would not have observed 24 independent observations, but rather 6 independent individuals (N = 6) at 4 time-points each (T = 4).\n\n\nCode\nzp3 &lt;- ggplot(df, aes(age, happiness)) +\n  geom_point( aes(x = age, y = happiness, shape = idname, colour = idname, fill = idname), \n              size = 2, stroke = 1) +\n  ylim(3.3, 9.2) + expand_limits(y = c(0, 0)) + \n  scale_colour_manual(values = cbp2[-c(1, 2)]) + \n  scale_fill_manual(values = cbp2[-c(1, 2)]) + \n  scale_shape_manual(values = c(15:18, 25, 20)) +\n  theme_classic() +\n  theme(legend.key = element_blank(), legend.title = element_blank(),\n        legend.position = c(0.95,0.05), legend.justification = c(\"right\", \"bottom\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"))\nzp3 \n\n\n\n\n\nWe can then decompose the available variance into three different parts:\n\nPooled variance\nBetween variance\nWithin variance\n\n\nPooled estimator\nThe pooled estimator equals what we have seen in the cross-sectional example: we basically assume that we have 24 independent observations and we ignore the person and time dimension. The Pooled OLS estimator is simply:\n\\[\ny_{it} = \\alpha + \\beta_{POLS} x_{it} + \\upsilon_{it},\n\\]\nAnd, as above, we use the simple lm() command\n\n\nCode\nlm1 &lt;- lm(happiness ~ age, data = df)\nsummary(lm1)\n\n\n\nCall:\nlm(formula = happiness ~ age, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4964 -0.4209 -0.1201  0.6615  1.6868 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.0506     0.5696   1.844   0.0787 .  \nage           0.1151     0.0121   9.515 2.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8206 on 22 degrees of freedom\nMultiple R-squared:  0.8045,    Adjusted R-squared:  0.7956 \nF-statistic: 90.53 on 1 and 22 DF,  p-value: 2.96e-09\n\n\nwhich graphically looks like:\n\n\nCode\n# Save the residual values\ndf$predicted &lt;- predict(lm1)\ndf$residuals &lt;- residuals(lm1)\n\nzp3 &lt;- ggplot(df, aes(age, happiness)) +\n  geom_point( aes(x = age, y = happiness, shape = idname, colour = idname, fill = idname), \n              size = 2, stroke = 1) +\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE,\n              color  = \"deeppink\") +\n  geom_segment(aes(xend = age, yend = predicted), \n               alpha = .3, color = \"purple\") +\n  annotate(\"text\", x = 35, y = 8.0, \n           label = paste0(\"beta[Pooled] ==\", round(lm1$coefficients[2], 3)), \n           parse = TRUE) +\n  ylim(3.3, 9.2) + expand_limits(y = c(0, 0)) + \n  scale_colour_manual(values = cbp2[-c(1, 2)]) + \n  scale_fill_manual(values = cbp2[-c(1, 2)]) + \n  scale_shape_manual(values = c(15:18, 25, 20)) +\n  ggtitle(\"A) Pooled Estimate\") +\n  theme_classic() +\n  theme(legend.key = element_blank(), \n        legend.title = element_blank(),\n        text = element_text(size = 14),\n        legend.position = c(0.95,0.05), \n        legend.justification = c(\"right\", \"bottom\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"))\nzp3\n\n\n\n\n\nInterpretation: The higher the age of an observation, the higher their happiness.\n\n\nBetween estimator\nThe between estimator only compares different persons and discards the within-person variance. Therefore, we simply run a model that only uses the person-specific means\n\\[\n\\bar{y_{i}} = \\alpha + \\beta_{BTW} \\bar{x_{i}} + \\bar{\\upsilon_{i}},\n\\]\nWe can either estimate this by hand:\n\n\nCode\ndf$m_happiness &lt;- ave(df$happiness, df$id, FUN = mean)\ndf$m_age &lt;- ave(df$age, df$id, FUN = mean)\n\nlm2 &lt;- lm(m_happiness ~ m_age, data = df)\nsummary(lm2)\n\n\n\nCall:\nlm(formula = m_happiness ~ m_age, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77437 -0.17518 -0.06819  0.25424  0.83169 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.733095   0.356036   2.059   0.0515 .  \nm_age       0.122176   0.007571  16.138 1.12e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5067 on 22 degrees of freedom\nMultiple R-squared:  0.9221,    Adjusted R-squared:  0.9186 \nF-statistic: 260.4 on 1 and 22 DF,  p-value: 1.118e-13\n\n\nor we use the plm package to do the job for us\n\n\nCode\nbtw2 &lt;- plm(happiness ~ age, data = df,\n           index = c(\"id\", \"time\"),\n           effect = \"individual\", model = \"between\")\nsummary(btw2)\n\n\nOneway (individual) effect Between Model\n\nCall:\nplm(formula = happiness ~ age, data = df, effect = \"individual\", \n    model = \"between\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 6, T = 4, N = 24\nObservations used in estimation: 6\n\nResiduals:\n        1         2         3         4         5         6 \n 0.254242 -0.158370 -0.774367  0.831689  0.021989 -0.175184 \n\nCoefficients:\n            Estimate Std. Error t-value Pr(&gt;|t|)   \n(Intercept) 0.733095   0.834979  0.8780 0.429528   \nage         0.122176   0.017755  6.8813 0.002337 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    18.13\nResidual Sum of Squares: 1.4122\nR-Squared:      0.92211\nAdj. R-Squared: 0.90263\nF-statistic: 47.3519 on 1 and 4 DF, p-value: 0.0023371\n\n\nNOTE the line “observations used in estimation” ! The manual approach with lm assumes we have 24 independent observations. This is not true, we only have 6 independent observations, where each is replicated 4 times in the data.\nGraphically this result looks like:\n\n\nCode\ndf2 &lt;- df\ndf2$happiness &lt;- df2$m_happiness\ndf2$age &lt;- df2$m_age\ndf2 &lt;- df2[which(df2$time == 1), ]\n\n# Save the residual values\nlm4 &lt;- lm(m_happiness ~ m_age, data = df2)\ndf2$predicted &lt;- predict(lm4)\ndf2$residuals &lt;- residuals(lm4)\n\nzp4 &lt;- ggplot(df, aes(age, happiness)) +\n  geom_point(aes(x = age, y = happiness, shape = idname), \n              size = 2, stroke = 1, colour = alpha(\"black\", .3), fill = alpha(\"black\", .3)) +\n  geom_point(aes(x = m_age, y = m_happiness, shape = idname, colour = idname,\n                 fill = idname), \n             size = 2, stroke = 1) +\n  geom_smooth(data = df2, \n              method = 'lm', formula = y ~ x, se = FALSE,\n              color  = \"deeppink\") +\n  geom_segment(data = df2, aes(xend = age, yend = predicted), \n               alpha = .3, color = \"purple\") +\n  annotate(\"text\", x = 35, y = 8.0, \n           label = paste0(\"beta[Between] ==\", round(lm4$coefficients[2], 3)), \n           parse = TRUE) +\n  ylim(3.3, 9.2) + expand_limits(y = c(0, 0)) + \n  scale_colour_manual(values = cbp2[-c(1, 2)]) + \n  scale_fill_manual(values = cbp2[-c(1, 2)]) + \n  scale_shape_manual(values = c(15:18, 25, 20)) +\n  ggtitle(\"B) Between Estimate\") +\n  theme_classic() +\n  theme(legend.key = element_blank(), \n        legend.title = element_blank(),\n        text = element_text(size = 14),\n        legend.position = c(0.95,0.05), \n        legend.justification = c(\"right\", \"bottom\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"))\n\nzp4  \n\n\n\n\n\nInterpretation: The older a person the higher their hapinness (as compared to younger people).\n\n\nWithin estimator\nThe within estimator only compares different periods within the same person and discards the between-person variance. We could also say the estimator is solely based on changes over time. To achieve this, we simply give every person their own intercept / add a dummy for each person (similar to the cohort example above).\n\\[\ny_{it} = \\alpha_i + \\beta_{WI} x_{it} + \\epsilon_{it},\n\\]\nAgain, we could run this manually\n\n\nCode\nlm3 &lt;- lm(happiness ~ age + idname, data = df)\nsummary(lm3)\n\n\n\nCall:\nlm(formula = happiness ~ age + idname, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.28791 -0.11875  0.03583  0.14172  0.27594 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     7.74785    0.44579  17.380 2.93e-12 ***\nage            -0.14824    0.01742  -8.511 1.56e-07 ***\nidnamePerson 2  1.75075    0.19396   9.026 6.80e-08 ***\nidnamePerson 3  3.29812    0.30964  10.652 6.09e-09 ***\nidnamePerson 4  7.06754    0.43927  16.089 1.01e-11 ***\nidnamePerson 5  8.42120    0.57348  14.684 4.34e-11 ***\nidnamePerson 6 10.38740    0.70968  14.637 4.57e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1908 on 17 degrees of freedom\nMultiple R-squared:  0.9918,    Adjusted R-squared:  0.9889 \nF-statistic: 344.1 on 6 and 17 DF,  p-value: &lt; 2.2e-16\n\n\nor we use plm:\n\n\nCode\nfe1 &lt;- plm(happiness ~ age, data = df,\n           index = c(\"id\", \"time\"),\n           effect = \"individual\", model = \"within\")\nsummary(fe1)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = happiness ~ age, data = df, effect = \"individual\", \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 6, T = 4, N = 24\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-0.287907 -0.118754  0.035834  0.141721  0.275941 \n\nCoefficients:\n     Estimate Std. Error t-value  Pr(&gt;|t|)    \nage -0.148245   0.017418 -8.5108 1.556e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    3.2561\nResidual Sum of Squares: 0.61893\nR-Squared:      0.80992\nAdj. R-Squared: 0.74283\nF-statistic: 72.4345 on 1 and 17 DF, p-value: 1.5556e-07\n\n\nGraphically, this would look like:\n\n\nCode\ndf2 &lt;- df\ndf2$happiness &lt;- df2$happiness - df2$m_happiness + mean(df$happiness)\ndf2$age &lt;- df2$age - df2$m_age  + mean(df$age)\n\n# Save the residual values\nfor(i in unique(df$id)){\n  oo &lt;- which(df$id == i)\n  lmt &lt;- lm(happiness ~ age, data = df[oo, ])\n  df$predicted[oo] &lt;- predict(lmt)\n  df$residuals[oo] &lt;- residuals(lmt)\n}\n\n\nzp5 &lt;- ggplot(df, aes(age, happiness)) +\n  geom_point( aes(x = age, y = happiness, shape = idname, colour = idname, fill  = idname), \n              size = 2, stroke = 1) +\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE,\n              mapping = aes(group = idname),\n              color  = \"deeppink\", linetype = \"dotted\") +\n  geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, show.legend = FALSE,\n              data = df2, color  = \"deeppink\", fullrange = TRUE) +\n  geom_segment(data = df, aes(xend = age, yend = predicted), \n               alpha = .3, color = \"purple\") +\n  annotate(\"text\", x = 35, y = 8.0, \n           label = paste0(\"beta[Within] ==\", round(lm3$coefficients[2], 3)), \n           parse = TRUE) +\n  ylim(3.3, 9.2) + expand_limits(y = c(0, 0)) + \n  scale_fill_manual(values = cbp2[-c(1, 2)]) +\n  scale_colour_manual(values = cbp2[-c(1, 2)]) + \n  scale_shape_manual(values = c(15:18, 25, 20)) +\n  ggtitle(\"C) Within Estimate / Fixed Effects\") +\n  theme_classic() +\n  theme(legend.key = element_blank(), \n        legend.title = element_blank(),\n        text = element_text(size = 14),\n        legend.position = c(0.95,0.05), \n        legend.justification = c(\"right\", \"bottom\"),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\"))\n\nzp5\n\n\n\n\n\nNow, we have estimated the effect by only taking observations within each person into account: given the person id, how does age correlate with happiness.\nInterpretation: The older a person gets, the lower this person’s happiness (as compared to the same persons younger age).\nThe principle is very similar to including a (binary) control variable. It’s just that we include a lot of them to get rid of a lot of (potentially confounded) variance.\n\n\n\nIllustration by Huntington-Klein (2021): Introducing fixed effects\n\n\n\n\nComparison\n\n\nCode\nscreenreg(list(lm1, lm2, lm3), digits = 3, \n          custom.model.names = c(\"POLS\", \"Between\", \"Within\"))\n\n\n\n==================================================\n                POLS        Between     Within    \n--------------------------------------------------\n(Intercept)      1.051       0.733       7.748 ***\n                (0.570)     (0.356)     (0.446)   \nage              0.115 ***              -0.148 ***\n                (0.012)                 (0.017)   \nm_age                        0.122 ***            \n                            (0.008)               \nidnamePerson 2                           1.751 ***\n                                        (0.194)   \nidnamePerson 3                           3.298 ***\n                                        (0.310)   \nidnamePerson 4                           7.068 ***\n                                        (0.439)   \nidnamePerson 5                           8.421 ***\n                                        (0.573)   \nidnamePerson 6                          10.387 ***\n                                        (0.710)   \n--------------------------------------------------\nR^2              0.805       0.922       0.992    \nAdj. R^2         0.796       0.919       0.989    \nNum. obs.       24          24          24        \n==================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nPOLS: The older an observation (person-year), the higher its happiness.\nBetween: The older a person, the higher their happiness.\nWithin: Increasing age within a person goes along with declining happiness within the same person.\nThe estimates for \\(\\beta_{POLS}\\) and \\(\\beta_{BTW}\\) are very similar here. We will see below why.\n\nThis is how the within-person relationship looks with real world data in Germany (SOEP)\n\n\n\nWorking paper by Kratz and Brüderl (2021)"
  },
  {
    "objectID": "Panel_part3.html",
    "href": "Panel_part3.html",
    "title": "Exercises",
    "section": "",
    "text": "Required packages\n\n\nCode\npkgs &lt;- c(\"plm\", \"feisr\", \"sandwich\", \"texreg\", \"tidyr\", \"haven\", \"dplyr\", \"ggplot2\", \"ggforce\") \nlapply(pkgs, require, character.only = TRUE)\n\n\n\n\nSession info\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggforce_0.4.1  ggplot2_3.4.2  dplyr_1.1.2    haven_2.5.3    tidyr_1.3.0   \n[6] texreg_1.38.6  sandwich_3.0-2 feisr_1.3.0    plm_2.6-3     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.3          generics_0.1.3      dreamerr_1.2.3     \n [4] lattice_0.21-8      hms_1.1.3           digest_0.6.32      \n [7] magrittr_2.0.3      evaluate_0.21       grid_4.3.1         \n[10] fastmap_1.1.1       jsonlite_1.8.5      Matrix_1.5-4.1     \n[13] Formula_1.2-5       httr_1.4.6          purrr_1.0.1        \n[16] fansi_1.0.4         scales_1.2.1        tweenr_2.0.2       \n[19] numDeriv_2016.8-1.1 lfe_2.9-0           Rdpack_2.4         \n[22] cli_3.6.1           rlang_1.1.1         polyclip_1.10-4    \n[25] rbibutils_2.2.13    miscTools_0.6-28    munsell_0.5.0      \n[28] withr_2.5.0         yaml_2.3.7          fixest_0.11.1      \n[31] tools_4.3.1         parallel_4.3.1      bdsmatrix_1.3-6    \n[34] colorspace_2.1-0    forcats_1.0.0       maxLik_1.5-2       \n[37] vctrs_0.6.3         R6_2.5.1            zoo_1.8-12         \n[40] lifecycle_1.0.3     htmlwidgets_1.6.2   MASS_7.3-60        \n[43] pkgconfig_2.0.3     gtable_0.3.3        pillar_1.9.0       \n[46] glue_1.6.2          Rcpp_1.0.10         collapse_1.9.6     \n[49] xfun_0.39           tibble_3.2.1        lmtest_0.9-40      \n[52] tidyselect_1.2.0    rstudioapi_0.14     knitr_1.43         \n[55] farver_2.1.1        xtable_1.8-4        htmltools_0.5.5    \n[58] nlme_3.1-162        rmarkdown_2.23      compiler_4.3.1     \n\n\n\n\nLoad data\nFor the purpose of this exercise, we will use a real-world data set. Instead of constructing our own data, we use a shortcut and use data from the replication package of Hospido (2012). The replication package can be found here.\nThis is an unbalanced panel with 32,066 observations and 2066 individuals for the period 1968–1993 of the PSID. It consists of male household heads aged 25–55 with at least 9 years of usable wages data.\n\n\nCode\n# Load stata file\ndata.df &lt;- read_dta(\"_data/h-data.dta\")\n\n# Lets order this\nnames &lt;- names(data.df)\nnames &lt;- c(\"pid\", \"year\", names[-which(names %in% c(\"pid\", \"year\"))])\ndata.df &lt;- data.df[, names]\n\ndata.df &lt;- data.df[order(data.df$pid, data.df$year), ]\n\n\n\n\n\n\n\n\n\nvariable name\ndescription\n\n\n\n\npid\nINDIVIDUAL IDENTIFIER\n\n\nyear\nYEAR OF INTERVIEW\n\n\nage\nAGE OF INDIVIDUAL\n\n\nwhite\nWHITE DUMMY\n\n\ndropout\nDROPOUT DUMMY\n\n\ngrad\nGRADUATE DUMMY\n\n\ncollege\nCOLLEGE DUMMY\n\n\nmarried\nMARRIED DUMMY\n\n\nchild\nNUMBER OF CHILDREN\n\n\nfsize\nFAMILY SIZE\n\n\nhours\nYEARLY HOURS OF WORK\n\n\nlogwages\nLOG OF REAL ANNUAL WAGES\n\n\nchangejob\nJOB CHANGE DUMMY\n\n\nten1\nTENURE DUMMY less than a year\n\n\nten2\nTENURE DUMMY a year\n\n\nten3\nTENURE DUMMY 2-3 years\n\n\nten4\nTENURE DUMMY 4 through 9 years\n\n\nten5\nTENURE DUMMY 10 through 19 years\n\n\nten6\nTENURE DUMMY 20 years or more\n\n\nprofes\nPROFESSIONAL, TECHNICAL, AND KINDRED WORKERS DUMMY\n\n\nadmin\nMANAGERS AND ADMINISTRATORS DUMMY\n\n\nsales\nCLERICAL AND SALES WORKERS DUMMY\n\n\ncrafts\nCRAFTSMAN AND KINDRED WORKERS DUMMY\n\n\noperat\nOPERATIVES WORKERS DUMMY\n\n\nservic\nLABORERS AND SERVICES WORKERS DUMMY\n\n\nsmsa\nSMSA (Standard Metropolitan Statistical Area) DUMMY\n\n\nneast\nNORTH-EAST DUMMY\n\n\nncentr\nNORTH-CENTRAL DUMMY\n\n\nsouth\nSOUTH DUMMY\n\n\nwest\nWEST DUMMY\n\n\n\n\n\nExercise 1\nDownload and load the data.\nHave a look at the data.\n\nHow many observations do we have in 1968? How many in 1990?\nWhat is the average age in 1968? What was it in 1984? And how is this possible?\nAt which age did individual with ID “5790002” become father?\n\n\n\nExercise 2\nJust to play a little bit around with the data, let us estimate some models.\n\nWhat is the correlation between age and wage? Please use different estimators to determine different types of correlations.\n\n\n\nExercise 3\nCan we use this dataset to replicate our earlier analysis on the marital wage premium? What might be a problem here? (tip: have a look a the marriage variable).\nHowever, we do something similar: We want to investigate if there is a fatherhood wage premium? In other words, do men experience an increase in wages when they become fathers?\n* Restrict the age at the start (first wage) to people aged 25-35\n\n\n\n* Use number of children to construct a binary indicator of wether there is a child in the household or not\n\n\n\n* Make sure we start only with men who are not yet fathers in the first period.\nNote: this feels like dropping a lot of information! However, it makes sense if we want to correctly identify the effect of interest.\n* Do we need to drop observations where people go from child to no child?\n\n\n\n* Calculate the effect of having a child on the wage of men (including controls if reasonable). \n\n\n\n* Calculate effects for POLS, RE, and FE (if you have some extra time, also FEIS). (#Hint: feis needs a class `data.frame` as input data)\n\n\n\n* Compare using cluster robust standard errors (and screenreg).\n\n\n\n* Interpret the results \n\nTry to perform a placebo test: what happens if you use the “lead” of becoming father. Why is this an interesting test?\n\n\n\nExercise 4\nCan we use one of the new event-study approaches, such as the Callaway and SantAnna estimator?\n* Preprocess data (treatment group and timing indicator)\n\n* Estimate the model using `att_gt`\n\n* Show the group-time specific estimates (use `method=ipw` and only restricted set of controls)\n\n* Interpret the results \n\n\n\n\n\n\n\n\n\n\nReferences\n\nHospido, L. 2012. “Modelling Heterogeneity and Dynamics in the Volatility of Individual Wages.” Journal of Applied Econometrics 27 (3): 386–414. https://doi.org/10.1002/jae.1204."
  }
]