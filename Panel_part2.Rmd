---
title: "2) Extensions"
author: "Tobias RÃ¼ttenauer"
date: "November 24, 2022"
output_dir: docs
output: 
  html_document:
    theme: flatly
    highlight: haddock
    code_folding: show
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 2
theme: united
bibliography: Panel.bib
link-citations: yes
---

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\plim}{\operatornamewithlimits{plim}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Prob}{\mathrm{Prob}}
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}

### Required packages

```{r, message = FALSE, warning = FALSE, results = 'hide'}
pkgs <- c("plm", "feisr", "did", "Synth", "SCtools", 
          "panelView", "texreg", "tidyr", "dplyr", "ggplot2", "ggforce") 
lapply(pkgs, require, character.only = TRUE)
```

### Session info

```{r}
sessionInfo()

```

# Outline

* Fixed Effects Individual Slopes

* Dynamic treatment effects

* Dynamic Diff-in-Diff

* Synthetic Control

* TBD: Generalized Synthetic Control


# Fixed Effects Individual Slopes


Remeber that we have to make the parallel trends assumption in twoways FE models. A violation of the parallel trends assumption leads to biased estimates. Usually, when controlling for time fixed effects, we make the assumption that every observation experiences the same "effect of time".

However, we can relax this assumption by giving each individual their own intercept __and__ their own slope.

The fixed effects individual slope (FEIS) estimator is a more general version of the well-known fixed effects estimator (FE), which allows to control for heterogeneous slopes in addition to time-constant heterogeneity [e.g. @Bruderl.2015; @Polachek.1994; @Ruttenauer.2020; @Wooldridge.2010]. Formally, the FEIS estimator can be expressed as 

$$
\begin{align} 
\bm y_{i} =& \bm X_{i}\bm\beta + \bm W_i \bm\alpha_i + \bm \epsilon_{i},
\end{align}
$$
where $\bm y_{i}$ is $T \times 1$, $\bm X_{i}$ is $T \times K$, and $\bm\epsilon_{i}$ is $T \times 1$. $\bm W_i$ is a $T \times J$ matrix of slope variables, and $\bm\alpha_i$ a $J \times 1$ vector of individual-specific slope parameters, for $J$ slope parameters including a constant term. If $\bm W_i$ consists of a constant term only, $\bm W_i = \bm 1$, thus $\bm\alpha_i$ reduces to $\alpha_{i1}$, and the above equation represents the well-known formula of a conventional FE model with individual fixed effects.

As with the conventional FE, FEIS can be estimated using `lm()` by including $N-1$ individual-specific dummies and interaction terms of each slope variable with the $N-1$ individual-specific dummies ($(N-1) *J$ controls). This is however highly inefficient. As with the conventional FE estimator, we can achieve the same result by running an `lm()` on pre-transformed data. Therefore, specify the 'residual maker' matrix $\bm M_i = \bm I_T - \bm W_i(\bm W^\intercal_i \bm W_i)^{-1}\bm W^\intercal_i$, and estimate
$$
\begin{align} 
y_{it} - \hat{y}_{it} =& (\bm x_{it} - \hat{\bm x}_{it})\bm\beta + \epsilon_{it} - \hat{\epsilon}_{it}, \\
\bm M_i \bm y_i =& \bm M_i \bm X_i\bm\beta + \bm M_i \bm \epsilon_{i}, \\
\tilde{\bm y}_{i} =& \tilde{\bm X}_{i}\bm\beta + \tilde{\bm \epsilon}_{i},
\end{align} 
$$
where $\tilde{\bm y}_{i}$, $\tilde{\bm X}_{i}$, and $\tilde{\bm \epsilon}_{i}$ are the residuals of regressing $\bm y_{i}$, each column-vector of $\bm X_{i}$, and $\bm \epsilon_{i}$ on $\bm W_i$. 

__Intuitively__, we 

1. estimate the individual-specific predicted values for the dependent variable and each covariate based on an individual intercept and the additional slope variables of $\bm W_i$, 

2. 'detrend' the original data by these individual-specific predicted values, and 

3. run an OLS model on the residual ('detrended') data. 

Similarly, we can estimate a correlated random effects (CRE) model [@Chamberlain.1982; @Mundlak.1978; @Wooldridge.2010] including the individual specific predictions $\hat{\bm X}_{i}$ to obtain the FEIS estimator:
$$
\begin{align} 
\bm y_{i} =& \bm X_{i}\bm\beta + \hat{\bm X}_{i}\bm\rho + \bm \epsilon_{i}.
\end{align}
$$

Although we are here mainly interested in controlling for individual time, FEIS can be used to control for individual specific effects of any covariate. For instance, @Ruttenauer.2020 discuss an example of controlling for family-specific pre-treatment conditions in a sibling study on the effect of participating in pre-school programs on later life outcomes.

### Example

As an example, we use the `mwp` panel data, containing information on wages and family status of 268 men. This is a random sample drawn from the National Longitudinal Survey of Youth [@NLSY79.2001], and more details on the selection of observations and variable construction can be found in @Ludwig.2018. 

```{r}
data("mwp", package = "feisr")
head(mwp)
```

The data set contains a unique person identifier (`id`) and survey year indicator (`year`). Furthermore, we have information about the log hourly wage rate (`lnwage`), work experience (`exp`) and its square (`expq`), family status (`marry`), enrollment in current education (`enrol`), years of formal education education (`yeduc`), age (`age`), birth cohort (`cohort`), and a grouped year indicator (`yeargr`).

we exemplary investigate the 'marriage wage premium': we analyze whether marriage leads to an increase in the hourly wage for men. We use the function `feis` to estimate fixed effects individual slope models to control for the hypothesis that those men who are more likely to marry or marry earlier, also have a steeper wage growth over time.

Let's start with our most common panel models (FE and RE):

```{r}
wages.fe <- plm(lnw ~ marry + enrol + yeduc + as.factor(yeargr)
                + exp + I(exp^2), data = mwp, index = c("id", "year"),
                model = "within", effect = "individual")
wages.re <- plm(lnw ~ marry + enrol + yeduc + as.factor(yeargr)
                + exp + I(exp^2), data = mwp, index = c("id", "year"),
                model = "random", effect = "individual")
summary(wages.fe)
```

and we calculate panel robust standard errors and attach them back to the model output:

```{r}
# Calculate vcov
vcovx_fe <- vcovHC(wages.fe, cluster = "group", method = "arellano", type = "HC3")
vcovx_re <- vcovHC(wages.re, cluster = "group", method = "arellano", type = "HC3")

# Replace original vcov in output
wages.fe$vcov <- vcovx_fe
wages.re$vcov <- vcovx_re
```

Replacing the vcov in the model output has the advantage that we now use the cluster robust SEs in all following operations (like `summary()` or `screenreg`).

```{r}
summary(wages.fe)
```

And finally, we allow for individual specific trends. To replicate the analysis of @Ludwig.2018, we use work experience (`exp`) and squared work experience as the slope variables.

__One mayor advantage of using work experience as slope is that we can still control for (grouped) time fixed effects__

__Assuming linear trends (only using exp), is a strong assumption. However, for each additional slope (e.g. polynomial), FEIS becomes more data hungry: each individual needs at least $T \geq K + 1$ observations to contribute to the model. If not, they are dropped!__

Here we use `feis` with panel robust standard errors. The command `felm` from `lfe` can be used to calculate individual slopes as well.

```{r}
wages.feis <- feis(lnw ~ marry + enrol + yeduc + as.factor(yeargr)
                   | exp + I(exp^2), data = mwp, id = "id",
                   robust = TRUE)
summary(wages.feis)
```

Let's compare the results.

```{r}
screenreg(list(wages.re, wages.fe, wages.feis), digits = 3,
          custom.model.names = c("RE", "FE", "FEIS"))
```

Interpretation:

* RE: Married observations have a significantly higher wage than unmarried observations.

* FE: If people marry, they experience an increase in wages afterwards. The effect is significant and slightly lower than the RE.

* FEIS: Accounting for the individual wage trend before marriage, we do not observe an increase in wages if people marry. The effect is small and non-significant.

Overall, this indicates that there is a problem with non-parallel trends: Those with steeper wage trajectories are more likely to marry (or marry earlier).

As mentioned above, we can achieve the same by 1) manually calculating the individual specific trends and 2) including them as additional covariates in the model.

```{r}
### Individual predicted value of covariates
vars <- c("marry", "enrol", "yeduc", 
          "yeargr2", "yeargr3", "yeargr4", "yeargr5")
for(v in vars){
  fm <- as.formula(paste(v, "~ exp + expq"))
  pred_x <- by(mwp[, c(v, "exp", "expq")],
                mwp$id,
                FUN = function(z) predict(lm(fm, data = z)))
  pred_x <- unlist(pred_x)
  
  mwp[, paste0("pred_", v)] <- pred_x
}

head(mwp[, c("id", "pred_marry", "pred_enrol")], n = 20)
```

This gives us individual-specific predicted values of each covariate based on and intercept, exp and expsq. Note that - in contrast to person-specific means - these predicted values (can) vary within a person.

Do you know why person-id 2 has all zeros on the pre_marry variable? 

Using these individual predicted values, we can retreive the FEIS estimates in a Mundlak-style model.

```{r}
wages.lmfeis <- lm(lnw ~ marry + enrol + yeduc + as.factor(yeargr) +
                     pred_marry + pred_enrol + pred_yeduc +
                     pred_yeargr2 + pred_yeargr3 + pred_yeargr4 + pred_yeargr5,
                   data = mwp)

screenreg(list(wages.feis, wages.lmfeis), digits = 3,
          custom.model.names = c("FEIS", "Manual FEIS"))
```

Note, however, that this manual approach will lead to incorrect standard errors!

# Dynamic treatment effects

Often, we are not only interested in the overall treatment effect, but we also want to know how treatment effects unfold after a treatment. For example, how does happiness change around specific life course events [@Clark.2013], or how do housing prices develop after the opening of an industrial plant [@Currie.2015]?

There are various ways of calculating how a treatment effect develops over time:

![Various impact functions for event study designs from [BrÃ¼derl/Ludwig 2019 teaching materials](https://www.ls3.soziologie.uni-muenchen.de/studium-lehre/archiv/teaching-marterials/panel-analysis_april-2019.pdf)](fig/Impact-function.PNG)

Usually, it is best to not impose a structural form, but rather to use dummy impact functions. However, even with this, there is an ongoing debate on what is the best choice of specification [@Ludwig.2021], or see for instance [blog post by Pedro H. C. SantâAnna and Brantly Callaway](https://pedrohcgs.github.io/posts/twfe).

__Note that these settings usually require a staggered treatment adoption: individuals are treated once, and afterwards remain treated__

There are many cases where this does not apply. However, one can think about potential ways of artificially creating such designs:

* Dichotomize continuous treatments (if theoretically plausible!)

* Create id-period splits. E.g. if a person gets divorced, either drop from sample, or treat as a "new id" as a person can re-marry (note that this assumes that first and second marriage have equal effects).

### Example

We stick with our example and try to estimate how the wage changes around the year of marriage.

To do so, we first estimate make sure the data is ordered by id and time

```{r}
mwp <- mwp[order(mwp$id, mwp$year), ]
head(mwp[, 1:6], n = 20)
```

Then, we make sure that our data looks like a staggered treatment design. Are there people who jump from married to not married in the data?

```{r}
# Change in marriage status within an id
mwp$fd_marry <- ave(mwp$marry,
                    mwp$id,
                    FUN = function(x) x - dplyr::lag(x, 1, default = 0)) # 0 insteat of NA for 1st year

# Mark observations starting with a negative fd value (jump from marry=1 to marry =0)
mwp$notstag_marry <- ave(ifelse(mwp$fd_marry == -1, 1, 0),
                         mwp$id,
                         FUN = function(x) cumsum(x))
table(mwp$fd_marry)
table(mwp$notstag_marry)
```

Luckily, the dataset is already cleaned: there are only transitions into marriage, not out of marriage.

Next we want to make sure if there are any individuals who already start with the treatment (who are married right from their first wave on).

__We only want to have those in our sample who potentially can go from not-treated to treated!__

```{r}
mwp <- mwp[order(mwp$id, mwp$year), ] # just to be sure

# Person year number
mwp$pynr <- ave(mwp$year,
                mwp$id,
                FUN = function(x) 1:length(x))

# Marry status at first wave
mwp$f_marry <- ifelse(mwp$pynr == 1, mwp$marry, NA)

# Distribute across individual, using mean and na.rm = TRUE
mwp$f_marry <- ave(mwp$f_marry,
                   mwp$id,
                   FUN = function(x) mean(x, na.rm = TRUE))

table(mwp$f_marry)
```

Again, someone has already done the job. There are no individuals who start married in the first wave.

We can also look at this graphically with `panelView` (mainly helpful for small N data):

```{r, fig.height = 10, fig.width = 5}
panelview(lnw ~ marry, 
          data = mwp, index = c("id","year"),
          type = "treat", theme.bw = TRUE)
```

Alright, so lets create a dummy impact function / a count variable around the treatment.

```{r}
mwp <- mwp[order(mwp$id, mwp$year), ] # just to be sure!!

# Function that creates distance to the treatment (assuming 0=control, 1=treated)
impfun <- function(x, default = -99){
  nas <- which(is.na(x))  #save nas
  ft <- which(x == 1)[1]  #first teatment index
  if(is.na(ft)){          #replicate default if never treated
    count <- rep(default, length(x))
  }else{
    ri <- 1:length(x)       #running index
    count <- ri - ft        #distance to first treatment
  }
  if(length(nas) != 0){   #replace nas if any
    count[nas] <- NA
  }
  return(count)           #return counter
}

# Apply to each individual
mwp$marry_if <- ave(mwp$marry,
                    mwp$id,
                    FUN = function(x) impfun(x))

head(mwp[, c("id", "year", "marry", "marry_if")], n = 50)
```

We can now use this time count function to estimate dynamic treatment effects. 

Note that we need to make to important decisions ([blog post by Pedro H. C. SantâAnna and Brantly Callaway](https://pedrohcgs.github.io/posts/twfe)):

* Which dates to use a reference category

* How many pre-treatment periods to include (to test for anticipation or potential pre-treatment differences)

Here, re will just include three periods before marriage and use the rest as reference categories

```{r}
# Set all before -3 to -99
mwp$marry_if[mwp$marry_if < -3 & mwp$marry_if > -99] <- -99

# Make factor with -99 as reference category
mwp$marry_if <- as.factor(mwp$marry_if)
mwp$marry_if <- relevel(mwp$marry_if, "-99")

```

And we use this as our treatment variable in the FE estimator.

```{r}
# Standard marriage indicator
wages.fe <- plm(lnw ~ marry + enrol + yeduc + as.factor(yeargr)
                + exp + I(exp^2), data = mwp, index = c("id", "year"),
                model = "within", effect = "individual")

# with dummy impact function
wages2.fe <- plm(lnw ~ marry_if + enrol + yeduc + as.factor(yeargr)
                 + exp + I(exp^2), data = mwp, index = c("id", "year"),
                 model = "within", effect = "individual")

# add cluster robust SEs
vcovx_fe2 <- vcovHC(wages2.fe, cluster = "group", method = "arellano", type = "HC3")
wages2.fe$vcov <- vcovx_fe2


summary(wages2.fe)
```

Let's plot that

```{r}
# Set up results matrix
coef.df <- data.frame(matrix(NA,
                  nrow = (length(levels(mwp$marry_if)) - 1),
                  ncol = 3))
colnames(coef.df) <- c("time", "att", "se")

# paste results
coef.df$time <- c(-3:14)
output <- summary(wages2.fe)$coefficients
coef.df[, c("att", "se")] <- output[which(grepl("marry_if", rownames(output))), 1:2]

# 95% CI
interval2  <-  -qnorm((1-0.95)/2)  # 95% multiplier
coef.df$ll <- coef.df$att - coef.df$se*interval2
coef.df$ul <- coef.df$att + coef.df$se*interval2

# Plot
zp1 <- ggplot(coef.df[coef.df$time < 7, ], 
              aes(x = time, y = att)) +  
  geom_pointrange(aes(x = time, y = att, ymin = ll, ymax = ul),
                                lwd = 1, fatten = 2) +
  geom_line(aes(x = time, y = att)) +
  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2, lwd = 1) +
  geom_vline(xintercept = -0.5, colour = "black", lty = 1, lwd = 1) +
  theme_bw()
zp1
```

An interesting finding here. There is a positive anticipation effect: "The anticipation of marriage already increases the husbands wage".

Is this plausible?

__FEIS__

Obviously, we can also use these dummy impact function in other estimators.


```{r}
### FEIS with dummy impact function
wages2.feis <- feis(lnw ~ marry_if + enrol + yeduc + as.factor(yeargr)
                   | exp + I(exp^2), 
                   data = mwp, id = "id",
                  robust = TRUE)
summary(wages2.feis)

### Plot

# Set up results matrix
coef2.df <- data.frame(matrix(NA,
                  nrow = (length(levels(mwp$marry_if)) - 1),
                  ncol = 3))
colnames(coef2.df) <- c("time", "att", "se")

# paste results
coef2.df$time <- c(-3:14)
output <- summary(wages2.feis)$coefficients
coef2.df[, c("att", "se")] <- output[which(grepl("marry_if", rownames(output))), 1:2]

# 95% CI
interval2  <-  -qnorm((1-0.95)/2)  # 95% multiplier
coef2.df$ll <- coef2.df$att - coef2.df$se*interval2
coef2.df$ul <- coef2.df$att + coef2.df$se*interval2

# Plot
zp2 <- ggplot(coef2.df[coef2.df$time < 7, ], 
              aes(x = time, y = att)) +  
  geom_pointrange(aes(x = time, y = att, ymin = ll, ymax = ul),
                                lwd = 1, fatten = 2) +
  geom_line(aes(x = time, y = att)) +
  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2, lwd = 1) +
  geom_vline(xintercept = -0.5, colour = "black", lty = 1, lwd = 1) +
  theme_bw()
zp2
```

This gives us what we already expected: using FEIS, the marital wage premium disappears.

# Dynamic Diff-in-Diff


Remember, we have defined the __2__ $\times$ __2 Diff-in-Diff__ as:

$$
y_{it} = \alpha + \gamma D_{i} + \lambda Post_{t} + \delta_{DD} (D_{i} \times Post_{t}) + \upsilon_{it},
$$

which we can easily estimate as:

$$
\hat{\delta}_{DD} = \E(\Delta y_{T}) - \E(\Delta y_{C}) = (\E(y_{T}^{post}) - \E(y_{T}^{pre})) - (\E(y_{C}^{post}) - \E(y_{C}^{pre})).
$$

Moreover, we have written the __twoways FE__ estimator as:

$$
y_{it} = \beta_{TWFE} D_{it} + \alpha_i + \zeta_t + \epsilon_{it},
$$
In a setting with only two time periods, a binary treatment, and all observations untreated in $t=1$, the Diff-in-Diff estimator equals the twoways FE estimator $\hat{\delta}_{DD} = \hat{\beta}_{TWFE}$. 


However, it is more complicated when we go beyond the 2 $\times$ 2 setting. There is an ongoing discussion on how the Difference in Differences estimator relates to the two-ways FE estimator when treatment timing varies: different individuals receive the treatment at different periods. 

Assume we can divide our setting into treatment groups (treated vs. control) and into timing groups (every observation treated in the same period form a timing group).

@Goodman-Bacon.2021 shows that the two-ways FE is a weighted average of all possible two-group/two-period DD estimators. The weights determine how much each of these single combinations contributes to the two-ways FE are determined by the group size (e.g. how long do we observe each combination before and after treatment) and the variance in the treatment.

![Two-ways FE and DD with varying treatment timing [@Goodman-Bacon.2021].](fig/goodman.jpg)

In the example above we have three groups: 1) control / never treated ($C$), 2) early treated (at period $k$), and 3) late treated (at period $l$). Those who are treated in later time periods are not only compared to those who are never treated but also to those who have already been treated in earlier periods.

@Goodman-Bacon.2021 shows that this setting with three treatment groups consists of 4 possible 2 $\times$ 2 Diff-in-Diff settings.

Panels A) and B) compare $j = k,l$ against control group $C$, and can be written as:
$$
\hat{\delta}_{DD}^{jC} = (\E(y_{j}^{post(j)}) - \E(y_{j}^{pre(j)})) - (\E(y_{C}^{post}) - \E(y_{C}^{pre})), ~\text{with} ~ j = k,l. 
$$

Panel C) compares early treated $k$ against untreated periods of late treated $l$. Note that we replace the $POST$ period with period between treatment of the early treated and the late treated $MID(k,l)$
$$
\hat{\delta}_{DD}^{kl} = (\E(y_{k}^{MID(k,l)}) - \E(y_{k}^{pre(k)})) - (\E(y_{l}^{MID(k,l)}) - \E(y_{l}^{pre(k)})). 
$$

Panel D) compares late treated $l$ against already treated periods of early treated $k$. Note that we replace the $PRE$ period with period between treatment of the early treated and the late treated $MID(k,l)$
$$
\hat{\delta}_{DD}^{lk} = (\E(y_{l}^{post(l)}) - \E(y_{l}^{MID(k,l)})) - (\E(y_{k}^{post(l)}) - \E(y_{k}^{MID(k,l)})). 
$$

THE twoways FE estimator can now be recovered as a weighted combinations of these four $2\times2$ Diff-in-Diff estimators.

The __weights__ of each of them depend on 

a) __the number of periods each subsample uses__, and 

b) __the amount of treatment variance within each subsample__.

Define $\bar{D}_{kl}$ as the mean of $D_{it}$ in the subsample that compares groups $k$ and $l$ - denoting the share of time the group spends treated -, and the relative size of each group in the pair $n_{kl} = \frac{n_k}{n_k + n_l}$. Further $\hat{V}_D$ equals the overal variance in $D_{it}$, and $\hat{V}_D^{jC}$ denotes the amount of identifying variation for comparison of groups $j$ and $C$. Then, the weights are given by:

$$
W_{jC} = \frac{(n_j + n_C)^2 \hat{V}_D^{jC}}{\hat{V}_D}, ~\text{with}~ \hat{V}_D^{jC} = n_{jC}(1-n_{jC}) \bar{D}_j(1-\bar{D}_j), ~for~ j = k,l. 
$$

$$
W_{kl} = \frac{\bigl( (n_k + n_l)(1-\bar{D}_l) \bigr)^2 \hat{V}_D^{kl}}{\hat{V}_D}, ~\text{with}~ \hat{V}_D^{kl} = n_{kl}(1-n_{kl}) \frac{\bar{D}_k-\bar{D}_l}{1-\bar{D}_l} \frac{1-\bar{D}_k}{1-\bar{D}_l}. 
$$

$$
W_{lk} = \frac{\bigl( (n_l + n_k)\bar{D}_k \bigr)^2 \hat{V}_D^{lk}}{\hat{V}_D}, ~\text{with}~ \hat{V}_D^{lk} = n_{lk}(1-n_{lk}) \frac{\bar{D}_l}{\bar{D}_k} \frac{\bar{D}_k-\bar{D}_l}{\bar{D}_k}. 
$$

If group-sizes are equal, then the treatment variance $\hat{V}_D$ in each combination group depends on the timing of the treatment. The variance get larger the more similar the groups sizes $n_{ju}$, and the more central (in the middle of the observation period) the treatment timing ($\bar{D}_k, \frac{\bar{D}_k-\bar{D}_l}{1-\bar{D}_l}, \frac{\bar{D}_l}{\bar{D}_k}$ close to 0.5).

Note that __these weights are always positive__.

QUESTION: Do you know which of the above groups A), B), C), D) get the highest weights in the example. Why?

### Biased Estimates

So why (or when) could this lead to problems with the twoways FE estimator? @deChaisemartin.2020 and @Sun.2021 criticize the TWFE on the grounds of negative weights of some subgroups / sub-effects, arguing that this may induce substantial bias in the case of heterogeneous treatment effects.

@Goodman-Bacon.2021 also explains the observation of these "negative" weights:

> "Negative weights only arise when average treatment effects vary over time. The DD decomposition shows why: when already-treated units act as controls, changes in their outcomes are subtracted and these changes may include time-varying treatment effects. This does not imply a failure of the design in the sense of non-parallel trends in counterfactual outcomes, but it does suggest caution when using TWFE estimators to summarize treatment effects."

Basically, if the treatment effect varies over time (e.g. treatment effect grows over time) TWFE might be biased because early treated groups (with an increasing treatment effect) are used as control groups for late treated groups, thereby masking the treatment effect of these late treated groups (which might however receive a high weight for the overall treatment effect).

Especially for "trend-breaking" treatment effects like in the following figure, this will lead to biased estimates of the average treatment effect [@Goodman-Bacon.2021; @Meer.2016].

![Bias of two-ways FE with trend-breaking treatment [@Goodman-Bacon.2021].](fig/goodman2.jpg)

In the middle period, we compare the trend in the early treated with the not-yet treated periods of the late treatment group, and we see the divergence between those two groups (a positive treatment effect). However, for the late treated (right art of the figure), the earlier treated are the control cases (which already includes a trend-breaking treatment effect). For the late treatment group, and we do not observe a positive, but actually a negative treatment effect as we erroneously have the treatment effect in the control group!!!

The trend-breaking treatment case is obviously an exception (this would be a really strong treatment effect). However, a similar problem arises with less strong dynamic treatment effects. Below, we set up panel data with three groups and and 12 periods. There is a never-treated group, and early-treated group and a late-treated group. As shown in Figure @fig:forbidden, we could either have A) a statistic treatment effect that changes the outcome level from one period to the other, or B) assume that the treatment effect unfolds dynamically over six periods before it stabilises.

```{r did-combinations, class.source = 'fold-hide', results='hide', message = FALSE, warning = FALSE}
#############################
### Example: Callaway DID ###
#############################

library(ggplot2)
library(gridExtra)
library(ggpubr)
library(dplyr)

### Set up six individuals with age and happiness
N <- 3
T <- 12

# id and time
df2 <- data.frame(matrix(NA, ncol = 2, nrow = N*T))
names(df2) <- c("id", "time")

df2$id <- rep(1:N, each = T)
df2$time <- rep(1:T, times = N)
df2$idname <- factor(df2$id, levels = c(1:N), labels = paste("Person", c(1:N)))

# Treatment group
df2$D <- 0 
df2$D[(1*T + 1):(2*T)] <- 1
df2$D[(2*T + 1):(3*T)] <- 2

df2$Treatment <- ifelse(df2$time >= 4 & df2$D == 1, 1, 0)
oo <- which(df2$time >= 8 & df2$D == 2)
df2$Treatment[oo] <- 1

# Starting wage
stw <- c(2300, 3000, 4000)

# Dynamic treatment
df2$DT <- as.numeric(as.character(df2$Treatment))
df2$DT_time <- ave(df2$DT,
                   df2$id,
                   FUN = function(x) cumsum(x))

for(i in 1:max(df2$DT_time)){
  X <- ifelse(df2$DT_time == i, 1, 0)
  if(i == 1){
    Treatmat <- X
  }else{
    Treatmat <- cbind(Treatmat, X)
  }
}

beta <- c(100, 300, 500, 400, 300, 200, 100, 100, 100)


# wage equation
df2$wage <- unname(rep(stw, each = T)) + (df2$time - 1)*50 + Treatmat %*% beta

# alternative wage equation
df2$alt_wage <- unname(rep(stw, each = T)) + (df2$time - 1)*50 + df2$Treatment * 500

# counterfactual
df2$wage0 <- unname(rep(stw, each = T)) + (df2$time - 1)*50

df2$Treatment <- as.factor(df2$Treatment)

# Add comparison groups
df2$wage2 <- ave(df2$wage,
                 df2$id,
                 FUN = function(x) dplyr::lag(x))
df2$time2 <- ave(df2$time,
                 df2$id,
                 FUN = function(x) dplyr::lag(x))
oo <- which(df2$Treatment == 1)
df2$wage2[oo] <- ave(df2$wage2[oo],
                 df2$id[oo],
                 FUN = function(x) x[1])
df2$time2[oo] <- ave(df2$time2[oo],
                 df2$id[oo],
                 FUN = function(x) x[1])

# for alpha
df2$D2 <- df2$D
df2$D3 <- df2$D
df2$D2[which(df2$id == 2 & df2$time >= 8)] <- 0
df2$D3[which(df2$id == 2 & df2$time >= 7)] <- 0


### Plot the Callaway Sant Anna Comparisons ###


zp1 <- ggplot(df2, aes(time, wage)) +
  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D)), lty = "solid", 
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point( aes(x = time, y = wage, fill = Treatment, shape = Treatment, alpha = as.factor(D)), 
              size = 4, stroke = 1.5, color = "white") +
  scale_alpha_manual(values = c(1, 1, 0.2), guide = "none") +
  theme_classic() +
  scale_x_continuous( breaks = seq(1, 12, 2))  +
  scale_fill_manual(values = c("#85144b", "#0074D9")) +
  scale_color_manual(values = c("#85144b", "#0074D9")) +
  scale_shape_manual(values = c(21, 24)) +
  ggtitle("Group 1: 11 2x2 DID estimates vs. never-treated") +
  theme(legend.position = c(0.05,0.95), legend.justification = c("left", "top"),
        legend.background = element_blank(),
        text = element_text(size = 14),
        legend.box.background = element_rect(colour = "black")) +
  geom_curve(aes(x = time2, y = wage2, xend = time, yend = wage, color = Treatment), 
             curvature = 0.3, data = df2[df2$D == 1 & !is.na(df2$wage2), ])


zp2 <- ggplot(df2, aes(time, wage)) +
  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D3)), lty = "solid", 
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point( aes(x = time, y = wage,  fill = Treatment, shape = Treatment, alpha = as.factor(D2)), 
              size = 4, stroke = 1.5,  color = "white") +
  scale_shape_manual(values = c(21, 24)) +
  scale_alpha_manual(values = c(0.2, 1, 0.2), guide = "none") +
  scale_fill_manual(values = c("#85144b", "#0074D9")) +
  scale_color_manual(values = c("#85144b", "#0074D9")) +
  geom_line(aes(x = time, y = wage, group = id, ), lty = "solid", colour = "black", lwd = 1,
            data = df2[df2$D2 == 2 & df2$time <= 7, ]) + 
  geom_point( aes(x = time, y = wage, fill = Treatment, shape = Treatment),
              data = df2[df2$D == 2 & df2$time <= 7, ],
              size = 4, stroke = 1.5,  color = "white") +
  scale_shape_manual(values = c(21, 24)) +
  theme_classic() +
  scale_x_continuous( breaks = seq(1, 12, 2))  +
  ggtitle("Group 1: 6 2x2 DID estimates vs. not-yet-treated") +
  theme(legend.position = c(0.05,0.95), legend.justification = c("left", "top"),
        legend.background = element_blank(),
        text = element_text(size = 14),
        legend.box.background = element_rect(colour = "black")) +
  geom_curve(aes(x = time2, y = wage2, xend = time, yend = wage, color = Treatment), 
             curvature = 0.3, data = df2[df2$D == 1 & !is.na(df2$wage2) & df2$time <= 7, ])


zp3 <- ggplot(df2, aes(time, wage)) +
  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D)), lty = "solid", 
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point(aes(x = time, y = wage, fill = Treatment, shape = Treatment, alpha = as.factor(D)), 
              size = 4, stroke =1.5, color = "white") +
  scale_color_manual(values = c("#85144b", "#0074D9")) +
  scale_fill_manual(values = c("#85144b", "#0074D9")) +
  scale_shape_manual(values = c(21, 24)) +
  scale_alpha_manual(values = c(1, 0.2, 1), guide = "none") +
  theme_classic() +
  scale_x_continuous( breaks = seq(1, 12, 2))  +
  ggtitle("Group 2: 11 2x2 DID estimates vs. never-treated") +
  theme(legend.position = c(0.05,0.95), legend.justification = c("left", "top"),
        text = element_text(size = 14),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  geom_curve(aes(x = time2, y = wage2, xend = time, yend = wage, color = Treatment), 
             curvature = 0.3, data = df2[df2$D == 2 & !is.na(df2$wage2), ])


text <- paste("DOES NOT compare\n",
                             "group 2 (late treatment) vs.\n",
                             "the already treated periods of group 1")
zp4 <- ggplot() + 
  annotate("text", x = 4, y = 25, size=8, label = text, color = "red") + 
  theme_void()




### Plot the Forbidden Comparisons Comparisons ###

df2$D4 <- df2$D
df2$D4[df2$time <= 3] <- 0

# Feed forward
df2$l_Treatment <- ave(df2$Treatment,
                       df2$id,
                       FUN = function(x) dplyr::lead(x))

fp1 <- ggplot(df2, aes(time, wage)) +
  geom_line(aes(x = time, y = wage, group = id, alpha = as.factor(D4)), lty = "solid", 
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point(aes(x = time, y = wage, fill = Treatment, shape = Treatment, alpha = as.factor(D4)), 
              size = 4, stroke =1.5, color = "white") +
  geom_line(aes(x = time, y = wage0, group = id), 
            data = df2[df2$l_Treatment == 1 | df2$Treatment == 1, ],
            lty = "dashed", alpha = 0.2,
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point(aes(x = time, y = wage0), 
             data = df2[df2$Treatment == 1, ],
             size = 3, stroke = 1, alpha = 0.2, shape = 21) +
  geom_blank(aes(x = time, y = alt_wage, fill = Treatment, shape = Treatment, alpha = as.factor(D4)), 
              size = 4, stroke =1.5, color = "white") +
  scale_color_manual(values = c("#85144b", "#0074D9")) +
  scale_fill_manual(values = c("#85144b", "#0074D9")) +
  scale_shape_manual(values = c(21, 24)) +
  scale_alpha_manual(values = c(0.2, 1, 1), guide = "none") +
  theme_classic() +
  scale_x_continuous( breaks = seq(1, 12, 2))  +
  ggtitle("B) Dynamic treatment effect \n    Late-treated vs. already-treated") +
  theme(legend.position = c(0.05,0.95), legend.justification = c("left", "top"),
        text = element_text(size = 14),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black")) 



fp2 <- ggplot(df2, aes(time, alt_wage)) +
  geom_line(aes(x = time, y = alt_wage, group = id, alpha = as.factor(D4)), lty = "solid", 
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point(aes(x = time, y = alt_wage, fill = Treatment, shape = Treatment, alpha = as.factor(D4)), 
              size = 4, stroke =1.5, color = "white") +
  geom_line(aes(x = time, y = wage0, group = id), 
            data = df2[df2$l_Treatment == 1 | df2$Treatment == 1, ],
            lty = "dashed", alpha = 0.2,
            colour = "black", lwd = 1, show.legend = FALSE) + 
  geom_point(aes(x = time, y = wage0), 
             data = df2[df2$Treatment == 1, ],
             size = 3, stroke = 1, alpha = 0.2, shape = 21) +
  scale_color_manual(values = c("#85144b", "#0074D9")) +
  scale_fill_manual(values = c("#85144b", "#0074D9")) +
  scale_shape_manual(values = c(21, 24)) +
  scale_alpha_manual(values = c(0.2, 1, 1), guide = "none") +
  theme_classic() +
  scale_x_continuous( breaks = seq(1, 12, 2))  +
  ggtitle("A) Static treatment effect \n    Late-treated vs. already-treated") +
  ylab("wage") +
  theme(legend.position = c(0.05,0.95), legend.justification = c("left", "top"),
        text = element_text(size = 14),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black")) 

```


```{r forbidden-plot, class.source = 'fold-hide', results='hide', message = FALSE, warning = FALSE}
zp <- ggarrange(fp2, fp1  + rremove("legend"), 
                ncol = 2, nrow = 1)

cairo_ps(file = paste("fig/", "Forbidden.eps", sep=""), width = 11, height = 4.5, 
          bg = "white", family = "Times New Roman")
par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 1), oma = c(0, 0, 0, 0))
zp
dev.off()

jpeg(file = paste("fig/", "Forbidden.jpeg", sep=""), width = 11, height = 4.5, 
    units = "in", res = 300, type = "cairo",
          bg = "white", family = "Times New Roman")
par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 1), oma = c(0, 0, 0, 0))
zp
dev.off()
```

![Forbidden comparison of late-treated vs already-treated](fig/Forbidden.jpeg) {#fig:forbidden}

The problem now arises when FE compares the late treated as treatment group vs. the already treated as control group. In the static treatment case (@fig-forbidden A), everything is fine as the already-treated run parallel to counterfactual of the late-treated. We can thus use them as control. However, in the dynamic treatment case (@fig-forbidden B), the "control" group of already-treated experience still an ongoing dynamic treatment effect when the late-treated are treated. They are thus not running parallel to the counterfactual of the late-treated group. This comparison is thus also called the forbidden comparison [@Roth.2023].

One way of counteracting this problem is to use event study / impact function designs (see above) to explicitly model time varying treatment effects.

### Callaway & SantâAnna dynamic Diff-in-Diff

A second way is the application of __flexible Diff-in-Diff__ estimators as proposed by @Callaway.2020.

Let's start again with the $2 \times 2$ Diff-in-Diff estimator as

$$
\delta = \E(\Delta y_{T}) - \E(\Delta y_{C}) = (\E(y_{T}^{post}) - \E(y_{T}^{pre})) - (\E(y_{C}^{post}) - \E(y_{C}^{pre})),
$$
where $\delta$ is the average treatment effect on the treated (ATT).

@Callaway.2020 show that we can generalize this $2 \times 2$ Diff-in-Diff to a mutlti-group and multi-timing setting by computing group-time average treatment effects. We group all treatment units which receive treatment at the same period into a common group $g$, and for each treatment-group $g$ and time period $t$ we estimate group-specific and time-specific ATTs: 

$$
\delta_{g,t} = \E(\Delta y_{g}) - \E(\Delta y_{C}) = (\E(y_{g}^{t}) - \E(y_{g}^{g-1})) - (\E(y_{C}^{t}) - \E(y_{C}^{g-1})),
$$

where the control group can either be the never-treated or the not-yet-treated. As shown in @fig-callaway by the convex lines, this means, we estimate an individual treatment effect for each combination of treatment-timing-group and control group.

Obviously, this gives us a large number of different treatment effects. So, in a second step, we re-aggregate these individual combinations back to group or time averaged treatment effect. In an event study design, @Callaway.2020 propose the following dynamic treatment effect for each period $e$ after the treatment:

$$
  \theta_D(e) := \sum_{g=1}^G \mathbf{1} \{ g + e \leq T \} \delta(g,g+e) P(G=g | G+e \leq T),
$$
where $e$ specifies for how long a unit has been exposed to the treatment. It's basically the average effects across all treatment-timing groups at the period $e$ after treatment. From here, one can easily calculate the cumulative effect or the overall aggregated effect.

Consider the situation in @fig-callaway, where we have a control group of never-treated units, one treatment group that is treated early (group 1) and one treatment group that is treated late (group 2). As shown below, with $T=12$ we can estimate 11 2 $\times$ 2 Diff-in-Diff estimates of group 1 against the never treated, we can estimate 6 2 $\times$ 2 Diff-in-Diff estimates of group 1 against the not-yet treated (late treatment group), and we can estimate 11 2 $\times$ 2 Diff-in-Diff estimates of group 2 against the never treated.

Note that the control period for all treated periods by default is set to the period before the treatment happened in each group. For group 1 this is period 3, and for group 2 this is period 7. This makes only sense if there is __no treatment anticipation__. Obviously, we can also use other (earlier) periods if we assume treatment anticipation.

```{r did-plot, class.source = 'fold-hide', results='hide', message = FALSE, warning = FALSE}
zp <- ggarrange(zp1, zp2  + rremove("legend"), 
                zp3  + rremove("legend"), zp4,
                ncol = 2, nrow = 2)

cairo_ps(file = paste("fig/", "DiD.eps", sep=""), width = 11, height = 8, 
          bg = "white", family = "Times New Roman")
par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 1), oma = c(0, 0, 0, 0))
zp
dev.off()

jpeg(file = paste("fig/", "DiD.jpeg", sep=""), width = 11, height = 8, 
    units = "in", res = 300, type = "cairo",
          bg = "white", family = "Times New Roman")
par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 1), oma = c(0, 0, 0, 0))
zp
dev.off()
```

![Multiple 2x2 DiD comparisons with dynamic DiD estimators](fig/DiD.jpeg){#fig-callaway}


For a more detailled introdution see @Callaway.2020 or the respective [package introcution](https://cran.r-project.org/web/packages/did/vignettes/multi-period-did.html).

__Assumptions__:

1. Staggered treatment adoption: once a unit has been treated, it remains treated thereafter (see also the note above).

2. Parallel trends assumption 
  + based on never-treated: treated units would have had parallel trends to never-treated if they would not have experienced treatment. In many empirical settings this is a strong assumption (why do some units never experience treatment?).
  + based on not-yet-treated: treated units would have had parallel trends to not-yet-treated if they would not have experienced treatment. This assumption might be a bit more likely to hold.
  
3. No treatment anticipation 
  + based on never-treated: We can relax the assumption by either back-dating the treatment or include the appropriate pre-treatment periods in our results to inspect anticipation qualitatively.
  + based on not-yet-treated: If there is treatment anticipation, our control group will be confounded by the treatment effect. A violation of the assumption can lead to stronger bias if we use the not-yet-treated as control group.
  
__Trade-off__: If assumption 2) is likely to hold, we can use only the never-treated as controls to relax assumption 3). If assumption 3) is likely to hold, we can include the not-yet-treated as control to relax assumption 2).



### Example

How does that look in our marriage example? To estimate the dynamic DD we use the `did` package, as describes in more detail [here](https://cran.r-project.org/web/packages/did/vignettes/did-basics.html) or in the authors [blog](https://pedrohcgs.github.io/posts/twfe).

__Note: This package works with staggered treatment adoption! We thus should perform all the steps we have performed above to restrict and prepare the data!__

As a first step, we need to define a variable that contains the treatment timing: the first year an ever-treated individual is treated.

This should be a positive number for all observations in treated groups. It defines which "group" a unit belongs to. It should be 0 for units in the untreated group.

```{r}
# treatment timing = year if married
mwp$treat_timing <- ifelse(mwp$marry == 1, mwp$year, NA)

# set never treated to zero
mwp$treat_timing[mwp$evermarry == 0] <- 0

# if married is not NA, used min year per id (removing NAs)
mwp$treat_timing[!is.na(mwp$marry)] <- ave(mwp$treat_timing[!is.na(mwp$marry)],
                                           mwp$id[!is.na(mwp$marry)],
                                           FUN = function(x) min(x, na.rm = TRUE))


head(mwp[, c("id", "year", "marry", "evermarry", "treat_timing")], n = 35)
```



```{r}
# estimate group-time average treatment effects using att_gt method
wages.attgt <- att_gt(yname = "lnw",
                      tname = "year",
                      idname = "id",
                      gname = "treat_timing",
                      #xformla = ~ enrol + yeduc + exp + I(exp^2), # note that we omit the yeargroup here
                      data = mwp,
                      allow_unbalanced_panel = TRUE,
                      control_group = "notyettreated",
                      est_method = "ipw"
                        )
```

One huge advantage: We do not need to make a decision about which periods (before treatment) we want to include, and which observations go into the reference category. 

However, we get a lot of individual treatment effects.

```{r}
# Show the group-time specific estimates
summary(wages.attgt)
```

These individual effects are similar to running a lot of individual regressions, where we compute a lot of individual $2 \times 2$ DD estimators, e.g. for group 1981:

```{r}
t <- 1981

# run individual effects
for(i in sort(unique(mwp$year))[-1]){
  
  # not yet treated
  mwp$notyettreated <- ifelse(mwp$treat_timing > t & mwp$treat_timing > i, 1, 0)
  
  # select 1980 group, never-treated and not yet treated
  oo <- which(mwp$treat_timing == t | mwp$treat_timing == 0 | mwp$notyettreated == 1)
  df <- mwp[oo, ]
  
  # after set to 1 for year rolling year i
  df$after <- NA
  df$after[df$year == i] <- 1 
  
  # control year
  if(i < t){
    # if i is still before actual treatment, compare to previous year
    tc <- i - 1
  }else{
    # if i is beyond actual treatment, compare to year before actual treatment (t-1)
    tc <- t - 1
  }
  df$after[df$year == tc] <- 0
  
  # Restrict to the two years we want to compare
  df <- df[!is.na(df$after), ]
  
  # Define treated group
  df$treat <- ifelse(df$treat_timing == t, 1, 0)
  
  # Estiamte 2x2 DD
  tmp.lm <- lm(lnw ~ treat*after, data = df)
  
  # Print
  print(paste0(i, ": ", round(tmp.lm$coefficients[4], 4)))
}
```

To make this more interpretable, we re-aggregate the individuals results to a dynamic time-averaged effect (we now restrict this to observations from -3 to 6).

```{r}
wages.dyn <- aggte(wages.attgt, type = "dynamic", na.rm = TRUE,
                   min_e = -3, max_e = 6)
summary(wages.dyn)
```

The `did` package also comes with a handy command `ggdid()` to plot the results 

```{r}
zp3 <- ggdid(wages.dyn) 
  
zp3 <- zp3 + 
  geom_hline(yintercept = 0, colour = gray(1/2), lty = 2) +
  geom_vline(xintercept = -0.5, colour = "black", lty = 1)

zp3
```

Although it is definitely not the same, this doesn't look too different from our earlier FEIS results. 

# Synthetic Control

So far, we have considered quantitative examples with multiple treated and multiple non-treated units, and we have applied "bruteforce" approaches to identify causal effects.

However, sometimes we face the situation of a single treated unit. This is especially common in comparative case studies. For instance, a single country or region has implemented a specific policy or experienced a specific shock. For instance, we might be interested in how a terrorist attack might influence the economic development of a region [@Abadie.2003].

The synthetic control estimator [@Abadie.2003; @Abadie.2010; @Abadie.2021] has become a very popular way of dealing with situation with a single treated unit. 

__The general idea: we use the control units to construct an average "synthetic control" unit in a way that the outcome trajectory of the synthetic control unit closely matches the outcome trajectory of the treated unit.__ Find weights to construct a "synthetic control" unit (by reweighing non-treated units) that optimally estimates a counterfactual trajectory for the treated unit. 

With covariates, the method first calculates the importance of several covariates for the outcome and subsequently computes weights, which minimizes the difference between the treatment and control groups in the importance-weighted covariates.

![Left: treated (California) unit and unweighted average of all other states. Right: treated (California) unit and synthetic control group [@Abadie.2010]](fig/abadie.jpg)

__Note that we have to assume that the control units are not affected by the treatment (no spillovers) and that the treatment has no effect on the outcome before the treatment period (no anticipation).__

If there are reasons to believe that there potential anticipation effects (often the case), one can back-date the treatment, i.e. we set the treatment timing to the first period where there might possibly be an effect on the outcome.

Formally, the (time-specific) treatment effect is estimated as the difference between the outcome of the treated unit and the average re-weighted control units [@Abadie.2003; @Abadie.2010, @Abadie.2011]:
$$
\hat{\delta}_{1t} = Y_{1t} - \sum_{j=2}^{J+1}w_j^*Y_{jt},
$$
where $i=1$ is the treated unit and $j\neq i$ are all potential control units (donor pool).

$w_j^*$ are the optimally chosen (non-negative) weights for each control unit, where we impose two restrictions: a) the weights are non-negative $w_j \geq 0$ for $j=2,\dots, J+1$, and the weights sum to one $\sum_{j=2}^{J+1}w_j = 1$.

This leaves us with the task to find the optimally chosen weights. So assume we have $k = 1,\dots,K$ covariates $\bm X$, where $x_{1k}$ is the $k$-th covariate of the treated unit, and $\bm x_{0k}$ a $1 \times J$ vector of the same covariate for the control units / donor pool. Then we choose $W^*$ as the value that minimizes 
$$
\sum_{k=1}^K v_k(x_{1k} - x_{0k}W)^2,
$$
with $v_k$ reflecting the relative importance of the covariate $k$. As we want the outcome trajectories of the treated unit and synthetic control to closely match, $v_1,\dots,v_K$ is usually determined by the predictive power of the respective covariate.

The original authors [@Abadie.2003; @Abadie.2010, @Abadie.2011] use a data driven approach to choose $V^*$ by minimizing the mean squared prediction error (MSPE) of the outcome over the pre-treatment periods $t=1,\dots,T_0$. Formally, we choose $V^*$ that minimizes:
$$
\sum_{t=1}^{T_0}\Big( Y_{1t} - \sum_{j=2}^{J+1}w_j^*(V)Y_{jt}\Big)
$$

Note: with increasing pre-intervention periods ($T_0$), we can assume that the synthetic control also accounts for unobserved factors affecting $Y$ [@Abadie.2010], assuming that only units with similar unobservables exhibit a similar outcome trajectory.

However, with increasing pre-intervention periods, it is important to check if the outcome trajectories match well not only at far temporal distances to the treatment but also in periods closely before the treatment.

A __critical assumption__ is that the outcome trajectory (and other covariates) of the treated unit falls into the "convex hull" of the donor pool's outcome trajectories. Intuitively: we construct the synthetic control by giving each control unit a weight of $\geq 0$ (we only interpolate, but do not extrapolate). Thus, there must be sufficient control units with "higher" and "lower" values in the outcome for pre-intervention periods. If the treated unit is an outlier before the intervention, Synthetic Control does not make much sense.

### Statistical inference with synthetic control

Calculating uncertainty estimates for Synthetic Control methods is an ongoing methodological challenge. Recent work, for instance, suggest to quantify uncertainty based on permutation inference procedures [@Chernozhukov.2021b] or on conditional prediction intervals [@Cattaneo.2021]. In practice, however, both methods are difficult to implement with standard software (especially with covariate matching).

__Placebo tests__

@Abadie.2003 and @Abadie.2010 propose a more qualitative approach to asses the significance of the findings: __placebo tests__. The idea is that we randomly assign the treatment to a control unit and perform the Synthetic Control method with this placebo treatment unit, and we repeat this exercise with every control unit.

Basically, we test what treatment effect we find if we assign treatment status to a unit that actually did not receive the treatment, thereby giving us a distribution of "treatment effects" for non-treated contries. We can then compare these placebo results to our actual treatment effect.

![Placebo test for Synthetic Control [@Abadie.2010]](fig/abadie_placebo.jpg)

To increase the comparability of the actual treatment unit and placebo treatment units, propose to exclude those units which have a poor pre-treatment fit (left panel above) by dropping those with a mean square prediction error above a specific threshold (right panel above).

If the actual treatment effect lies at the extremes of the distribution of the placebo "treatment" units, we can say that the treatment likely had a significant effect on the outcome. If, in contrast, the actual treatment effect lies in the middle of the placebo effects for those not receiving the treatment, this indicates that the treatment did not have a significant influence.

### Example

Can we use our marriage wage premium example and the respective data to perform a Synthetic Control analysis? Why (not)?

In R, we can use the package `Synth` to produce Synthetic Control estimates. For a demonstration using Stata by Jens Hainmueller see the following [video](https://web.stanford.edu/~jhain/Video/SynthDemo.mp4).

Here, we use the example of @Abadie.2003, analyzing the influence of the terrorist conflict in Basque on economic outcomes.

```{r}
library(Synth)
# devtools::install_github("bcastanho/SCtools")
library(SCtools)

data("basque")
basque <- basque[-which(basque$regionno == 1), ]
head(basque[basque$year > 1963, ])
```

The data contains the GDP per capita for Spanish regions in long format, and several covariates. Our treatment is the onset of political unrest in Basque in the year 1970. However, the terrorist activities were low at the beginning, but increased sharply in 1974.

First, we need to create a dataprep object which ensures that everything is in the right format for the `synth()` function. Apart from the standard information, we here have to decide which control units should be included, which covariates should be included in the matching procedure, and for which periods these covariates should be included. 

Here we want to include the following covariates:

* The average GDP per capita in 1960 - 1964 and in 1965-1969 (we use the `special.predictors` option).

* The share with high school ("school.high") and more than high school ("school.post.high") for the entire observation period (which need to specified using the option `time.predictors.prior`).

* Population density ("popdens") in 1969 (we use the `special.predictors` option).

* Percentage working in the agricultural ("sec.agriculture"), the industrial ("sec.industry"), and the service sector ("sec.services.venta") from 1961, 1963, 1965, 1967, 1969 (we use the `special.predictors` option).

```{r}
data_prep.out <- dataprep(foo = basque, 
                          predictors = c("school.high", "school.post.high"),
                          predictors.op = "mean", 
                          time.predictors.prior = 1960:1969,
                          special.predictors = list(
                            list("gdpcap", 1960:1964, "mean"),
                            list("gdpcap", 1965:1969, "mean"),
                            list("sec.agriculture", seq(1961, 1969, 2), "mean"),
                            list("sec.industry", seq(1961, 1969, 2), "mean"),
                            list("sec.services.venta", seq(1961, 1969, 2), "mean")
                          ),
                          dependent = "gdpcap", 
                          unit.variable = "regionno",
                          time.variable = "year", 
                          treatment.identifier = 17,
                          controls.identifier = c(2:16, 18), 
                          time.optimize.ssr = 1960:1969, 
                          time.plot = 1960:1997,
                          unit.names.variable = "regionname")

cbind(data_prep.out$X0, data_prep.out$X1)
```

So, this prepares a list of different objects that are used in the optimazition algorithm later on. Above, we see all the used covariates used for mathing.

we can also use it to print the raw trajecotries of the treated region and the control pool

```{r class.source = 'fold-hide', message = FALSE, warning = FALSE}
Y1 <- data.frame(data_prep.out$Y1plot)
names(Y1) <- "y"
Y1$treat <- "Treatment"
Y1$year <- as.numeric(rownames(Y1))

Y0 <- data.frame(rowMeans(data_prep.out$Y0plot))
names(Y0) <- "y"
Y0$treat <- "Control"
Y0$year <- as.numeric(rownames(Y0))

data <- rbind(Y1, Y0)

p1 <- ggplot(data = data, aes(x = year, y = y)) +
  geom_line(aes(group = treat, color = treat, linetype = treat),
            size = 1.2) +
  scale_x_continuous(guide = guide_axis(check.overlap = TRUE))+
  theme_bw() +
  theme(legend.key = element_blank(), legend.title = element_blank(),
        legend.position = c(0.05,0.95), legend.justification = c("left", "top"),
        legend.background = element_blank(),
        legend.box.background = element_rect(colour = "black"),
        legend.spacing.y = unit(-0.1, "cm"))
p1

```

This seems pretty hard to compare as there were already strong differences between Basque and other Spanish reasons before the onset of terrorist activities.

So, let's see if we can do better by estimating a synthetic control unit for Basque using the `synth()` function.

```{r}
synth.out <- synth(data.prep.obj = data_prep.out)
```

This already gives us some important information. "solution.v" tells us the weights for the individual covariates, and "solution.w" gives us the weights for the units in the donor pool for constructing the synthetic control unit.

Before looking at the actual results, it is helpful to check some core statistics. For instance, below we see how well the synthetic control unit mirrors our treated region across the included covariates.

```{r}
synth.tables  <- synth.tab(dataprep.res = data_prep.out, synth.res = synth.out)
synth.tables$tab.pred
```

For the GDP per capita variables, the synthetic control closely resembles the treated region. That's good!! However, for other covariates like "school.high" or "school.post.high", the synthetic control region matches notatbly worse than the raw sample... Nevertheless, why do we not need to worry so much about that? (look at "solution.v" above)

Obviously, we also want to know the weights that each region receives for the construction of the synthetic control.

```{r}
synth.tables$tab.w
```

So, only the Baleares, Cataluna and Madrid contribute to the synthetic control region, while all others are set to zero. Cataluna by far receives the highest weight.

Finally, how does the synthetic control average look like and how does it compare to the treated unit of Basque?

```{r}
path.plot(synth.res = synth.out, dataprep.res = data_prep.out,
          Ylab = c("GDP per capita"))
```

Before 1970, the synthetic control region aligns very well with the Basque region. From 1970, Basque shows a slightly lower GDP than its counterfactual with terrorist activities. From 1974/75 (after the sharp increase in terrorist activities) we see a pronounced decline in the GDP as compared to the synthetic control unit.

We can also show these differences more clearly, when plotting the differences instead of the trajectory.

```{r}
gaps.plot(synth.res = synth.out, dataprep.res = data_prep.out,
          Ylab = c("Difference in GDP per capita"))
```

__Inferencial statistics__

However, is this difference statistically significant or might it be a result of pure chance?

So give some intuition of the significance of the treatment effect, we perform an interaction of placebo test, artificially assigning the treatment status to control regions.

The function `generate.placebos()` of the `SCtools` package does all of this automatically for us.

```{r, results = 'hide', message = FALSE, warning = FALSE}
placebo <- generate.placebos(dataprep.out = data_prep.out,
                             synth.out = synth.out, 
                             strategy = "multisession")
```

Also creating the respective plot.

```{r}
plot_placebos(placebo)
```


The real treatment region seems quite special. For some control regions, we observe similarly stong "treatment effects". However, these extreme regions have a poor pre-treatment fit, and thus should be disregarded.

# Generalized Synthetic Control

Generalized Synthetic Control [@Xu.2017] is a way of generalising the "qualitative" approach with one single treated unit to a setting with multiple treated units based on Interactive Fixed Effects or Factor Models:

$$
Y_{it} = \sum_{r=1}^R \gamma_{ir} \delta_{tr} + \epsilon_{it} \quad \text{or} \quad,
\mathbf{Y} = \mathbf U \mathbf V^\mathrm T + \mathbf{\varepsilon}.
$$

- with with $\mathbf U$ being an $N \times r$ matrix of unknown factor loadings (unit-specific intercepts),
- and $\mathbf V$ an $T \times r$  matrix of unobserved common factors (time-varying coefficients).

Estimate $\\delta$ and $\\gamma$ by least squares and use to impute missing values.

$$
\hat Y _{NT} = \sum_{r=1}^R \hat \delta_{Nr} \hat \gamma_{rT}.
$$

In a matrix form, the $Y_{N \times T}$ can be rewritten as:

$$
Y_{N\times T}= \mathbf U \mathbf V^\mathrm T + \epsilon_{N \times T} =  \mathbf L_{N \times T} + \epsilon_{N \times T} = \\ \left(
\begin{array}{ccccccc}
 \delta_{11} & \dots & \delta_{R1}  \\
\vdots & \dots & \vdots   \\
\vdots & \dots & \vdots   \\
\vdots & \dots & \vdots   \\
\delta_{1N} & \dots & \delta_{RN}  \\
\end{array}\right)
\left(
\begin{array}{ccccccc}
\gamma_{11}  & \dots \dots \dots & \gamma_{1T}  \\
\vdots & \dots \dots \dots & \vdots   \\
\gamma_{R1}  & \dots \dots \dots & \gamma_{RT}  \\
\end{array}
\right) + \epsilon_{N \times T}
$$

# Matrix Completion

Matrix Completion [@Athey.2021] uses a very similar approach. Instead of estimating the factors, we estimate the matrix $\mathbf L_{N \times T}$ directly. It is supposed to generalise the horizontal and vertical approach.

$$
Y_{N\times T}=\left(
\begin{array}{cccccccccc}
 {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & {\color{red} ?}& \checkmark  & \dots  & {\color{red} ?}\\
\checkmark & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & \checkmark & {\color{red} ?}   & \dots & \checkmark  \\
{\color{red} ?}  & \checkmark & {\color{red} ?}  & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & \dots & {\color{red} ?}  \\
 {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & {\color{red} ?}& \checkmark  & \dots  & {\color{red} ?}\\
\checkmark & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & {\color{red} ?}   & \dots & \checkmark  \\
{\color{red} ?}  & \checkmark & {\color{red} ?}  & {\color{red} ?} & {\color{red} ?} & {\color{red} ?} & \dots & {\color{red} ?}  \\
\vdots   &  \vdots & \vdots &\vdots   &  \vdots & \vdots &\ddots &\vdots \\
{\color{red} ?}  & {\color{red} ?} & {\color{red} ?} & {\color{red} ?}& \checkmark & {\color{red} ?}   & \dots & {\color{red} ?}\\
\end{array}
\right)
$$


This can be done via Nuclear Norm Minimization:

$$
\min_{L}\frac{1}{|\cal{O}|}
\sum_{(i,t) \in \cal{o}} \left(Y_{it} -
L_{it} \right)^2+\lambda_L \|L\|_*
$$

- where $\cal{O}$ denote the set of pairs of indices corresponding to the observed entries (the entries with $W_{it} = 0$).

Given any $N\times T$ matrix $A$, define the two $N\times T$ matrices  $P_\cal{O}(A)$
and  $P_\cal{O}^\perp(A)$
with typical elements:
$$
P_\cal{O}(A)_{it}=
\left\{
\begin{array}{ll}
A_{it}\hskip1cm & {\rm if}\ (i,t)\in\cal{O}\,,\\
0&{\rm if}\ (i,t)\notin\cal{O}\,,
\end{array}\right.
$$
and
$$
P_\cal{O}^\perp(A)_{it}=
\left\{
\begin{array}{ll}
0\hskip1cm & {\rm if}\ (i,t)\in\cal{O}\,,\\
A_{it}&{\rm if}\ (i,t)\notin\cal{O}\,.
\end{array}\right.
$$

Let $A=S\Sigma R^\top$ be the Singular Value Decomposition for $A$, with  $\sigma_1(A),\ldots,\sigma_{\min(N,T)}(A)$, denoting the singular values. Then define the matrix shrinkage operator
$$
\ shrink_\lambda(A)=S \tilde\Sigma R^\top\,,
$$
where $\tilde\Sigma$ is equal to $\Sigma$ with the $i$-th  singular value $\sigma_i(A)$ replaced by $\max(\sigma_i(A)-\lambda,0)$.

### The algorithm 

(1) Start with the initial choice $L_1(\lambda)=P_\cal{O}(Y)$, with zeros for the missing values.
(2) Then for $k=1,2,\ldots,$ define,
$$
L_{k+1}(\lambda)=shrink_\lambda\Biggl\{P_\cal{O}(Y)+P_\cal{O}^\perp\Big(L_k(\lambda)\Big)\Biggr\}\,
$$
until the sequence $\left\{L_k(\lambda)\right\}_{k\ge 1}$ converges.
(3) The limiting matrix $L^*$ is our estimator for the regularization parameter $\lambda$, denoted by $\hat{L}(\lambda,\cal{O})$.

![](fig/correlation.png)

# References

